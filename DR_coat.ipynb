{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DR_coat.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOE4jLmn1adtmVfnwr7o9oi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Recommendation-System-Project/Recommendation-system/blob/main/DR_coat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wdCXyym1HQXh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "HtgyefyshZro"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_emputation(w,y,r,e,p):\n",
        "  return  ((w*r)**2 - 2*w*y*r + y**2 - e)**2/p\n",
        "\n",
        "def gradient_descent(w, y, le, observed_e, predicted_r, propensities, max_iter, lr=0.1):\n",
        "  w.requires_grad = True\n",
        "  y.requires_grad = True\n",
        "  m = observed_e.shape[0]\n",
        "  theta = [w.item(), y.item()]\n",
        "  iter_list = []\n",
        "  loss_list = []\n",
        "  for i in range(max_iter):\n",
        "    loss = 0\n",
        "    rand1 = np.random.randint(0, m, 1)\n",
        "    r = torch.from_numpy(predicted_r[rand1]).type(torch.float32)\n",
        "    e = torch.from_numpy(observed_e[rand1]).type(torch.float32)\n",
        "    p = torch.from_numpy(propensities[rand1]).type(torch.float32)\n",
        "\n",
        "    output = le(w,y,r,e,p)\n",
        "    output.backward()\n",
        "    w.data -= lr * w.grad\n",
        "    y.data -= lr * y.grad\n",
        "    w.grad.fill_(0)\n",
        "    y.grad.fill_(0)\n",
        "\n",
        "    for j in range(m):\n",
        "      every_loss = le(w,y,predicted_r[j],observed_e[j],propensities[j]).item()\n",
        "      loss = loss + every_loss\n",
        "    theta[0] = w.item()\n",
        "    theta[1] = y.item()\n",
        "    # print(\"iter_count: \", i, \"the loss: \",loss)\n",
        "    # print(\"theta:\", theta)\n",
        "    iter_list.append(i)\n",
        "    loss_list.append(loss)\n",
        "      \n",
        "  plt.plot(iter_list, loss_list)\n",
        "  plt.xlabel(\"iter\")\n",
        "  plt.ylabel(\"loss\")\n",
        "  plt.show()\n",
        "  return theta, loss"
      ],
      "metadata": {
        "id": "xG8T_-h_Kz6V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FM 特征组合层\n",
        "class crossLayer(layers.Layer):\n",
        "    def __init__(self,input_dim, output_dim=10, **kwargs):\n",
        "        super(crossLayer, self).__init__(**kwargs)\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        # 定义交叉特征的权重\n",
        "        self.kernel = self.add_weight(name='kernel', shape=(self.input_dim, self.output_dim), initializer='glorot_uniform',trainable=True)\n",
        "        \n",
        "    def call(self, x): # 对照上述公式中的二次项优化公式一起理解\n",
        "        a = K.pow(K.dot(x, self.kernel), 2)\n",
        "        b = K.dot(K.pow(x, 2), K.pow(self.kernel, 2))\n",
        "        return 0.5 * K.mean(a-b, 1, keepdims=True)\n",
        "\n",
        "# 定义FM模型\n",
        "def FM(feature_dim, e, p, O_mask):\n",
        "    inputs = Input(shape=(feature_dim, ))\n",
        "    \n",
        "    # 一阶特征\n",
        "    linear = Dense(units=1, kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01))(inputs)\n",
        "    \n",
        "    # 二阶特征\n",
        "    cross = crossLayer(feature_dim)(inputs)\n",
        "    add = Add()([linear, cross])  # 将一阶特征与二阶特征相加构建FM模型\n",
        "    \n",
        "    pred = add\n",
        "    model = Model(inputs=inputs, outputs=pred)\n",
        "    \n",
        "    model.summary()\n",
        "    # def my_loss_fn(y_true, y_pred):\n",
        "    #   squared_difference = tf.square(y_true - y_pred)\n",
        "    #   return tf.reduce_mean(squared_difference, axis=-1)  # Note the `axis=-1`\n",
        "    # model.compile(loss=my_loss_fn, optimizer=optimizers.SGD())\n",
        "\n",
        "    # 以下两段loss定义格式不应更改，keras的loss有且仅有两个参数y_true, y_pred\n",
        "    def my_coef(y_true, y_pred, predicted_e, propensities, mask):\n",
        "      error = tf.square(y_true - y_pred)\n",
        "      error = tf.reshape(error, [290, 300])\n",
        "      delta = tf.subtract(error, predicted_e)\n",
        "      a = tf.multiply(mask, delta)\n",
        "      b = tf.divide(a, propensities)\n",
        "      return tf.add(predicted_e, b)\n",
        "\n",
        "    def my_loss_fn(predicted_e, propensities, mask):\n",
        "      def loss(y_true, y_pred):\n",
        "        return my_coef(y_true, y_pred, predicted_e, propensities, mask)\n",
        "      return loss\n",
        "      \n",
        "    model.compile(loss=my_loss_fn(predicted_e = e, propensities = p, mask = O_mask),\n",
        "                  optimizer=optimizers.SGD(), metrics=['accuracy'])\n",
        "\n",
        "    \n",
        "    return model    "
      ],
      "metadata": {
        "id": "0F_8fV4FkcF6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def result(history):\n",
        "  # list all data in history\n",
        "  print(history.history.keys())\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  # summarize history for loss\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "I7-gnBB2-FY0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "  observed_r = np.genfromtxt(\"data/train.ascii\", dtype=None) # observed ratings\n",
        "  observed_r = observed_r.astype('float')\n",
        "  observed_r[observed_r == 0] = np.nan\n",
        "    \n",
        "  predicted_r = np.random.randint(1,5,size=observed_r.shape) # predicted rating\n",
        "    \n",
        "  propensities = np.genfromtxt(\"data/our_propensities.ascii\", dtype=None) # propensities for all user item pairs\n",
        "\n",
        "  O_mask = np.int64(observed_r != np.nan)\n",
        "\n",
        "  item_features = np.genfromtxt(\"data/item_features_encoded.ascii\", dtype=None)\n",
        "  user_features = np.genfromtxt(\"data/user_features.ascii\", dtype=None)\n",
        "\n",
        "  test_r = np.genfromtxt(\"data/test.ascii\", dtype=None) # observed ratings for test\n",
        "  test_r = observed_r.astype('float')\n",
        "   \n",
        "  return observed_r, predicted_r, propensities, O_mask, item_features, user_features"
      ],
      "metadata": {
        "id": "W76NRDhOlDpx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features_matrix(O_mask, user_features, item_features):\n",
        "  features_matrix = []\n",
        "  for u in range(O_mask.shape[0]):\n",
        "    for i in range(O_mask.shape[1]):\n",
        "      user_f = user_features[u]\n",
        "      item_f = item_features[i]\n",
        "      observed = np.concatenate((user_f, item_f))\n",
        "      features_matrix.append(observed)\n",
        "  features_matrix = np.array(features_matrix)\n",
        "  return features_matrix"
      ],
      "metadata": {
        "id": "88QB5S9olZdT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "observed_r, predicted_r, propensities, O_mask, item_features, user_features = get_data()\n",
        "features_matrix = get_features_matrix(O_mask, user_features, item_features)\n",
        "\n",
        "observed_r_1d = observed_r.flatten()\n",
        "observed_r_1d = observed_r_1d[~np.isnan(observed_r_1d)]\n",
        "propensities_1d = np.array(propensities*O_mask).flatten()\n",
        "propensities_1d = propensities_1d[propensities_1d != 0]\n",
        "predicted_r_1d = np.array(predicted_r*O_mask).flatten()\n",
        "predicted_r_1d = predicted_r_1d[predicted_r_1d != 0]\n",
        "\n",
        "ratings_train = np.genfromtxt(\"data/train.ascii\", dtype=None).astype(np.float32)\n",
        "ratings_test = np.genfromtxt(\"data/test.ascii\", dtype=None).astype(np.float32)\n",
        "test_accuracy = 0.5\n",
        "\n",
        "iter_count = 0\n",
        "max_iter_joint = 10\n",
        "while test_accuracy <= 0.95 and iter_count < max_iter_joint:\n",
        "  observed_e = predicted_r - observed_r # e_ui\n",
        "  observed_e_1d = observed_e.flatten()\n",
        "  observed_e_1d = observed_e_1d[~np.isnan(observed_e_1d)]\n",
        "\n",
        "  # Imputation error\n",
        "  w = torch.randn(1)\n",
        "  y = torch.randn(1)\n",
        "  lr = 0.0001\n",
        "  max_iter = 1000\n",
        "\n",
        "  start = time.time()\n",
        "  theta,loss_list = gradient_descent(w, y, loss_emputation, observed_e_1d, predicted_r_1d, propensities_1d, max_iter, lr)\n",
        "  end = time.time()\n",
        "  print(\"The time of execution of above program is :\", end-start)\n",
        "  print(\"theta for imputation error: \", theta)\n",
        "\n",
        "  predicted_e = theta[0]*((predicted_r - theta[1])**2)\n",
        "\n",
        "  # Training FM\n",
        "  x_trn = features_matrix\n",
        "  y_trn = ratings_train.flatten()\n",
        "  y_tst = ratings_test.flatten()\n",
        "\n",
        "  # 定义模型\n",
        "  model = FM(x_trn.shape[1], predicted_e.astype(np.float32), propensities, O_mask.astype(np.float32))\n",
        "\n",
        "  # 训练模型\n",
        "  history = model.fit(x_trn, y_trn, epochs=500, batch_size=87000, validation_data=(x_trn, y_tst))\n",
        "  predicted_r_1d = model.predict(features_matrix)\n",
        "  predicted_r = predicted_r_1d.reshape(290,300)\n",
        "  iter_count += 1\n",
        "  test_accuracy = history.history['val_accuracy'][-1]\n",
        "\n",
        "  result(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "OjOyRzAxhtzk",
        "outputId": "abf0aed9-da2e-4ebf-93ed-cb18f2608647"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5748c224ab60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0miter_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmax_iter_joint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.95\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miter_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_iter_joint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mobserved_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicted_r\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mobserved_r\u001b[0m \u001b[0;31m# e_ui\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mobserved_e_1d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobserved_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts, bins = np.histogram(predicted_r)\n",
        "plt.hist(bins[:-1], bins, weights=counts)"
      ],
      "metadata": {
        "id": "bMMBf4MVqg-y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}