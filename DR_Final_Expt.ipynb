{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5e8b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f0d48e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sample_num=2000):\n",
    "    observed_r = np.random.uniform(low=0, high=5, size=(sample_num,)) # observed_r\n",
    "    predicted_r = np.random.uniform(low=0, high=5, size=(sample_num,)) # predicted rating\n",
    "    imputed_e = np.power((predicted_r - 0.5), 2)*0.3 ## e_hat_ui = 7*(r_ui - 5)^2\n",
    "    \n",
    "    observed_e = predicted_r - observed_r # e_ui\n",
    "    propensities = np.random.uniform(low=0, high=1, size=(sample_num,)) # propensities for observed X_ui\n",
    "   \n",
    "    return observed_r, predicted_r, imputed_e, observed_e, propensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f733992b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.69869252, 4.32719075, 4.41498194, ..., 0.30308346, 1.19902313,\n",
       "        2.65493588]),\n",
       " array([0.58589618, 2.67536863, 4.05944089, ..., 4.88281051, 0.63120312,\n",
       "        1.2423625 ]),\n",
       " array([2.21344598e-03, 1.41966861e+00, 3.80088584e+00, ...,\n",
       "        5.76270838e+00, 5.16427778e-03, 1.65330623e-01]),\n",
       " array([-1.11279635, -1.65182212, -0.35554104, ...,  4.57972705,\n",
       "        -0.56782   , -1.41257338]),\n",
       " array([0.52466091, 0.91450814, 0.31854313, ..., 0.84216456, 0.27657348,\n",
       "        0.88789906]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ce0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fdbd0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(predicted_r,\n",
    "        imputed_e,\n",
    "        observed_e,\n",
    "        propensities,\n",
    "        step_size=2,\n",
    "        max_iter_count=2000):\n",
    "\n",
    "    m = predicted_r.shape[0]\n",
    "    var = 1\n",
    "    theta = np.zeros(2)\n",
    "    imputed_e = imputed_e.flatten()\n",
    "    loss = 1\n",
    "    iter_count = 0\n",
    "    iter_list = []\n",
    "    loss_list = []\n",
    "    theta1 = []\n",
    "    theta2 = []\n",
    "\n",
    "    while loss > 0.01 and iter_count < max_iter_count:\n",
    "        loss = 0\n",
    "        theta = [1,2]\n",
    "        theta1.append(theta[0])\n",
    "        theta2.append(theta[1])\n",
    "        rand1 = np.random.randint(0, m, 1)\n",
    "\n",
    "        gradient = 4 * (theta[0] * predicted_r[rand1] - 2 * theta[0] *\n",
    "                        theta[1] * predicted_r[rand1] + theta[1]**2 -\n",
    "                        observed_e[rand1]) * (theta[0] * predicted_r[rand1] -\n",
    "                                              theta[0] * theta[1]) / propensities[rand1]\n",
    "\n",
    "        for i in range(len(theta)):\n",
    "            theta[i] = theta[i] - step_size * gradient \n",
    "        h = np.power((predicted_r - theta[1]), 2) * theta[0]\n",
    "        for i in range(m):\n",
    "            every_loss = np.power((h[i] - observed_e[i]), 2) / propensities[i]\n",
    "            loss = loss + every_loss\n",
    "\n",
    "        print(\"iter_count: \", iter_count, \"the loss:\", loss, \"the gradient:\", gradient)\n",
    "\n",
    "        iter_list.append(iter_count)\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        iter_count += 1\n",
    "\n",
    "    plt.plot(iter_list, loss_list)\n",
    "    plt.xlabel(\"iter\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()\n",
    "    return theta1, theta2, theta, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "92241909",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  0 the loss: 217425.99069123453 the gradient: [1.88796568]\n",
      "iter_count:  1 the loss: 241175.13272269416 the gradient: [-64.81255636]\n",
      "iter_count:  2 the loss: 225130.0073762092 the gradient: [-20.71967914]\n",
      "iter_count:  3 the loss: 240950.51131308306 the gradient: [-64.22155992]\n",
      "iter_count:  4 the loss: 235858.1468855407 the gradient: [-50.62852886]\n",
      "iter_count:  5 the loss: 247516.30727664553 the gradient: [-81.20650295]\n",
      "iter_count:  6 the loss: 216959.80778292308 the gradient: [3.28771335]\n",
      "iter_count:  7 the loss: 218807.65119956608 the gradient: [-2.23898722]\n",
      "iter_count:  8 the loss: 223330.47164671888 the gradient: [-15.52604742]\n",
      "iter_count:  9 the loss: 524544.6132096569 the gradient: [-502.21138026]\n",
      "iter_count:  10 the loss: 228697.98417801334 the gradient: [-30.86437729]\n",
      "iter_count:  11 the loss: 217531.32699133593 the gradient: [1.57219629]\n",
      "iter_count:  12 the loss: 2251395.5204473822 the gradient: [-1209.20352333]\n",
      "iter_count:  13 the loss: 220506.66833846082 the gradient: [-7.26998976]\n",
      "iter_count:  14 the loss: 218083.5371577271 the gradient: [-0.08010809]\n",
      "iter_count:  15 the loss: 240427.07375753138 the gradient: [-62.84157416]\n",
      "iter_count:  16 the loss: 265968.34855612303 the gradient: [-125.92777382]\n",
      "iter_count:  17 the loss: 229755.75618077247 the gradient: [-33.8334758]\n",
      "iter_count:  18 the loss: 236251.71860529116 the gradient: [-51.69255433]\n",
      "iter_count:  19 the loss: 601157.1056434258 the gradient: [-570.62982704]\n",
      "iter_count:  20 the loss: 267027.7645838254 the gradient: [-128.36982277]\n",
      "iter_count:  21 the loss: 226345.54725024092 the gradient: [-24.19842744]\n",
      "iter_count:  22 the loss: 237484.1696642569 the gradient: [-55.00977472]\n",
      "iter_count:  23 the loss: 272009.9914716079 the gradient: [-139.68336474]\n",
      "iter_count:  24 the loss: 221879.49711326527 the gradient: [-11.3001254]\n",
      "iter_count:  25 the loss: 243082.72887682894 the gradient: [-69.80290756]\n",
      "iter_count:  26 the loss: 261514.3061136272 the gradient: [-115.5172814]\n",
      "iter_count:  27 the loss: 242667.6594087702 the gradient: [-68.7214173]\n",
      "iter_count:  28 the loss: 338430.06244089175 the gradient: [-268.18274447]\n",
      "iter_count:  29 the loss: 245548.696235344 the gradient: [-76.17884904]\n",
      "iter_count:  30 the loss: 230218.96094753148 the gradient: [-35.12819961]\n",
      "iter_count:  31 the loss: 305058.9366437115 the gradient: [-208.30000935]\n",
      "iter_count:  32 the loss: 324180.137191621 the gradient: [-243.61635832]\n",
      "iter_count:  33 the loss: 272481.0799691616 the gradient: [-140.73877247]\n",
      "iter_count:  34 the loss: 226600.36270365145 the gradient: [-24.92469967]\n",
      "iter_count:  35 the loss: 218405.42824090383 the gradient: [-1.04088572]\n",
      "iter_count:  36 the loss: 311846.263974292 the gradient: [-221.16603987]\n",
      "iter_count:  37 the loss: 225529.7746179093 the gradient: [-21.86637463]\n",
      "iter_count:  38 the loss: 240951.93198233284 the gradient: [-64.22530006]\n",
      "iter_count:  39 the loss: 234808.79107065205 the gradient: [-47.78038077]\n",
      "iter_count:  40 the loss: 1008174.086068907 the gradient: [-821.31977157]\n",
      "iter_count:  41 the loss: 222503.99030781956 the gradient: [-13.12315899]\n",
      "iter_count:  42 the loss: 215334.26396545026 the gradient: [8.19750078]\n",
      "iter_count:  43 the loss: 354457.84073045786 the gradient: [-294.23380106]\n",
      "iter_count:  44 the loss: 242030.41967133744 the gradient: [-67.05635146]\n",
      "iter_count:  45 the loss: 286166.197181848 the gradient: [-170.372491]\n",
      "iter_count:  46 the loss: 750677.2113442768 the gradient: [-679.5129398]\n",
      "iter_count:  47 the loss: 224023.98838225278 the gradient: [-17.53379449]\n",
      "iter_count:  48 the loss: 225000.64609376466 the gradient: [-20.34807]\n",
      "iter_count:  49 the loss: 218154.19463740085 the gradient: [-0.29115546]\n",
      "iter_count:  50 the loss: 220519.75672020222 the gradient: [-7.30855961]\n",
      "iter_count:  51 the loss: 225077.39766197186 the gradient: [-20.56858245]\n",
      "iter_count:  52 the loss: 222550.6159579883 the gradient: [-13.25901354]\n",
      "iter_count:  53 the loss: 268024.37557852233 the gradient: [-130.65533699]\n",
      "iter_count:  54 the loss: 1339107.1186294605 the gradient: [-957.07360394]\n",
      "iter_count:  55 the loss: 218851.24245540163 the gradient: [-2.36866931]\n",
      "iter_count:  56 the loss: 524544.6132096569 the gradient: [-502.21138026]\n",
      "iter_count:  57 the loss: 217642.57902749485 the gradient: [1.23889712]\n",
      "iter_count:  58 the loss: 272481.0799691616 the gradient: [-140.73877247]\n",
      "iter_count:  59 the loss: 216074.1370725836 the gradient: [5.95718883]\n",
      "iter_count:  60 the loss: 617621.7871595158 the gradient: [-584.0249716]\n",
      "iter_count:  61 the loss: 264975.08955386665 the gradient: [-123.62641968]\n",
      "iter_count:  62 the loss: 244219.93563123708 the gradient: [-72.7536813]\n",
      "iter_count:  63 the loss: 230197.6336421065 the gradient: [-35.06865952]\n",
      "iter_count:  64 the loss: 219026.35583299454 the gradient: [-2.8893029]\n",
      "iter_count:  65 the loss: 270923.2393334528 the gradient: [-137.23930564]\n",
      "iter_count:  66 the loss: 218853.09651741665 the gradient: [-2.37418436]\n",
      "iter_count:  67 the loss: 5043799.331122604 the gradient: [-1623.56964939]\n",
      "iter_count:  68 the loss: 311846.263974292 the gradient: [-221.16603987]\n",
      "iter_count:  69 the loss: 333090.6399316272 the gradient: [-259.14093219]\n",
      "iter_count:  70 the loss: 223422.32167951675 the gradient: [-15.79240336]\n",
      "iter_count:  71 the loss: 259853.37708477056 the gradient: [-111.57463107]\n",
      "iter_count:  72 the loss: 233306.21297801734 the gradient: [-43.67351382]\n",
      "iter_count:  73 the loss: 272844.82437643665 the gradient: [-141.55202375]\n",
      "iter_count:  74 the loss: 230565.16481864228 the gradient: [-36.09372672]\n",
      "iter_count:  75 the loss: 284291.4175303841 the gradient: [-166.42622603]\n",
      "iter_count:  76 the loss: 243988.8395499683 the gradient: [-72.15549637]\n",
      "iter_count:  77 the loss: 229540.75394325994 the gradient: [-33.23138553]\n",
      "iter_count:  78 the loss: 221226.9377128591 the gradient: [-9.38832022]\n",
      "iter_count:  79 the loss: 255235.3706421063 the gradient: [-100.43459772]\n",
      "iter_count:  80 the loss: 280910.44961242203 the gradient: [-159.22065229]\n",
      "iter_count:  81 the loss: 235806.31399932803 the gradient: [-50.4882277]\n",
      "iter_count:  82 the loss: 537727.2195238738 the gradient: [-514.78273335]\n",
      "iter_count:  83 the loss: 1732453.6841732878 the gradient: [-1081.10523494]\n",
      "iter_count:  84 the loss: 219686.91788337217 the gradient: [-4.84861632]\n",
      "iter_count:  85 the loss: 215528.68734805437 the gradient: [7.60788353]\n",
      "iter_count:  86 the loss: 226783.41704867923 the gradient: [-25.44580363]\n",
      "iter_count:  87 the loss: 4467042.632882606 the gradient: [-1558.82527752]\n",
      "iter_count:  88 the loss: 252904.14026836696 the gradient: [-94.70915608]\n",
      "iter_count:  89 the loss: 218654.11513382636 the gradient: [-1.7819701]\n",
      "iter_count:  90 the loss: 257607.35397953648 the gradient: [-106.18959306]\n",
      "iter_count:  91 the loss: 218227.96745687997 the gradient: [-0.51141825]\n",
      "iter_count:  92 the loss: 244411.62973573542 the gradient: [-73.24931509]\n",
      "iter_count:  93 the loss: 61166710487688.47 the gradient: [-39952.22334465]\n",
      "iter_count:  94 the loss: 287679.8371142502 the gradient: [-173.53339357]\n",
      "iter_count:  95 the loss: 380022.1731517054 the gradient: [-332.75376067]\n",
      "iter_count:  96 the loss: 218723.28381694478 the gradient: [-1.98790734]\n",
      "iter_count:  97 the loss: 283442.7625140038 the gradient: [-164.62838996]\n",
      "iter_count:  98 the loss: 231698.8874247223 the gradient: [-39.24267915]\n",
      "iter_count:  99 the loss: 233820.93078540335 the gradient: [-45.08415856]\n",
      "iter_count:  100 the loss: 236740.19046608114 the gradient: [-53.00997148]\n",
      "iter_count:  101 the loss: 218290.43430230193 the gradient: [-0.69785321]\n",
      "iter_count:  102 the loss: 238165.40670115774 the gradient: [-56.83384614]\n",
      "iter_count:  103 the loss: 277754.55766762455 the gradient: [-152.38916297]\n",
      "iter_count:  104 the loss: 226338.36231176258 the gradient: [-24.17793406]\n",
      "iter_count:  105 the loss: 271478.4481135866 the gradient: [-138.48957963]\n",
      "iter_count:  106 the loss: 211590.0676874215 the gradient: [19.68009222]\n",
      "iter_count:  107 the loss: 243490.10722696685 the gradient: [-70.8620206]\n",
      "iter_count:  108 the loss: 222277.61899857342 the gradient: [-12.46306899]\n",
      "iter_count:  109 the loss: 293517.82762273407 the gradient: [-185.51912557]\n",
      "iter_count:  110 the loss: 226728.25984923597 the gradient: [-25.28884256]\n",
      "iter_count:  111 the loss: 233738.72308199154 the gradient: [-44.8591259]\n",
      "iter_count:  112 the loss: 223634.1984035466 the gradient: [-16.40630259]\n",
      "iter_count:  113 the loss: 500731.7572552794 the gradient: [-478.52573992]\n",
      "iter_count:  114 the loss: 286301.4700324801 the gradient: [-170.65588929]\n",
      "iter_count:  115 the loss: 328113.89369019965 the gradient: [-250.53894272]\n",
      "iter_count:  116 the loss: 243706.17387525432 the gradient: [-71.42281919]\n",
      "iter_count:  117 the loss: 218268.66234644572 the gradient: [-0.63288134]\n",
      "iter_count:  118 the loss: 1026649.9587403354 the gradient: [-830.00423084]\n",
      "iter_count:  119 the loss: 289777.6552734043 the gradient: [-177.87756002]\n",
      "iter_count:  120 the loss: 217460.08689118232 the gradient: [1.78573404]\n",
      "iter_count:  121 the loss: 228791.2619683565 the gradient: [-31.12690215]\n",
      "iter_count:  122 the loss: 218139.78103113282 the gradient: [-0.24811018]\n",
      "iter_count:  123 the loss: 222192.78010761883 the gradient: [-12.21546606]\n",
      "iter_count:  124 the loss: 237516.40700927086 the gradient: [-55.09624532]\n",
      "iter_count:  125 the loss: 268545.00088243134 the gradient: [-131.84477692]\n",
      "iter_count:  126 the loss: 226461.47796346544 the gradient: [-24.52897966]\n",
      "iter_count:  127 the loss: 232947.18010054488 the gradient: [-42.68717941]\n",
      "iter_count:  128 the loss: 529829.6301602285 the gradient: [-507.29554161]\n",
      "iter_count:  129 the loss: 228669.88319846682 the gradient: [-30.78526212]\n",
      "iter_count:  130 the loss: 229887.3662781603 the gradient: [-34.20168226]\n",
      "iter_count:  131 the loss: 254609.43778855586 the gradient: [-98.9041267]\n",
      "iter_count:  132 the loss: 239308.567758151 the gradient: [-59.87964874]\n",
      "iter_count:  133 the loss: 218100.7741676783 the gradient: [-0.13160121]\n",
      "iter_count:  134 the loss: 218227.66798879803 the gradient: [-0.51052432]\n",
      "iter_count:  135 the loss: 237419.23262497858 the gradient: [-54.83554735]\n",
      "iter_count:  136 the loss: 9391807.46549702 the gradient: [-1970.88331872]\n",
      "iter_count:  137 the loss: 2773987.5102584055 the gradient: [-1313.18704757]\n",
      "iter_count:  138 the loss: 222537.45325314102 the gradient: [-13.22066457]\n",
      "iter_count:  139 the loss: 302654.3756580522 the gradient: [-203.64980219]\n",
      "iter_count:  140 the loss: 245024.26024097565 the gradient: [-74.82990148]\n",
      "iter_count:  141 the loss: 1260985.4984113302 the gradient: [-928.29068531]\n",
      "iter_count:  142 the loss: 226783.43335100377 the gradient: [-25.44585001]\n",
      "iter_count:  143 the loss: 250346.92893545688 the gradient: [-88.34809738]\n",
      "iter_count:  144 the loss: 8455409.6767613 the gradient: [-1910.22443421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  145 the loss: 276994.2062296876 the gradient: [-150.72769857]\n",
      "iter_count:  146 the loss: 293517.82762273407 the gradient: [-185.51912557]\n",
      "iter_count:  147 the loss: 218687.09902644766 the gradient: [-1.88018369]\n",
      "iter_count:  148 the loss: 242268.04176646488 the gradient: [-67.677909]\n",
      "iter_count:  149 the loss: 363987.728772081 the gradient: [-309.00324767]\n",
      "iter_count:  150 the loss: 405308.3444990515 the gradient: [-367.7044143]\n",
      "iter_count:  151 the loss: 26759627852.15617 the gradient: [-10646.14143015]\n",
      "iter_count:  152 the loss: 218227.96745687997 the gradient: [-0.51141825]\n",
      "iter_count:  153 the loss: 26759627852.15617 the gradient: [-10646.14143015]\n",
      "iter_count:  154 the loss: 230036.99268657708 the gradient: [-34.61996764]\n",
      "iter_count:  155 the loss: 247701.23462681822 the gradient: [-81.6763378]\n",
      "iter_count:  156 the loss: 234012.6864880555 the gradient: [-45.60867019]\n",
      "iter_count:  157 the loss: 321878.5069203417 the gradient: [-239.51402744]\n",
      "iter_count:  158 the loss: 470694.6899054799 the gradient: [-446.64831968]\n",
      "iter_count:  159 the loss: 251477.3623199721 the gradient: [-91.17055576]\n",
      "iter_count:  160 the loss: 254804.99276989346 the gradient: [-99.3828127]\n",
      "iter_count:  161 the loss: 229270.34431153032 the gradient: [-32.47311494]\n",
      "iter_count:  162 the loss: 268745.7982238069 the gradient: [-132.3027041]\n",
      "iter_count:  163 the loss: 227830.9201165855 the gradient: [-28.41756565]\n",
      "iter_count:  164 the loss: 221881.77664269178 the gradient: [-11.30679148]\n",
      "iter_count:  165 the loss: 396472.6189927173 the gradient: [-355.81580133]\n",
      "iter_count:  166 the loss: 219315.68767852138 the gradient: [-3.7483967]\n",
      "iter_count:  167 the loss: 286906.1335103904 the gradient: [-171.92047656]\n",
      "iter_count:  168 the loss: 230264.5112018823 the gradient: [-35.25534014]\n",
      "iter_count:  169 the loss: 262031.89459209694 the gradient: [-116.73911977]\n",
      "iter_count:  170 the loss: 226525.61589739667 the gradient: [-24.71176393]\n",
      "iter_count:  171 the loss: 237775.07843244603 the gradient: [-55.78953426]\n",
      "iter_count:  172 the loss: 220226.00812070046 the gradient: [-6.44223489]\n",
      "iter_count:  173 the loss: 229172.56733516598 the gradient: [-32.19865351]\n",
      "iter_count:  174 the loss: 324137.6299780924 the gradient: [-243.5409463]\n",
      "iter_count:  175 the loss: 220310.22634964922 the gradient: [-6.6907581]\n",
      "iter_count:  176 the loss: 243293.33416114762 the gradient: [-70.35073341]\n",
      "iter_count:  177 the loss: 241782.4900285274 the gradient: [-66.40698511]\n",
      "iter_count:  178 the loss: 217763.94300343253 the gradient: [0.87554244]\n",
      "iter_count:  179 the loss: 470694.6899054799 the gradient: [-446.64831968]\n",
      "iter_count:  180 the loss: 228717.91481270106 the gradient: [-30.9204823]\n",
      "iter_count:  181 the loss: 24330743.263660975 the gradient: [-2561.96155077]\n",
      "iter_count:  182 the loss: 292095.0442419176 the gradient: [-182.62768645]\n",
      "iter_count:  183 the loss: 215334.26396545026 the gradient: [8.19750078]\n",
      "iter_count:  184 the loss: 283277.2650791271 the gradient: [-164.27695089]\n",
      "iter_count:  185 the loss: 355486.4548952437 the gradient: [-295.85276521]\n",
      "iter_count:  186 the loss: 266709.2173308947 the gradient: [-127.63690254]\n",
      "iter_count:  187 the loss: 258708.49784555088 the gradient: [-108.83742949]\n",
      "iter_count:  188 the loss: 237050.65539383522 the gradient: [-53.84548044]\n",
      "iter_count:  189 the loss: 274441.1210623986 the gradient: [-145.10392396]\n",
      "iter_count:  190 the loss: 217857.44660382983 the gradient: [0.59576946]\n",
      "iter_count:  191 the loss: 219565.30168031374 the gradient: [-4.48843425]\n",
      "iter_count:  192 the loss: 10728056.716263957 the gradient: [-2048.93935359]\n",
      "iter_count:  193 the loss: 225386.1904041371 the gradient: [-21.45481088]\n",
      "iter_count:  194 the loss: 222942.91541123125 the gradient: [-14.40066299]\n",
      "iter_count:  195 the loss: 306090.96805672534 the gradient: [-210.28085855]\n",
      "iter_count:  196 the loss: 247232.17812147544 the gradient: [-80.48373398]\n",
      "iter_count:  197 the loss: 237813.76251357424 the gradient: [-55.89313126]\n",
      "iter_count:  198 the loss: 998880.3927541447 the gradient: [-816.89000283]\n",
      "iter_count:  199 the loss: 238841.77491963888 the gradient: [-58.63821984]\n",
      "iter_count:  200 the loss: 221192.50270818576 the gradient: [-9.2872411]\n",
      "iter_count:  201 the loss: 211271.00840862741 the gradient: [20.66991393]\n",
      "iter_count:  202 the loss: 228647.41036391305 the gradient: [-30.7219835]\n",
      "iter_count:  203 the loss: 225839.8322816341 the gradient: [-22.75398735]\n",
      "iter_count:  204 the loss: 217511.90590495785 the gradient: [1.63040127]\n",
      "iter_count:  205 the loss: 292595.7582920143 the gradient: [-183.64740624]\n",
      "iter_count:  206 the loss: 4769699.731385006 the gradient: [-1593.66157236]\n",
      "iter_count:  207 the loss: 244324.16646250372 the gradient: [-73.02323781]\n",
      "iter_count:  208 the loss: 376310.00057300297 the gradient: [-327.3719298]\n",
      "iter_count:  209 the loss: 526355.8518928182 the gradient: [-503.96056373]\n",
      "iter_count:  210 the loss: 1762349.4895283687 the gradient: [-1089.40135309]\n",
      "iter_count:  211 the loss: 230506.11945623087 the gradient: [-35.92918585]\n",
      "iter_count:  212 the loss: 257594.86812530173 the gradient: [-106.15948331]\n",
      "iter_count:  213 the loss: 1050659.1380919644 the gradient: [-841.05597598]\n",
      "iter_count:  214 the loss: 220226.62451927335 the gradient: [-6.44405427]\n",
      "iter_count:  215 the loss: 257044.46205650433 the gradient: [-104.83024954]\n",
      "iter_count:  216 the loss: 245272.31522660833 the gradient: [-75.46841529]\n",
      "iter_count:  217 the loss: 272587.604613453 the gradient: [-140.97708798]\n",
      "iter_count:  218 the loss: 231761.2481121635 the gradient: [-39.41531754]\n",
      "iter_count:  219 the loss: 266242.86878634396 the gradient: [-126.56181061]\n",
      "iter_count:  220 the loss: 217998.54125624071 the gradient: [0.17387838]\n",
      "iter_count:  221 the loss: 290835.69802517025 the gradient: [-180.05260151]\n",
      "iter_count:  222 the loss: 252444.41781080567 the gradient: [-93.57186266]\n",
      "iter_count:  223 the loss: 748027.0641834433 the gradient: [-677.79790628]\n",
      "iter_count:  224 the loss: 360710.47471331724 the gradient: [-303.98159358]\n",
      "iter_count:  225 the loss: 220519.75672020222 the gradient: [-7.30855961]\n",
      "iter_count:  226 the loss: 237182.10679340368 the gradient: [-54.19881112]\n",
      "iter_count:  227 the loss: 251725.88056279402 the gradient: [-91.78881475]\n",
      "iter_count:  228 the loss: 219569.15921307183 the gradient: [-4.49986262]\n",
      "iter_count:  229 the loss: 218568.50536978856 the gradient: [-1.52697123]\n",
      "iter_count:  230 the loss: 232038.6349969685 the gradient: [-40.18251309]\n",
      "iter_count:  231 the loss: 224226.6222880839 the gradient: [-18.1189553]\n",
      "iter_count:  232 the loss: 235279.93034128434 the gradient: [-49.06116165]\n",
      "iter_count:  233 the loss: 243617.0507199998 the gradient: [-71.19157963]\n",
      "iter_count:  234 the loss: 219837.02899056478 the gradient: [-5.29284967]\n",
      "iter_count:  235 the loss: 223817.50342325025 the gradient: [-16.93683011]\n",
      "iter_count:  236 the loss: 231537.75859286907 the gradient: [-38.79633777]\n",
      "iter_count:  237 the loss: 241533.66433670354 the gradient: [-65.75440053]\n",
      "iter_count:  238 the loss: 221789.95198985448 the gradient: [-11.03819931]\n",
      "iter_count:  239 the loss: 218378.69517442168 the gradient: [-0.96115954]\n",
      "iter_count:  240 the loss: 223476.83118449312 the gradient: [-15.95041073]\n",
      "iter_count:  241 the loss: 307100.4300598437 the gradient: [-212.20977008]\n",
      "iter_count:  242 the loss: 220261.9979050455 the gradient: [-6.54845311]\n",
      "iter_count:  243 the loss: 238550.29768365494 the gradient: [-57.86144626]\n",
      "iter_count:  244 the loss: 189771.9744992259 the gradient: [91.6949713]\n",
      "iter_count:  245 the loss: 277205.8562312689 the gradient: [-151.19079324]\n",
      "iter_count:  246 the loss: 374678.7439920292 the gradient: [-324.98526271]\n",
      "iter_count:  247 the loss: 220510.42973284167 the gradient: [-7.28107442]\n",
      "iter_count:  248 the loss: 246620.10097943444 the gradient: [-78.92302467]\n",
      "iter_count:  249 the loss: 218506.01604720636 the gradient: [-1.34076158]\n",
      "iter_count:  250 the loss: 219335.36782499042 the gradient: [-3.80678078]\n",
      "iter_count:  251 the loss: 750677.2113442768 the gradient: [-679.5129398]\n",
      "iter_count:  252 the loss: 743546.156609886 the gradient: [-674.88311694]\n",
      "iter_count:  253 the loss: 272587.604613453 the gradient: [-140.97708798]\n",
      "iter_count:  254 the loss: 217531.32699133593 the gradient: [1.57219629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  255 the loss: 218069.69930300963 the gradient: [-0.03876584]\n",
      "iter_count:  256 the loss: 5043799.331122604 the gradient: [-1623.56964939]\n",
      "iter_count:  257 the loss: 219783.0620686294 the gradient: [-5.13318516]\n",
      "iter_count:  258 the loss: 246847.06788315423 the gradient: [-79.50234927]\n",
      "iter_count:  259 the loss: 218367.6288862898 the gradient: [-0.92815296]\n",
      "iter_count:  260 the loss: 226972.8137497631 the gradient: [-25.98440402]\n",
      "iter_count:  261 the loss: 217980.07291886854 the gradient: [0.22908195]\n",
      "iter_count:  262 the loss: 475742.0227813397 the gradient: [-452.17405971]\n",
      "iter_count:  263 the loss: 223376.43587787318 the gradient: [-15.65935623]\n",
      "iter_count:  264 the loss: 237484.1696642569 the gradient: [-55.00977472]\n",
      "iter_count:  265 the loss: 217468.7142571332 the gradient: [1.75986948]\n",
      "iter_count:  266 the loss: 263353.59286204126 the gradient: [-119.84466257]\n",
      "iter_count:  267 the loss: 220132.7175974002 the gradient: [-6.1668023]\n",
      "iter_count:  268 the loss: 236603.62990901602 the gradient: [-52.64201838]\n",
      "iter_count:  269 the loss: 469298.41556391853 the gradient: [-445.10696675]\n",
      "iter_count:  270 the loss: 356800.7734897158 the gradient: [-297.9125213]\n",
      "iter_count:  271 the loss: 224226.6222880839 the gradient: [-18.1189553]\n",
      "iter_count:  272 the loss: 236453.86620759967 the gradient: [-52.23817539]\n",
      "iter_count:  273 the loss: 225839.8322816341 the gradient: [-22.75398735]\n",
      "iter_count:  274 the loss: 218078.91581426916 the gradient: [-0.06630163]\n",
      "iter_count:  275 the loss: 223964.90921491047 the gradient: [-17.36306235]\n",
      "iter_count:  276 the loss: 725318.6380176718 the gradient: [-662.82696528]\n",
      "iter_count:  277 the loss: 374342.69576101145 the gradient: [-324.49192833]\n",
      "iter_count:  278 the loss: 798100.976867517 the gradient: [-709.14842198]\n",
      "iter_count:  279 the loss: 317067.5809526963 the gradient: [-230.81184501]\n",
      "iter_count:  280 the loss: 302244.0881185937 the gradient: [-202.8514057]\n",
      "iter_count:  281 the loss: 217182.73986633835 the gradient: [2.61788361]\n",
      "iter_count:  282 the loss: 200609.0487640033 the gradient: [54.80381444]\n",
      "iter_count:  283 the loss: 220047.42768257728 the gradient: [-5.91486426]\n",
      "iter_count:  284 the loss: 265968.34855612303 the gradient: [-125.92777382]\n",
      "iter_count:  285 the loss: 295915.72195659677 the gradient: [-190.34997887]\n",
      "iter_count:  286 the loss: 281662.13376844523 the gradient: [-160.83266318]\n",
      "iter_count:  287 the loss: 228727.47391953456 the gradient: [-30.94738911]\n",
      "iter_count:  288 the loss: 220182.5397734473 the gradient: [-6.31391616]\n",
      "iter_count:  289 the loss: 270682.627727885 the gradient: [-136.69640912]\n",
      "iter_count:  290 the loss: 228740.50427186282 the gradient: [-30.98406443]\n",
      "iter_count:  291 the loss: 1803155.3927351898 the gradient: [-1100.51408429]\n",
      "iter_count:  292 the loss: 269396.7349870585 the gradient: [-133.78406023]\n",
      "iter_count:  293 the loss: 668010.8955513325 the gradient: [-622.64777055]\n",
      "iter_count:  294 the loss: 284291.4175303841 the gradient: [-166.42622603]\n",
      "iter_count:  295 the loss: 353678.9066357523 the gradient: [-293.00372225]\n",
      "iter_count:  296 the loss: 263389.39532172936 the gradient: [-119.92849766]\n",
      "iter_count:  297 the loss: 221795.63153229188 the gradient: [-11.0548163]\n",
      "iter_count:  298 the loss: 221982.5539378028 the gradient: [-11.60141131]\n",
      "iter_count:  299 the loss: 481004.9729514367 the gradient: [-457.86060921]\n",
      "iter_count:  300 the loss: 242937.47142885302 the gradient: [-69.42470375]\n",
      "iter_count:  301 the loss: 286718.6541094582 the gradient: [-171.52876627]\n",
      "iter_count:  302 the loss: 385814.79882849863 the gradient: [-341.01803013]\n",
      "iter_count:  303 the loss: 239273.7008771438 the gradient: [-59.78702917]\n",
      "iter_count:  304 the loss: 324106.2486688909 the gradient: [-243.48526431]\n",
      "iter_count:  305 the loss: 242450.05639675364 the gradient: [-68.15347546]\n",
      "iter_count:  306 the loss: 219315.68767852138 the gradient: [-3.7483967]\n",
      "iter_count:  307 the loss: 241909.45621723702 the gradient: [-66.73963749]\n",
      "iter_count:  308 the loss: 234951.38145726765 the gradient: [-48.16835766]\n",
      "iter_count:  309 the loss: 221881.77664269178 the gradient: [-11.30679148]\n",
      "iter_count:  310 the loss: 237302.2303194136 the gradient: [-54.52147247]\n",
      "iter_count:  311 the loss: 239046.7277368536 the gradient: [-59.18367547]\n",
      "iter_count:  312 the loss: 573528.3340945682 the gradient: [-547.1830957]\n",
      "iter_count:  313 the loss: 228740.50427186282 the gradient: [-30.98406443]\n",
      "iter_count:  314 the loss: 220405.14735060415 the gradient: [-6.97072346]\n",
      "iter_count:  315 the loss: 220967.6492433192 the gradient: [-8.62673422]\n",
      "iter_count:  316 the loss: 697534.4558557156 the gradient: [-643.80010713]\n",
      "iter_count:  317 the loss: 247809.45686193203 the gradient: [-81.95107883]\n",
      "iter_count:  318 the loss: 216474.60889776907 the gradient: [4.74848118]\n",
      "iter_count:  319 the loss: 4467042.632882606 the gradient: [-1558.82527752]\n",
      "iter_count:  320 the loss: 318558.0992556155 the gradient: [-233.52664101]\n",
      "iter_count:  321 the loss: 277829.5847326887 the gradient: [-152.5527766]\n",
      "iter_count:  322 the loss: 458261.65642968693 the gradient: [-432.72309359]\n",
      "iter_count:  323 the loss: 773089.3659657866 the gradient: [-693.76123745]\n",
      "iter_count:  324 the loss: 298397.4686998337 the gradient: [-195.29485531]\n",
      "iter_count:  325 the loss: 372822.34113499906 the gradient: [-322.25280351]\n",
      "iter_count:  326 the loss: 221192.50270818576 the gradient: [-9.2872411]\n",
      "iter_count:  327 the loss: 320278.8839771688 the gradient: [-236.63988426]\n",
      "iter_count:  328 the loss: 218622.7661677439 the gradient: [-1.68860776]\n",
      "iter_count:  329 the loss: 475896.3765811493 the gradient: [-452.34192051]\n",
      "iter_count:  330 the loss: 1013600.3888655865 the gradient: [-823.88701733]\n",
      "iter_count:  331 the loss: 219864.9092124091 the gradient: [-5.37531604]\n",
      "iter_count:  332 the loss: 229765.43246674404 the gradient: [-33.86055634]\n",
      "iter_count:  333 the loss: 274056.3618929355 the gradient: [-144.25033222]\n",
      "iter_count:  334 the loss: 216833.11111161747 the gradient: [3.6687671]\n",
      "iter_count:  335 the loss: 285952.4441681924 the gradient: [-169.92430965]\n",
      "iter_count:  336 the loss: 412789.27220003045 the gradient: [-377.51619398]\n",
      "iter_count:  337 the loss: 691257.0789565939 the gradient: [-639.38631544]\n",
      "iter_count:  338 the loss: 220276.70735255454 the gradient: [-6.59185955]\n",
      "iter_count:  339 the loss: 231778.14074245183 the gradient: [-39.46207263]\n",
      "iter_count:  340 the loss: 240342.2844057839 the gradient: [-62.61766928]\n",
      "iter_count:  341 the loss: 271226.5092339846 the gradient: [-137.9226625]\n",
      "iter_count:  342 the loss: 243966.1039739507 the gradient: [-72.0966061]\n",
      "iter_count:  343 the loss: 285449.6805294067 the gradient: [-168.86838031]\n",
      "iter_count:  344 the loss: 225335.63920737067 the gradient: [-21.30983457]\n",
      "iter_count:  345 the loss: 2251395.5204473822 the gradient: [-1209.20352333]\n",
      "iter_count:  346 the loss: 216663.99774634582 the gradient: [4.17781916]\n",
      "iter_count:  347 the loss: 302180.9631481402 the gradient: [-202.72843934]\n",
      "iter_count:  348 the loss: 305182.93673428154 the gradient: [-208.53848456]\n",
      "iter_count:  349 the loss: 240901.02533434433 the gradient: [-64.09126225]\n",
      "iter_count:  350 the loss: 231616.23748403293 the gradient: [-39.01378088]\n",
      "iter_count:  351 the loss: 272481.0799691616 the gradient: [-140.73877247]\n",
      "iter_count:  352 the loss: 222258.17735530846 the gradient: [-12.40633882]\n",
      "iter_count:  353 the loss: 219126.30186685384 the gradient: [-3.18622442]\n",
      "iter_count:  354 the loss: 232883.60696531125 the gradient: [-42.5123286]\n",
      "iter_count:  355 the loss: 361229.0523050076 the gradient: [-304.78014421]\n",
      "iter_count:  356 the loss: 227620.59356539964 the gradient: [-27.82225754]\n",
      "iter_count:  357 the loss: 258511.477859157 the gradient: [-108.36476926]\n",
      "iter_count:  358 the loss: 225287.8712881381 the gradient: [-21.17280287]\n",
      "iter_count:  359 the loss: 249750217.1037796 the gradient: [-4388.88523138]\n",
      "iter_count:  360 the loss: 219774.06129094306 the gradient: [-5.10655107]\n",
      "iter_count:  361 the loss: 257327.74812953454 the gradient: [-105.51485739]\n",
      "iter_count:  362 the loss: 320278.8839771688 the gradient: [-236.63988426]\n",
      "iter_count:  363 the loss: 223438.72791580064 the gradient: [-15.83996539]\n",
      "iter_count:  364 the loss: 255839.76559934072 the gradient: [-101.90770415]\n",
      "iter_count:  365 the loss: 300328.48451489845 the gradient: [-199.10446283]\n",
      "iter_count:  366 the loss: 338533.52737798076 the gradient: [-268.35608933]\n",
      "iter_count:  367 the loss: 262727.1321384655 the gradient: [-118.3752827]\n",
      "iter_count:  368 the loss: 230565.16481864228 the gradient: [-36.09372672]\n",
      "iter_count:  369 the loss: 220464.81621953673 the gradient: [-7.14663767]\n",
      "iter_count:  370 the loss: 236229.12373010349 the gradient: [-51.63153062]\n",
      "iter_count:  371 the loss: 227369.7732433235 the gradient: [-27.11142581]\n",
      "iter_count:  372 the loss: 237837.44166238807 the gradient: [-55.9565339]\n",
      "iter_count:  373 the loss: 242184.0128735487 the gradient: [-67.458202]\n",
      "iter_count:  374 the loss: 289255.6801289565 the gradient: [-176.80060013]\n",
      "iter_count:  375 the loss: 218000.25542500074 the gradient: [0.16875486]\n",
      "iter_count:  376 the loss: 302062.38005315745 the gradient: [-202.49734854]\n",
      "iter_count:  377 the loss: 249355.10954591748 the gradient: [-85.85788448]\n",
      "iter_count:  378 the loss: 549748.8962312 the gradient: [-525.93564136]\n",
      "iter_count:  379 the loss: 245548.696235344 the gradient: [-76.17884904]\n",
      "iter_count:  380 the loss: 263770.1680526955 the gradient: [-120.8191785]\n",
      "iter_count:  381 the loss: 259477.6864363889 the gradient: [-110.67818866]\n",
      "iter_count:  382 the loss: 11912884.76696609 the gradient: [-2111.40115633]\n",
      "iter_count:  383 the loss: 234750.57131024878 the gradient: [-47.62188248]\n",
      "iter_count:  384 the loss: 253105.78272644625 the gradient: [-95.20713318]\n",
      "iter_count:  385 the loss: 257664.55971842483 the gradient: [-106.32752051]\n",
      "iter_count:  386 the loss: 222550.6159579883 the gradient: [-13.25901354]\n",
      "iter_count:  387 the loss: 231698.8874247223 the gradient: [-39.24267915]\n",
      "iter_count:  388 the loss: 220525.71350051305 the gradient: [-7.32611257]\n",
      "iter_count:  389 the loss: 282324.82541697426 the gradient: [-162.24905445]\n",
      "iter_count:  390 the loss: 225848.33449526594 the gradient: [-22.77830532]\n",
      "iter_count:  391 the loss: 235303.2780395721 the gradient: [-49.12454605]\n",
      "iter_count:  392 the loss: 218398.8444681962 the gradient: [-1.02125201]\n",
      "iter_count:  393 the loss: 261746.38536847834 the gradient: [-116.06553336]\n",
      "iter_count:  394 the loss: 222602.95955419194 the gradient: [-13.41148633]\n",
      "iter_count:  395 the loss: 260691.05602506938 the gradient: [-113.56725171]\n",
      "iter_count:  396 the loss: 218718.39864200552 the gradient: [-1.97336525]\n",
      "iter_count:  397 the loss: 224481.39406192346 the gradient: [-18.85373836]\n",
      "iter_count:  398 the loss: 250048.1339948354 the gradient: [-87.59926456]\n",
      "iter_count:  399 the loss: 260238.89106352947 the gradient: [-112.49272648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  400 the loss: 248424.94718194587 the gradient: [-83.51062066]\n",
      "iter_count:  401 the loss: 8099689.491293081 the gradient: [-1885.64357426]\n",
      "iter_count:  402 the loss: 317489.14576155675 the gradient: [-231.58139463]\n",
      "iter_count:  403 the loss: 270728.3596413191 the gradient: [-136.7996445]\n",
      "iter_count:  404 the loss: 267346.783174317 the gradient: [-129.10265999]\n",
      "iter_count:  405 the loss: 221737.55788586082 the gradient: [-10.88488162]\n",
      "iter_count:  406 the loss: 244459.60336583154 the gradient: [-73.37327369]\n",
      "iter_count:  407 the loss: 223470.99654618138 the gradient: [-15.9335001]\n",
      "iter_count:  408 the loss: 261320.75436348756 the gradient: [-115.05955007]\n",
      "iter_count:  409 the loss: 505966.83353800146 the gradient: [-483.8462288]\n",
      "iter_count:  410 the loss: 290144.22856705525 the gradient: [-178.63233591]\n",
      "iter_count:  411 the loss: 589065074139.5171 the gradient: [-18208.78125505]\n",
      "iter_count:  412 the loss: 224185.4066978224 the gradient: [-17.99998772]\n",
      "iter_count:  413 the loss: 245543.27958291557 the gradient: [-76.16493565]\n",
      "iter_count:  414 the loss: 239635.35461870919 the gradient: [-60.74686873]\n",
      "iter_count:  415 the loss: 239275.12841732783 the gradient: [-59.7908216]\n",
      "iter_count:  416 the loss: 276377.3879970341 the gradient: [-149.37538171]\n",
      "iter_count:  417 the loss: 242450.05639675364 the gradient: [-68.15347546]\n",
      "iter_count:  418 the loss: 217586.481402036 the gradient: [1.40693341]\n",
      "iter_count:  419 the loss: 218527.2118432552 the gradient: [-1.40392951]\n",
      "iter_count:  420 the loss: 236537.22770193405 the gradient: [-52.46300321]\n",
      "iter_count:  421 the loss: 218100.7741676783 the gradient: [-0.13160121]\n",
      "iter_count:  422 the loss: 394295.60261008446 the gradient: [-352.83475162]\n",
      "iter_count:  423 the loss: 239138.0203775052 the gradient: [-59.42644423]\n",
      "iter_count:  424 the loss: 237022.08263069455 the gradient: [-53.76864573]\n",
      "iter_count:  425 the loss: 216765.2408564212 the gradient: [3.87300645]\n",
      "iter_count:  426 the loss: 222577.3440971137 the gradient: [-13.3368761]\n",
      "iter_count:  427 the loss: 217883.07976839622 the gradient: [0.51909803]\n",
      "iter_count:  428 the loss: 218078.65287009618 the gradient: [-0.06551607]\n",
      "iter_count:  429 the loss: 257532.5133298726 the gradient: [-106.00908519]\n",
      "iter_count:  430 the loss: 262292.37062830426 the gradient: [-117.35279616]\n",
      "iter_count:  431 the loss: 222504.58150662764 the gradient: [-13.1248818]\n",
      "iter_count:  432 the loss: 217460.08689118232 the gradient: [1.78573404]\n",
      "iter_count:  433 the loss: 247267.93226334942 the gradient: [-80.57474539]\n",
      "iter_count:  434 the loss: 218424.6593210297 the gradient: [-1.09823124]\n",
      "iter_count:  435 the loss: 218925.95085880277 the gradient: [-2.59084945]\n",
      "iter_count:  436 the loss: 243200.3811048865 the gradient: [-70.10902042]\n",
      "iter_count:  437 the loss: 215528.68734805437 the gradient: [7.60788353]\n",
      "iter_count:  438 the loss: 233971.16341487673 the gradient: [-45.49513841]\n",
      "iter_count:  439 the loss: 4174848.022497596 the gradient: [-1523.1571136]\n",
      "iter_count:  440 the loss: 746417.8188829164 the gradient: [-676.75328371]\n",
      "iter_count:  441 the loss: 603562.5612822808 the gradient: [-572.61265252]\n",
      "iter_count:  442 the loss: 219955.72673339426 the gradient: [-5.64385369]\n",
      "iter_count:  443 the loss: 266169.2176748819 the gradient: [-126.39179025]\n",
      "iter_count:  444 the loss: 544206.370982777 the gradient: [-520.8294695]\n",
      "iter_count:  445 the loss: 244248.0553279406 the gradient: [-72.82641783]\n",
      "iter_count:  446 the loss: 218823.0781775718 the gradient: [-2.28488545]\n",
      "iter_count:  447 the loss: 237860.68819661316 the gradient: [-56.01877026]\n",
      "iter_count:  448 the loss: 260881.4941328264 the gradient: [-114.01906849]\n",
      "iter_count:  449 the loss: 233225.1651889148 the gradient: [-43.45102943]\n",
      "iter_count:  450 the loss: 225386.1904041371 the gradient: [-21.45481088]\n",
      "iter_count:  451 the loss: 248440.5884738501 the gradient: [-83.5501868]\n",
      "iter_count:  452 the loss: 232382.34157000977 the gradient: [-41.13151155]\n",
      "iter_count:  453 the loss: 272767.3449531692 the gradient: [-141.37891857]\n",
      "iter_count:  454 the loss: 272387.4904015167 the gradient: [-140.52929237]\n",
      "iter_count:  455 the loss: 233655.92209157968 the gradient: [-44.6323666]\n",
      "iter_count:  456 the loss: 236453.86620759967 the gradient: [-52.23817539]\n",
      "iter_count:  457 the loss: 223868.23071649796 the gradient: [-17.08355063]\n",
      "iter_count:  458 the loss: 448980.1628492279 the gradient: [-422.02234766]\n",
      "iter_count:  459 the loss: 224026.68845288872 the gradient: [-17.54159604]\n",
      "iter_count:  460 the loss: 247996.74598299275 the gradient: [-82.42617328]\n",
      "iter_count:  461 the loss: 218665.47540962123 the gradient: [-1.81579878]\n",
      "iter_count:  462 the loss: 247131.55783941262 the gradient: [-80.22751456]\n",
      "iter_count:  463 the loss: 258763.70593855015 the gradient: [-108.96979069]\n",
      "iter_count:  464 the loss: 252026.71163953905 the gradient: [-92.53614148]\n",
      "iter_count:  465 the loss: 219632.14125911065 the gradient: [-4.68641882]\n",
      "iter_count:  466 the loss: 220298.27654374487 the gradient: [-6.65550207]\n",
      "iter_count:  467 the loss: 547617.357342251 the gradient: [-523.97905901]\n",
      "iter_count:  468 the loss: 226219.45315701418 the gradient: [-23.83865405]\n",
      "iter_count:  469 the loss: 219854.49495201564 the gradient: [-5.34451341]\n",
      "iter_count:  470 the loss: 278763.03494598076 the gradient: [-154.58344481]\n",
      "iter_count:  471 the loss: 238550.29768365494 the gradient: [-57.86144626]\n",
      "iter_count:  472 the loss: 266242.86878634396 the gradient: [-126.56181061]\n",
      "iter_count:  473 the loss: 253649.37626270886 the gradient: [-96.54699016]\n",
      "iter_count:  474 the loss: 221322.20888926325 the gradient: [-9.6678735]\n",
      "iter_count:  475 the loss: 218748.7351131466 the gradient: [-2.06366375]\n",
      "iter_count:  476 the loss: 297678.88585320197 the gradient: [-193.868761]\n",
      "iter_count:  477 the loss: 238165.40670115774 the gradient: [-56.83384614]\n",
      "iter_count:  478 the loss: 565929.8761277875 the gradient: [-540.50643682]\n",
      "iter_count:  479 the loss: 359015.75233889837 the gradient: [-301.36144388]\n",
      "iter_count:  480 the loss: 221224.23410027515 the gradient: [-9.38038485]\n",
      "iter_count:  481 the loss: 217290.5691759297 the gradient: [2.29419853]\n",
      "iter_count:  482 the loss: 469298.41556391853 the gradient: [-445.10696675]\n",
      "iter_count:  483 the loss: 220891.17616093627 the gradient: [-8.4019043]\n",
      "iter_count:  484 the loss: 262777.48547707015 the gradient: [-118.49355994]\n",
      "iter_count:  485 the loss: 394295.60261008446 the gradient: [-352.83475162]\n",
      "iter_count:  486 the loss: 224551.60264243023 the gradient: [-19.05604194]\n",
      "iter_count:  487 the loss: 288195.56997804035 the gradient: [-174.60530118]\n",
      "iter_count:  488 the loss: 220788.96850211962 the gradient: [-8.10126406]\n",
      "iter_count:  489 the loss: 223347.8032476131 the gradient: [-15.57631782]\n",
      "iter_count:  490 the loss: 218227.96745687997 the gradient: [-0.51141825]\n",
      "iter_count:  491 the loss: 66826204.63988931 the gradient: [-3281.24160184]\n",
      "iter_count:  492 the loss: 241447.68760600628 the gradient: [-65.52870978]\n",
      "iter_count:  493 the loss: 307100.4300598437 the gradient: [-212.20977008]\n",
      "iter_count:  494 the loss: 395828.2502121117 the gradient: [-354.93562745]\n",
      "iter_count:  495 the loss: 363248.04000743444 the gradient: [-307.87499592]\n",
      "iter_count:  496 the loss: 383139.98753101594 the gradient: [-337.22186505]\n",
      "iter_count:  497 the loss: 226941.60421897925 the gradient: [-25.89569039]\n",
      "iter_count:  498 the loss: 300328.48451489845 the gradient: [-199.10446283]\n",
      "iter_count:  499 the loss: 233822.29203731145 the gradient: [-45.08788395]\n",
      "iter_count:  500 the loss: 232633.1272551765 the gradient: [-41.82281723]\n",
      "iter_count:  501 the loss: 223800.2479537469 the gradient: [-16.88691195]\n",
      "iter_count:  502 the loss: 254660.56380888555 the gradient: [-99.02932157]\n",
      "iter_count:  503 the loss: 260391.91934327365 the gradient: [-112.85666039]\n",
      "iter_count:  504 the loss: 251174.47960368966 the gradient: [-90.41596082]\n",
      "iter_count:  505 the loss: 591628.756439354 the gradient: [-562.68571623]\n",
      "iter_count:  506 the loss: 242684.4999261965 the gradient: [-68.76534324]\n",
      "iter_count:  507 the loss: 226345.54725024092 the gradient: [-24.19842744]\n",
      "iter_count:  508 the loss: 281904.4412324289 the gradient: [-161.35107162]\n",
      "iter_count:  509 the loss: 218818.52497279982 the gradient: [-2.2713392]\n",
      "iter_count:  510 the loss: 389480.9551618975 the gradient: [-346.16665059]\n",
      "iter_count:  511 the loss: 229425.616728159 the gradient: [-32.90866167]\n",
      "iter_count:  512 the loss: 248386.48173547818 the gradient: [-83.41330473]\n",
      "iter_count:  513 the loss: 227674.24399923327 the gradient: [-27.97417577]\n",
      "iter_count:  514 the loss: 252185.03424150392 the gradient: [-92.9289756]\n",
      "iter_count:  515 the loss: 400573.8886585778 the gradient: [-361.3755563]\n",
      "iter_count:  516 the loss: 1179191.52402459 the gradient: [-896.21672911]\n",
      "iter_count:  517 the loss: 218599.45289366145 the gradient: [-1.61916633]\n",
      "iter_count:  518 the loss: 298397.4686998337 the gradient: [-195.29485531]\n",
      "iter_count:  519 the loss: 237837.44166238807 the gradient: [-55.9565339]\n",
      "iter_count:  520 the loss: 253604.88354360376 the gradient: [-96.43746635]\n",
      "iter_count:  521 the loss: 235406.03826807297 the gradient: [-49.40342371]\n",
      "iter_count:  522 the loss: 240171.04103468603 the gradient: [-62.1651509]\n",
      "iter_count:  523 the loss: 352120.62453402666 the gradient: [-290.53223874]\n",
      "iter_count:  524 the loss: 237516.40700927086 the gradient: [-55.09624532]\n",
      "iter_count:  525 the loss: 217414.14287989668 the gradient: [1.92349394]\n",
      "iter_count:  526 the loss: 221985.95511317736 the gradient: [-11.61135166]\n",
      "iter_count:  527 the loss: 217893.85108962125 the gradient: [0.48688321]\n",
      "iter_count:  528 the loss: 221224.23410027515 the gradient: [-9.38038485]\n",
      "iter_count:  529 the loss: 202859.1248141909 the gradient: [47.42625882]\n",
      "iter_count:  530 the loss: 218069.69930300963 the gradient: [-0.03876584]\n",
      "iter_count:  531 the loss: 280370.1896614192 the gradient: [-158.05847459]\n",
      "iter_count:  532 the loss: 221438.97888686234 the gradient: [-10.01030653]\n",
      "iter_count:  533 the loss: 6033443.637640664 the gradient: [-1720.85436541]\n",
      "iter_count:  534 the loss: 218823.0781775718 the gradient: [-2.28488545]\n",
      "iter_count:  535 the loss: 223634.1984035466 the gradient: [-16.40630259]\n",
      "iter_count:  536 the loss: 338533.52737798076 the gradient: [-268.35608933]\n",
      "iter_count:  537 the loss: 220263.6298751869 the gradient: [-6.55326911]\n",
      "iter_count:  538 the loss: 218219.87263378725 the gradient: [-0.48725409]\n",
      "iter_count:  539 the loss: 697534.4558557156 the gradient: [-643.80010713]\n",
      "iter_count:  540 the loss: 589065074139.5171 the gradient: [-18208.78125505]\n",
      "iter_count:  541 the loss: 218624.75371551904 the gradient: [-1.6945275]\n",
      "iter_count:  542 the loss: 286091.32888856914 the gradient: [-170.21556382]\n",
      "iter_count:  543 the loss: 306639.5343964249 the gradient: [-211.33012924]\n",
      "iter_count:  544 the loss: 244404.89305405994 the gradient: [-73.2319057]\n",
      "iter_count:  545 the loss: 259977.72207665438 the gradient: [-111.87095414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  546 the loss: 228133.82322828865 the gradient: [-29.27368155]\n",
      "iter_count:  547 the loss: 214033.25640910064 the gradient: [12.15975969]\n",
      "iter_count:  548 the loss: 218851.24245540163 the gradient: [-2.36866931]\n",
      "iter_count:  549 the loss: 338841.51069469366 the gradient: [-268.87167033]\n",
      "iter_count:  550 the loss: 4769699.731385006 the gradient: [-1593.66157236]\n",
      "iter_count:  551 the loss: 294742.20925341226 the gradient: [-187.99237169]\n",
      "iter_count:  552 the loss: 237884.8660237041 the gradient: [-56.08349157]\n",
      "iter_count:  553 the loss: 223274.72316464342 the gradient: [-15.36431557]\n",
      "iter_count:  554 the loss: 227369.7732433235 the gradient: [-27.11142581]\n",
      "iter_count:  555 the loss: 293410.0511783301 the gradient: [-185.30075577]\n",
      "iter_count:  556 the loss: 232431.4369553565 the gradient: [-41.26692099]\n",
      "iter_count:  557 the loss: 402836.6469981112 the gradient: [-364.41210831]\n",
      "iter_count:  558 the loss: 221106.785626483 the gradient: [-9.03554568]\n",
      "iter_count:  559 the loss: 239361.13243751795 the gradient: [-60.01924728]\n",
      "iter_count:  560 the loss: 216765.2408564212 the gradient: [3.87300645]\n",
      "iter_count:  561 the loss: 218119.41233555987 the gradient: [-0.18727445]\n",
      "iter_count:  562 the loss: 218823.0781775718 the gradient: [-2.28488545]\n",
      "iter_count:  563 the loss: 330236.1079173388 the gradient: [-254.22804794]\n",
      "iter_count:  564 the loss: 216886.382392448 the gradient: [3.50851478]\n",
      "iter_count:  565 the loss: 288441.081834024 the gradient: [-175.11467431]\n",
      "iter_count:  566 the loss: 250844.12664185872 the gradient: [-89.59155979]\n",
      "iter_count:  567 the loss: 217427.94399297668 the gradient: [1.88210851]\n",
      "iter_count:  568 the loss: 224003.17411717534 the gradient: [-17.47365004]\n",
      "iter_count:  569 the loss: 218718.39864200552 the gradient: [-1.97336525]\n",
      "iter_count:  570 the loss: 246941.74738413247 the gradient: [-79.74380897]\n",
      "iter_count:  571 the loss: 228189.27613203684 the gradient: [-29.43025607]\n",
      "iter_count:  572 the loss: 231507.33143892913 the gradient: [-38.71200731]\n",
      "iter_count:  573 the loss: 261557.77532910267 the gradient: [-115.62002001]\n",
      "iter_count:  574 the loss: 229831.8139704473 the gradient: [-34.04629589]\n",
      "iter_count:  575 the loss: 252572.2947994441 the gradient: [-93.88848772]\n",
      "iter_count:  576 the loss: 218081.62111718475 the gradient: [-0.07438388]\n",
      "iter_count:  577 the loss: 243972.28222580996 the gradient: [-72.11260988]\n",
      "iter_count:  578 the loss: 216859.02867299592 the gradient: [3.59079506]\n",
      "iter_count:  579 the loss: 226693.00357272642 the gradient: [-25.18848839]\n",
      "iter_count:  580 the loss: 223338.9298887237 the gradient: [-15.55058121]\n",
      "iter_count:  581 the loss: 222487.2116481404 the gradient: [-13.07426181]\n",
      "iter_count:  582 the loss: 256906.9838574699 the gradient: [-104.49765039]\n",
      "iter_count:  583 the loss: 249658.6993159461 the gradient: [-86.62150429]\n",
      "iter_count:  584 the loss: 236397.1222020305 the gradient: [-52.08507709]\n",
      "iter_count:  585 the loss: 268512.80000404123 the gradient: [-131.77129888]\n",
      "iter_count:  586 the loss: 2850868.1216507177 the gradient: [-1326.95362909]\n",
      "iter_count:  587 the loss: 222372.01438852557 the gradient: [-12.73842418]\n",
      "iter_count:  588 the loss: 214345.72347065384 the gradient: [11.2054629]\n",
      "iter_count:  589 the loss: 300943.12536544964 the gradient: [-200.31018445]\n",
      "iter_count:  590 the loss: 362453.8865888025 the gradient: [-306.66033717]\n",
      "iter_count:  591 the loss: 264967.9283435157 the gradient: [-123.60978564]\n",
      "iter_count:  592 the loss: 218333.90378732348 the gradient: [-0.82755097]\n",
      "iter_count:  593 the loss: 239688.8116660463 the gradient: [-60.88858604]\n",
      "iter_count:  594 the loss: 295456.3905339413 the gradient: [-189.42866834]\n",
      "iter_count:  595 the loss: 239273.7008771438 the gradient: [-59.78702917]\n",
      "iter_count:  596 the loss: 523888.8419360141 the gradient: [-501.57631694]\n",
      "iter_count:  597 the loss: 221950.4834491611 the gradient: [-11.50767214]\n",
      "iter_count:  598 the loss: 245548.696235344 the gradient: [-76.17884904]\n",
      "iter_count:  599 the loss: 248495.62440926902 the gradient: [-83.68937946]\n",
      "iter_count:  600 the loss: 229746.32043543333 the gradient: [-33.80706705]\n",
      "iter_count:  601 the loss: 218113.59714397372 the gradient: [-0.16990478]\n",
      "iter_count:  602 the loss: 267027.7645838254 the gradient: [-128.36982277]\n",
      "iter_count:  603 the loss: 268281.5740838098 the gradient: [-131.24332716]\n",
      "iter_count:  604 the loss: 253400.4884131926 the gradient: [-95.93399875]\n",
      "iter_count:  605 the loss: 218290.43430230193 the gradient: [-0.69785321]\n",
      "iter_count:  606 the loss: 219036.67939453837 the gradient: [-2.91998008]\n",
      "iter_count:  607 the loss: 250413.9707923137 the gradient: [-88.51595461]\n",
      "iter_count:  608 the loss: 668010.8955513325 the gradient: [-622.64777055]\n",
      "iter_count:  609 the loss: 227593.15961594405 the gradient: [-27.74455721]\n",
      "iter_count:  610 the loss: 251109.16340187055 the gradient: [-90.25307647]\n",
      "iter_count:  611 the loss: 203977.62250524326 the gradient: [43.7940751]\n",
      "iter_count:  612 the loss: 218882.02186079766 the gradient: [-2.46021751]\n",
      "iter_count:  613 the loss: 221283.84894630327 the gradient: [-9.55533229]\n",
      "iter_count:  614 the loss: 775972.977654733 the gradient: [-695.56218524]\n",
      "iter_count:  615 the loss: 386029.63609785936 the gradient: [-341.32146698]\n",
      "iter_count:  616 the loss: 290431.4925153284 the gradient: [-179.22292099]\n",
      "iter_count:  617 the loss: 218925.95085880277 the gradient: [-2.59084945]\n",
      "iter_count:  618 the loss: 255813.35856292295 the gradient: [-101.84343784]\n",
      "iter_count:  619 the loss: 310930.11139715347 the gradient: [-219.45132308]\n",
      "iter_count:  620 the loss: 233971.16341487673 the gradient: [-45.49513841]\n",
      "iter_count:  621 the loss: 297393.4975766016 the gradient: [-193.30110187]\n",
      "iter_count:  622 the loss: 218917.62454717146 the gradient: [-2.56609193]\n",
      "iter_count:  623 the loss: 237302.2303194136 the gradient: [-54.52147247]\n",
      "iter_count:  624 the loss: 400838.32024896605 the gradient: [-361.73153825]\n",
      "iter_count:  625 the loss: 219006.37686165405 the gradient: [-2.82992894]\n",
      "iter_count:  626 the loss: 235906.23943200917 the gradient: [-50.75867017]\n",
      "iter_count:  627 the loss: 228118.82976702484 the gradient: [-29.23133835]\n",
      "iter_count:  628 the loss: 217642.57902749485 the gradient: [1.23889712]\n",
      "iter_count:  629 the loss: 295040.8784562212 the gradient: [-188.59359469]\n",
      "iter_count:  630 the loss: 222277.61899857342 the gradient: [-12.46306899]\n",
      "iter_count:  631 the loss: 232495.04804577515 the gradient: [-41.44231164]\n",
      "iter_count:  632 the loss: 266910.11019741086 the gradient: [-128.09925691]\n",
      "iter_count:  633 the loss: 281337.6029963347 the gradient: [-160.1374042]\n",
      "iter_count:  634 the loss: 266242.86878634396 the gradient: [-126.56181061]\n",
      "iter_count:  635 the loss: 486328.72544280684 the gradient: [-463.53678883]\n",
      "iter_count:  636 the loss: 226219.45315701418 the gradient: [-23.83865405]\n",
      "iter_count:  637 the loss: 223179.70693759012 the gradient: [-15.0885477]\n",
      "iter_count:  638 the loss: 219783.0620686294 the gradient: [-5.13318516]\n",
      "iter_count:  639 the loss: 217883.07976839622 the gradient: [0.51909803]\n",
      "iter_count:  640 the loss: 251716.44568440193 the gradient: [-91.76535753]\n",
      "iter_count:  641 the loss: 703049.7425196595 the gradient: [-647.64221464]\n",
      "iter_count:  642 the loss: 222602.95955419194 the gradient: [-13.41148633]\n",
      "iter_count:  643 the loss: 223868.23071649796 the gradient: [-17.08355063]\n",
      "iter_count:  644 the loss: 524544.6132096569 the gradient: [-502.21138026]\n",
      "iter_count:  645 the loss: 402836.6469981112 the gradient: [-364.41210831]\n",
      "iter_count:  646 the loss: 307568.65173338907 the gradient: [-213.10158815]\n",
      "iter_count:  647 the loss: 218886.88333450066 the gradient: [-2.47467569]\n",
      "iter_count:  648 the loss: 220226.00812070046 the gradient: [-6.44223489]\n",
      "iter_count:  649 the loss: 593730.9112664361 the gradient: [-564.45085458]\n",
      "iter_count:  650 the loss: 244635.37436214497 the gradient: [-73.82717539]\n",
      "iter_count:  651 the loss: 26759627852.15617 the gradient: [-10646.14143015]\n",
      "iter_count:  652 the loss: 251509.74262962633 the gradient: [-91.25115648]\n",
      "iter_count:  653 the loss: 218040.56192037693 the gradient: [0.04829615]\n",
      "iter_count:  654 the loss: 218105.63727382527 the gradient: [-0.14612814]\n",
      "iter_count:  655 the loss: 241399.58825228648 the gradient: [-65.40240237]\n",
      "iter_count:  656 the loss: 237791.37707046722 the gradient: [-55.8331851]\n",
      "iter_count:  657 the loss: 232694.27329196903 the gradient: [-41.99122541]\n",
      "iter_count:  658 the loss: 229862.54162490086 the gradient: [-34.13225067]\n",
      "iter_count:  659 the loss: 220405.14735060415 the gradient: [-6.97072346]\n",
      "iter_count:  660 the loss: 229627.84146780486 the gradient: [-33.47535094]\n",
      "iter_count:  661 the loss: 218063.32387853597 the gradient: [-0.01971741]\n",
      "iter_count:  662 the loss: 242482.47637132506 the gradient: [-68.23813336]\n",
      "iter_count:  663 the loss: 218535.2488695455 the gradient: [-1.42787957]\n",
      "iter_count:  664 the loss: 244411.62973573542 the gradient: [-73.24931509]\n",
      "iter_count:  665 the loss: 261557.77532910267 the gradient: [-115.62002001]\n",
      "iter_count:  666 the loss: 218144.9971302057 the gradient: [-0.26368812]\n",
      "iter_count:  667 the loss: 216074.1370725836 the gradient: [5.95718883]\n",
      "iter_count:  668 the loss: 223754.81502750004 the gradient: [-16.75545648]\n",
      "iter_count:  669 the loss: 220073.9798125631 the gradient: [-5.99330962]\n",
      "iter_count:  670 the loss: 250066744.96116126 the gradient: [-4390.05330652]\n",
      "iter_count:  671 the loss: 217846.05175482002 the gradient: [0.62985619]\n",
      "iter_count:  672 the loss: 229755.75618077247 the gradient: [-33.8334758]\n",
      "iter_count:  673 the loss: 221903.64725189246 the gradient: [-11.3707438]\n",
      "iter_count:  674 the loss: 229163.41719732332 the gradient: [-32.17296133]\n",
      "iter_count:  675 the loss: 229503.80090455138 the gradient: [-33.12783042]\n",
      "iter_count:  676 the loss: 615311.7981277236 the gradient: [-582.17022294]\n",
      "iter_count:  677 the loss: 221336.73699635488 the gradient: [-9.71049003]\n",
      "iter_count:  678 the loss: 217926.599054472 the gradient: [0.38895278]\n",
      "iter_count:  679 the loss: 231123.46592435325 the gradient: [-37.64688791]\n",
      "iter_count:  680 the loss: 236380.33354224084 the gradient: [-52.03977133]\n",
      "iter_count:  681 the loss: 387289.03467819066 the gradient: [-343.0959006]\n",
      "iter_count:  682 the loss: 330236.1079173388 the gradient: [-254.22804794]\n",
      "iter_count:  683 the loss: 276984.44249912177 the gradient: [-150.70632389]\n",
      "iter_count:  684 the loss: 229228.69699466924 the gradient: [-32.35622847]\n",
      "iter_count:  685 the loss: 445132.3281527389 the gradient: [-417.50595341]\n",
      "iter_count:  686 the loss: 221319.56474518284 the gradient: [-9.66011683]\n",
      "iter_count:  687 the loss: 261039.97929869627 the gradient: [-114.39474264]\n",
      "iter_count:  688 the loss: 237816.29091880567 the gradient: [-55.89990163]\n",
      "iter_count:  689 the loss: 725318.6380176718 the gradient: [-662.82696528]\n",
      "iter_count:  690 the loss: 217093.85292729764 the gradient: [2.88485532]\n",
      "iter_count:  691 the loss: 254609.43778855586 the gradient: [-98.9041267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  692 the loss: 241506.35318995695 the gradient: [-65.6827195]\n",
      "iter_count:  693 the loss: 627065.896547244 the gradient: [-591.5271747]\n",
      "iter_count:  694 the loss: 287004.28366867936 the gradient: [-172.12540992]\n",
      "iter_count:  695 the loss: 214924.21859029628 the gradient: [9.4431545]\n",
      "iter_count:  696 the loss: 226099.8794509667 the gradient: [-23.49725118]\n",
      "iter_count:  697 the loss: 238120.23440912197 the gradient: [-56.71310222]\n",
      "iter_count:  698 the loss: 220427.023752362 the gradient: [-7.03522575]\n",
      "iter_count:  699 the loss: 225408.8880037575 the gradient: [-21.51989226]\n",
      "iter_count:  700 the loss: 222860.76206808453 the gradient: [-14.16179236]\n",
      "iter_count:  701 the loss: 228317.19527367793 the gradient: [-29.79125952]\n",
      "iter_count:  702 the loss: 228189.27613203684 the gradient: [-29.43025607]\n",
      "iter_count:  703 the loss: 229137.97172408042 the gradient: [-32.10150751]\n",
      "iter_count:  704 the loss: 223814.84244160907 the gradient: [-16.9291325]\n",
      "iter_count:  705 the loss: 222487.2116481404 the gradient: [-13.07426181]\n",
      "iter_count:  706 the loss: 220788.96850211962 the gradient: [-8.10126406]\n",
      "iter_count:  707 the loss: 248299.9700562019 the gradient: [-83.19436161]\n",
      "iter_count:  708 the loss: 225839.8322816341 the gradient: [-22.75398735]\n",
      "iter_count:  709 the loss: 221981.67261178925 the gradient: [-11.5988355]\n",
      "iter_count:  710 the loss: 231778.14074245183 the gradient: [-39.46207263]\n",
      "iter_count:  711 the loss: 338195.959050317 the gradient: [-267.79027013]\n",
      "iter_count:  712 the loss: 235503.56645035234 the gradient: [-49.6679575]\n",
      "iter_count:  713 the loss: 277974.09984970046 the gradient: [-152.86775817]\n",
      "iter_count:  714 the loss: 66826204.63988931 the gradient: [-3281.24160184]\n",
      "iter_count:  715 the loss: 302062.38005315745 the gradient: [-202.49734854]\n",
      "iter_count:  716 the loss: 239688.8116660463 the gradient: [-60.88858604]\n",
      "iter_count:  717 the loss: 218851.24245540163 the gradient: [-2.36866931]\n",
      "iter_count:  718 the loss: 217103.72938001892 the gradient: [2.85518479]\n",
      "iter_count:  719 the loss: 273113.71955170424 the gradient: [-142.15228224]\n",
      "iter_count:  720 the loss: 236347.52415117918 the gradient: [-51.95122016]\n",
      "iter_count:  721 the loss: 223422.32167951675 the gradient: [-15.79240336]\n",
      "iter_count:  722 the loss: 399732.0461305187 the gradient: [-360.24026274]\n",
      "iter_count:  723 the loss: 224189.5037846017 the gradient: [-18.01181507]\n",
      "iter_count:  724 the loss: 218219.87263378725 the gradient: [-0.48725409]\n",
      "iter_count:  725 the loss: 217002.34755946003 the gradient: [3.15983163]\n",
      "iter_count:  726 the loss: 238388.99316331084 the gradient: [-57.43104904]\n",
      "iter_count:  727 the loss: 219610.96512799503 the gradient: [-4.62370137]\n",
      "iter_count:  728 the loss: 355486.4548952437 the gradient: [-295.85276521]\n",
      "iter_count:  729 the loss: 324180.137191621 the gradient: [-243.61635832]\n",
      "iter_count:  730 the loss: 236617.5467413411 the gradient: [-52.67952886]\n",
      "iter_count:  731 the loss: 223274.72316464342 the gradient: [-15.36431557]\n",
      "iter_count:  732 the loss: 238377.61622289006 the gradient: [-57.40067855]\n",
      "iter_count:  733 the loss: 218381.73134114826 the gradient: [-0.97021492]\n",
      "iter_count:  734 the loss: 374259.8385966985 the gradient: [-324.37020223]\n",
      "iter_count:  735 the loss: 228476.62185564465 the gradient: [-30.24082233]\n",
      "iter_count:  736 the loss: 228614.72081939955 the gradient: [-30.62992275]\n",
      "iter_count:  737 the loss: 218066.00331625325 the gradient: [-0.02772309]\n",
      "iter_count:  738 the loss: 662621.0691110075 the gradient: [-618.67466854]\n",
      "iter_count:  739 the loss: 256643.72481143518 the gradient: [-103.86009308]\n",
      "iter_count:  740 the loss: 224248.69901919624 the gradient: [-18.18266785]\n",
      "iter_count:  741 the loss: 220741.14159333816 the gradient: [-7.96052354]\n",
      "iter_count:  742 the loss: 230201.65542131528 the gradient: [-35.07988778]\n",
      "iter_count:  743 the loss: 219380.42201313257 the gradient: [-3.94041634]\n",
      "iter_count:  744 the loss: 268531.0437385935 the gradient: [-131.81293005]\n",
      "iter_count:  745 the loss: 238888.2928554204 the gradient: [-58.7620745]\n",
      "iter_count:  746 the loss: 272844.82437643665 the gradient: [-141.55202375]\n",
      "iter_count:  747 the loss: 500731.7572552794 the gradient: [-478.52573992]\n",
      "iter_count:  748 the loss: 383725.4275853337 the gradient: [-338.05564078]\n",
      "iter_count:  749 the loss: 250413.9707923137 the gradient: [-88.51595461]\n",
      "iter_count:  750 the loss: 229765.43246674404 the gradient: [-33.86055634]\n",
      "iter_count:  751 the loss: 233731.69106415717 the gradient: [-44.83987197]\n",
      "iter_count:  752 the loss: 241670.2720295199 the gradient: [-66.11278389]\n",
      "iter_count:  753 the loss: 227812.42316423258 the gradient: [-28.36523978]\n",
      "iter_count:  754 the loss: 224606.32664351622 the gradient: [-19.21367221]\n",
      "iter_count:  755 the loss: 219198.92183211193 the gradient: [-3.40186024]\n",
      "iter_count:  756 the loss: 249686.17949907848 the gradient: [-86.690565]\n",
      "iter_count:  757 the loss: 237419.23262497858 the gradient: [-54.83554735]\n",
      "iter_count:  758 the loss: 515280.47386924404 the gradient: [-493.15152409]\n",
      "iter_count:  759 the loss: 217932.27046482047 the gradient: [0.37199467]\n",
      "iter_count:  760 the loss: 303132.58999562234 the gradient: [-204.57855484]\n",
      "iter_count:  761 the loss: 237182.10679340368 the gradient: [-54.19881112]\n",
      "iter_count:  762 the loss: 232754.62545944835 the gradient: [-42.15739161]\n",
      "iter_count:  763 the loss: 6529299.201242425 the gradient: [-1764.43856667]\n",
      "iter_count:  764 the loss: 277104.39211192273 the gradient: [-150.96884668]\n",
      "iter_count:  765 the loss: 227620.59356539964 the gradient: [-27.82225754]\n",
      "iter_count:  766 the loss: 236740.19046608114 the gradient: [-53.00997148]\n",
      "iter_count:  767 the loss: 240950.51131308306 the gradient: [-64.22155992]\n",
      "iter_count:  768 the loss: 260785.2241219653 the gradient: [-113.79072147]\n",
      "iter_count:  769 the loss: 202859.1248141909 the gradient: [47.42625882]\n",
      "iter_count:  770 the loss: 236483.95776105684 the gradient: [-52.31934476]\n",
      "iter_count:  771 the loss: 266742.42638948397 the gradient: [-127.71336512]\n",
      "iter_count:  772 the loss: 257671.28298215938 the gradient: [-106.34372815]\n",
      "iter_count:  773 the loss: 219511.6759817226 the gradient: [-4.32953642]\n",
      "iter_count:  774 the loss: 302654.3756580522 the gradient: [-203.64980219]\n",
      "iter_count:  775 the loss: 228613.23588182835 the gradient: [-30.62574046]\n",
      "iter_count:  776 the loss: 1008174.086068907 the gradient: [-821.31977157]\n",
      "iter_count:  777 the loss: 226099.8794509667 the gradient: [-23.49725118]\n",
      "iter_count:  778 the loss: 217103.72938001892 the gradient: [2.85518479]\n",
      "iter_count:  779 the loss: 245458.2486953887 the gradient: [-75.94646997]\n",
      "iter_count:  780 the loss: 372822.34113499906 the gradient: [-322.25280351]\n",
      "iter_count:  781 the loss: 218467.74802713082 the gradient: [-1.22669568]\n",
      "iter_count:  782 the loss: 219126.30186685384 the gradient: [-3.18622442]\n",
      "iter_count:  783 the loss: 589065074139.5171 the gradient: [-18208.78125505]\n",
      "iter_count:  784 the loss: 2754964.504169525 the gradient: [-1309.72750709]\n",
      "iter_count:  785 the loss: 218561.5895041391 the gradient: [-1.50636612]\n",
      "iter_count:  786 the loss: 229457.65693784895 the gradient: [-32.99848943]\n",
      "iter_count:  787 the loss: 254323.6528470624 the gradient: [-98.20369831]\n",
      "iter_count:  788 the loss: 221366.95862106426 the gradient: [-9.79913055]\n",
      "iter_count:  789 the loss: 220509.0609208839 the gradient: [-7.27704062]\n",
      "iter_count:  790 the loss: 341949.27818246663 the gradient: [-274.03991282]\n",
      "iter_count:  791 the loss: 338195.959050317 the gradient: [-267.79027013]\n",
      "iter_count:  792 the loss: 258908.06776525974 the gradient: [-109.3157207]\n",
      "iter_count:  793 the loss: 232705.6496513938 the gradient: [-42.02255189]\n",
      "iter_count:  794 the loss: 601157.1056434258 the gradient: [-570.62982704]\n",
      "iter_count:  795 the loss: 247131.55783941262 the gradient: [-80.22751456]\n",
      "iter_count:  796 the loss: 331643.1463474534 the gradient: [-256.65667238]\n",
      "iter_count:  797 the loss: 236547.15397302006 the gradient: [-52.48976778]\n",
      "iter_count:  798 the loss: 218205.21204795764 the gradient: [-0.44348743]\n",
      "iter_count:  799 the loss: 344088.61890295724 the gradient: [-277.56183872]\n",
      "iter_count:  800 the loss: 217596.61855714076 the gradient: [1.37656437]\n",
      "iter_count:  801 the loss: 222395.0345579289 the gradient: [-12.80555284]\n",
      "iter_count:  802 the loss: 221628.996017903 the gradient: [-10.56706003]\n",
      "iter_count:  803 the loss: 231537.75859286907 the gradient: [-38.79633777]\n",
      "iter_count:  804 the loss: 219687.9108763719 the gradient: [-4.85155618]\n",
      "iter_count:  805 the loss: 217277.3991132931 the gradient: [2.33372222]\n",
      "iter_count:  806 the loss: 283184.3359020369 the gradient: [-164.0794922]\n",
      "iter_count:  807 the loss: 218069.69930300963 the gradient: [-0.03876584]\n",
      "iter_count:  808 the loss: 231385.0494730554 the gradient: [-38.37295373]\n",
      "iter_count:  809 the loss: 258116.3711263684 the gradient: [-107.41544864]\n",
      "iter_count:  810 the loss: 268281.5740838098 the gradient: [-131.24332716]\n",
      "iter_count:  811 the loss: 233813.20832857408 the gradient: [-45.06302368]\n",
      "iter_count:  812 the loss: 230371.8743976988 the gradient: [-35.55488727]\n",
      "iter_count:  813 the loss: 226684.14516192954 the gradient: [-25.16327053]\n",
      "iter_count:  814 the loss: 232382.34157000977 the gradient: [-41.13151155]\n",
      "iter_count:  815 the loss: 217093.85292729764 the gradient: [2.88485532]\n",
      "iter_count:  816 the loss: 268314.9718047321 the gradient: [-131.31962364]\n",
      "iter_count:  817 the loss: 249366.83191611688 the gradient: [-85.88739245]\n",
      "iter_count:  818 the loss: 285449.6805294067 the gradient: [-168.86838031]\n",
      "iter_count:  819 the loss: 244884.16587667243 the gradient: [-74.46891373]\n",
      "iter_count:  820 the loss: 2009755.109540203 the gradient: [-1153.40160678]\n",
      "iter_count:  821 the loss: 222461.8276374687 the gradient: [-13.00027777]\n",
      "iter_count:  822 the loss: 218286.21194813336 the gradient: [-0.68525349]\n",
      "iter_count:  823 the loss: 241993.68361345472 the gradient: [-66.96018863]\n",
      "iter_count:  824 the loss: 249750217.1037796 the gradient: [-4388.88523138]\n",
      "iter_count:  825 the loss: 222892.69020386986 the gradient: [-14.25464028]\n",
      "iter_count:  826 the loss: 2773987.5102584055 the gradient: [-1313.18704757]\n",
      "iter_count:  827 the loss: 208008.99313447572 the gradient: [30.89354353]\n",
      "iter_count:  828 the loss: 226728.25984923597 the gradient: [-25.28884256]\n",
      "iter_count:  829 the loss: 220506.66833846082 the gradient: [-7.26998976]\n",
      "iter_count:  830 the loss: 255235.3706421063 the gradient: [-100.43459772]\n",
      "iter_count:  831 the loss: 218624.75371551904 the gradient: [-1.6945275]\n",
      "iter_count:  832 the loss: 338841.51069469366 the gradient: [-268.87167033]\n",
      "iter_count:  833 the loss: 238593.59184389818 the gradient: [-57.97690087]\n",
      "iter_count:  834 the loss: 260693.42786303934 the gradient: [-113.57288162]\n",
      "iter_count:  835 the loss: 218823.0781775718 the gradient: [-2.28488545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  836 the loss: 317067.5809526963 the gradient: [-230.81184501]\n",
      "iter_count:  837 the loss: 222272.49549397934 the gradient: [-12.44811935]\n",
      "iter_count:  838 the loss: 266329.6182249174 the gradient: [-126.76198751]\n",
      "iter_count:  839 the loss: 229329.96411122935 the gradient: [-32.64039569]\n",
      "iter_count:  840 the loss: 265825.71321409766 the gradient: [-125.59799631]\n",
      "iter_count:  841 the loss: 219774.06129094306 the gradient: [-5.10655107]\n",
      "iter_count:  842 the loss: 222958.15341057503 the gradient: [-14.4449572]\n",
      "iter_count:  843 the loss: 250748.83195038966 the gradient: [-89.35348473]\n",
      "iter_count:  844 the loss: 322884.8496235057 the gradient: [-241.31247203]\n",
      "iter_count:  845 the loss: 289255.6801289565 the gradient: [-176.80060013]\n",
      "iter_count:  846 the loss: 244729.71744890945 the gradient: [-74.07062635]\n",
      "iter_count:  847 the loss: 218925.95085880277 the gradient: [-2.59084945]\n",
      "iter_count:  848 the loss: 304684.4178399133 the gradient: [-207.57895239]\n",
      "iter_count:  849 the loss: 221737.55788586082 the gradient: [-10.88488162]\n",
      "iter_count:  850 the loss: 225529.7746179093 the gradient: [-21.86637463]\n",
      "iter_count:  851 the loss: 236740.19046608114 the gradient: [-53.00997148]\n",
      "iter_count:  852 the loss: 219758.46965011815 the gradient: [-5.06041086]\n",
      "iter_count:  853 the loss: 229919.5827892532 the gradient: [-34.29177378]\n",
      "iter_count:  854 the loss: 219492.51952251952 the gradient: [-4.27276245]\n",
      "iter_count:  855 the loss: 219315.68767852138 the gradient: [-3.7483967]\n",
      "iter_count:  856 the loss: 470694.6899054799 the gradient: [-446.64831968]\n",
      "iter_count:  857 the loss: 1739915.0744565576 the gradient: [-1083.18832068]\n",
      "iter_count:  858 the loss: 4769699.731385006 the gradient: [-1593.66157236]\n",
      "iter_count:  859 the loss: 242030.41967133744 the gradient: [-67.05635146]\n",
      "iter_count:  860 the loss: 216583.09791630015 the gradient: [4.42150983]\n",
      "iter_count:  861 the loss: 218810.52498895724 the gradient: [-2.2475376]\n",
      "iter_count:  862 the loss: 262777.48547707015 the gradient: [-118.49355994]\n",
      "iter_count:  863 the loss: 220414.13137682385 the gradient: [-6.99721371]\n",
      "iter_count:  864 the loss: 405308.3444990515 the gradient: [-367.7044143]\n",
      "iter_count:  865 the loss: 229329.96411122935 the gradient: [-32.64039569]\n",
      "iter_count:  866 the loss: 231778.14074245183 the gradient: [-39.46207263]\n",
      "iter_count:  867 the loss: 239132.40772815424 the gradient: [-59.41152233]\n",
      "iter_count:  868 the loss: 274858.1319516692 the gradient: [-146.02726094]\n",
      "iter_count:  869 the loss: 258116.3711263684 the gradient: [-107.41544864]\n",
      "iter_count:  870 the loss: 218305.3479593459 the gradient: [-0.74235394]\n",
      "iter_count:  871 the loss: 218453.8750682995 the gradient: [-1.18533832]\n",
      "iter_count:  872 the loss: 307637.91351886466 the gradient: [-213.23335649]\n",
      "iter_count:  873 the loss: 285245982.1225328 the gradient: [-4512.61744499]\n",
      "iter_count:  874 the loss: 218779.09336835364 the gradient: [-2.15401162]\n",
      "iter_count:  875 the loss: 228023.2013770323 the gradient: [-28.96119027]\n",
      "iter_count:  876 the loss: 228435.8365060386 the gradient: [-30.12585065]\n",
      "iter_count:  877 the loss: 233515.4272078588 the gradient: [-44.24737069]\n",
      "iter_count:  878 the loss: 560798.8717045116 the gradient: [-535.93896223]\n",
      "iter_count:  879 the loss: 217988.8231754356 the gradient: [0.20292591]\n",
      "iter_count:  880 the loss: 217002.34755946003 the gradient: [3.15983163]\n",
      "iter_count:  881 the loss: 286718.6541094582 the gradient: [-171.52876627]\n",
      "iter_count:  882 the loss: 219955.72673339426 the gradient: [-5.64385369]\n",
      "iter_count:  883 the loss: 224734.6815488504 the gradient: [-19.58320429]\n",
      "iter_count:  884 the loss: 235146.79608868656 the gradient: [-48.69957377]\n",
      "iter_count:  885 the loss: 314516.0659878461 the gradient: [-226.12488058]\n",
      "iter_count:  886 the loss: 231530.9707876867 the gradient: [-38.77752624]\n",
      "iter_count:  887 the loss: 319087.2036063261 the gradient: [-234.48627505]\n",
      "iter_count:  888 the loss: 268314.9718047321 the gradient: [-131.31962364]\n",
      "iter_count:  889 the loss: 332896.1561809371 the gradient: [-258.80798108]\n",
      "iter_count:  890 the loss: 215334.26396545026 the gradient: [8.19750078]\n",
      "iter_count:  891 the loss: 286316.511285504 the gradient: [-170.68738981]\n",
      "iter_count:  892 the loss: 217936.15098462143 the gradient: [0.36039183]\n",
      "iter_count:  893 the loss: 257176.69903464062 the gradient: [-105.14994646]\n",
      "iter_count:  894 the loss: 230434.67303040475 the gradient: [-35.73001534]\n",
      "iter_count:  895 the loss: 307402.8612047918 the gradient: [-212.78601586]\n",
      "iter_count:  896 the loss: 243082.72887682894 the gradient: [-69.80290756]\n",
      "iter_count:  897 the loss: 228317.19527367793 the gradient: [-29.79125952]\n",
      "iter_count:  898 the loss: 218887.86250860637 the gradient: [-2.47758774]\n",
      "iter_count:  899 the loss: 272387.4904015167 the gradient: [-140.52929237]\n",
      "iter_count:  900 the loss: 230434.87349534174 the gradient: [-35.73057428]\n",
      "iter_count:  901 the loss: 3706500.7457523653 the gradient: [-1461.08427578]\n",
      "iter_count:  902 the loss: 256162.69794819545 the gradient: [-102.6929093]\n",
      "iter_count:  903 the loss: 256288.07637926334 the gradient: [-102.99741215]\n",
      "iter_count:  904 the loss: 215511.7068438754 the gradient: [7.65935351]\n",
      "iter_count:  905 the loss: 238377.61622289006 the gradient: [-57.40067855]\n",
      "iter_count:  906 the loss: 220548.8017206896 the gradient: [-7.39414151]\n",
      "iter_count:  907 the loss: 238865.06050774615 the gradient: [-58.70022195]\n",
      "iter_count:  908 the loss: 239889.07499848542 the gradient: [-61.41913015]\n",
      "iter_count:  909 the loss: 290944.70825856674 the gradient: [-180.27609363]\n",
      "iter_count:  910 the loss: 223800.2479537469 the gradient: [-16.88691195]\n",
      "iter_count:  911 the loss: 230155.5195044358 the gradient: [-34.95106758]\n",
      "iter_count:  912 the loss: 236380.33354224084 the gradient: [-52.03977133]\n",
      "iter_count:  913 the loss: 216663.99774634582 the gradient: [4.17781916]\n",
      "iter_count:  914 the loss: 286301.4700324801 the gradient: [-170.65588929]\n",
      "iter_count:  915 the loss: 232309.8587444085 the gradient: [-40.93153061]\n",
      "iter_count:  916 the loss: 359419.9457198835 the gradient: [-301.98781493]\n",
      "iter_count:  917 the loss: 217763.94300343253 the gradient: [0.87554244]\n",
      "iter_count:  918 the loss: 254177.43496008136 the gradient: [-97.84493163]\n",
      "iter_count:  919 the loss: 219396.49434738964 the gradient: [-3.98808041]\n",
      "iter_count:  920 the loss: 269811.29137181886 the gradient: [-134.72499045]\n",
      "iter_count:  921 the loss: 202696.82352138212 the gradient: [47.95524369]\n",
      "iter_count:  922 the loss: 1562523.2696235296 the gradient: [-1031.21259334]\n",
      "iter_count:  923 the loss: 224766.55263620664 the gradient: [-19.67491968]\n",
      "iter_count:  924 the loss: 233664.9101158522 the gradient: [-44.65698624]\n",
      "iter_count:  925 the loss: 221795.63153229188 the gradient: [-11.0548163]\n",
      "iter_count:  926 the loss: 218364.98252451918 the gradient: [-0.92025956]\n",
      "iter_count:  927 the loss: 298397.4686998337 the gradient: [-195.29485531]\n",
      "iter_count:  928 the loss: 224336.03706982368 the gradient: [-18.43464487]\n",
      "iter_count:  929 the loss: 231085.61706626043 the gradient: [-37.54174574]\n",
      "iter_count:  930 the loss: 250413.9707923137 the gradient: [-88.51595461]\n",
      "iter_count:  931 the loss: 213570.2125430341 the gradient: [13.57704366]\n",
      "iter_count:  932 the loss: 286316.511285504 the gradient: [-170.68738981]\n",
      "iter_count:  933 the loss: 259476.6388554326 the gradient: [-110.6756866]\n",
      "iter_count:  934 the loss: 218946.8243805619 the gradient: [-2.65290984]\n",
      "iter_count:  935 the loss: 218076.14190920533 the gradient: [-0.0580143]\n",
      "iter_count:  936 the loss: 219776.10408702266 the gradient: [-5.112596]\n",
      "iter_count:  937 the loss: 256162.69794819545 the gradient: [-102.6929093]\n",
      "iter_count:  938 the loss: 293410.0511783301 the gradient: [-185.30075577]\n",
      "iter_count:  939 the loss: 218819.14565858155 the gradient: [-2.27318583]\n",
      "iter_count:  940 the loss: 218274.98407826098 the gradient: [-0.65174748]\n",
      "iter_count:  941 the loss: 419201.99678484286 the gradient: [-385.75009075]\n",
      "iter_count:  942 the loss: 218364.98252451918 the gradient: [-0.92025956]\n",
      "iter_count:  943 the loss: 308895.20631430583 the gradient: [-215.61844695]\n",
      "iter_count:  944 the loss: 232759.0481815343 the gradient: [-42.16956641]\n",
      "iter_count:  945 the loss: 249750217.1037796 the gradient: [-4388.88523138]\n",
      "iter_count:  946 the loss: 217984.58974577548 the gradient: [0.21558021]\n",
      "iter_count:  947 the loss: 1050659.1380919644 the gradient: [-841.05597598]\n",
      "iter_count:  948 the loss: 225405.85061510192 the gradient: [-21.51118356]\n",
      "iter_count:  949 the loss: 395828.2502121117 the gradient: [-354.93562745]\n",
      "iter_count:  950 the loss: 229228.69699466924 the gradient: [-32.35622847]\n",
      "iter_count:  951 the loss: 250700.7450878445 the gradient: [-89.23330394]\n",
      "iter_count:  952 the loss: 251716.44568440193 the gradient: [-91.76535753]\n",
      "iter_count:  953 the loss: 261169.52383974163 the gradient: [-114.70159077]\n",
      "iter_count:  954 the loss: 599956.0240977021 the gradient: [-569.63638617]\n",
      "iter_count:  955 the loss: 252185.03424150392 the gradient: [-92.9289756]\n",
      "iter_count:  956 the loss: 3706500.7457523653 the gradient: [-1461.08427578]\n",
      "iter_count:  957 the loss: 505966.83353800146 the gradient: [-483.8462288]\n",
      "iter_count:  958 the loss: 230036.99268657708 the gradient: [-34.61996764]\n",
      "iter_count:  959 the loss: 266718.4116699042 the gradient: [-127.65807342]\n",
      "iter_count:  960 the loss: 282531.00512191834 the gradient: [-162.68882057]\n",
      "iter_count:  961 the loss: 218263.2190558324 the gradient: [-0.61663623]\n",
      "iter_count:  962 the loss: 229425.616728159 the gradient: [-32.90866167]\n",
      "iter_count:  963 the loss: 286502.35367271793 the gradient: [-171.07641207]\n",
      "iter_count:  964 the loss: 223065.24755194146 the gradient: [-14.75615455]\n",
      "iter_count:  965 the loss: 456992.2995254459 the gradient: [-431.2754276]\n",
      "iter_count:  966 the loss: 216773.15298776876 the gradient: [3.84919274]\n",
      "iter_count:  967 the loss: 219492.51952251952 the gradient: [-4.27276245]\n",
      "iter_count:  968 the loss: 2850868.1216507177 the gradient: [-1326.95362909]\n",
      "iter_count:  969 the loss: 217688.79601621252 the gradient: [1.10049754]\n",
      "iter_count:  970 the loss: 268745.7982238069 the gradient: [-132.3027041]\n",
      "iter_count:  971 the loss: 327692.4419019161 the gradient: [-249.80255562]\n",
      "iter_count:  972 the loss: 287004.28366867936 the gradient: [-172.12540992]\n",
      "iter_count:  973 the loss: 221646.02529967076 the gradient: [-10.61692712]\n",
      "iter_count:  974 the loss: 560798.8717045116 the gradient: [-535.93896223]\n",
      "iter_count:  975 the loss: 220276.90076538382 the gradient: [-6.59243028]\n",
      "iter_count:  976 the loss: 221731.5000320942 the gradient: [-10.86715198]\n",
      "iter_count:  977 the loss: 219126.30186685384 the gradient: [-3.18622442]\n",
      "iter_count:  978 the loss: 229755.75618077247 the gradient: [-33.8334758]\n",
      "iter_count:  979 the loss: 205572.01794803626 the gradient: [38.65643044]\n",
      "iter_count:  980 the loss: 218567.57220414415 the gradient: [-1.52419101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  981 the loss: 217315.56571992542 the gradient: [2.21919135]\n",
      "iter_count:  982 the loss: 481004.9729514367 the gradient: [-457.86060921]\n",
      "iter_count:  983 the loss: 229919.5827892532 the gradient: [-34.29177378]\n",
      "iter_count:  984 the loss: 218486.27547972344 the gradient: [-1.28192373]\n",
      "iter_count:  985 the loss: 232088.62207697707 the gradient: [-40.32064259]\n",
      "iter_count:  986 the loss: 293675.9436011027 the gradient: [-185.83929579]\n",
      "iter_count:  987 the loss: 262359.0863737803 the gradient: [-117.50984679]\n",
      "iter_count:  988 the loss: 246519.67406699975 the gradient: [-78.66646582]\n",
      "iter_count:  989 the loss: 239688.8116660463 the gradient: [-60.88858604]\n",
      "iter_count:  990 the loss: 251509.74262962633 the gradient: [-91.25115648]\n",
      "iter_count:  991 the loss: 233822.29203731145 the gradient: [-45.08788395]\n",
      "iter_count:  992 the loss: 224023.98838225278 the gradient: [-17.53379449]\n",
      "iter_count:  993 the loss: 271027.5348482586 the gradient: [-137.47442954]\n",
      "iter_count:  994 the loss: 290431.4925153284 the gradient: [-179.22292099]\n",
      "iter_count:  995 the loss: 274441.1210623986 the gradient: [-145.10392396]\n",
      "iter_count:  996 the loss: 224077.55478879073 the gradient: [-17.68854662]\n",
      "iter_count:  997 the loss: 242691.41297884315 the gradient: [-68.78337374]\n",
      "iter_count:  998 the loss: 214340.6794286399 the gradient: [11.22085434]\n",
      "iter_count:  999 the loss: 227225.61349325356 the gradient: [-26.70242439]\n",
      "iter_count:  1000 the loss: 264629.91135983524 the gradient: [-122.82395898]\n",
      "iter_count:  1001 the loss: 218447.80456450352 the gradient: [-1.16724023]\n",
      "iter_count:  1002 the loss: 217584.4370889991 the gradient: [1.413058]\n",
      "iter_count:  1003 the loss: 213108.00216696947 the gradient: [14.99549641]\n",
      "iter_count:  1004 the loss: 310978.69273507985 the gradient: [-219.54241959]\n",
      "iter_count:  1005 the loss: 1732453.6841732878 the gradient: [-1081.10523494]\n",
      "iter_count:  1006 the loss: 264925.7174352938 the gradient: [-123.5117261]\n",
      "iter_count:  1007 the loss: 219026.35583299454 the gradient: [-2.8893029]\n",
      "iter_count:  1008 the loss: 224905.14652255343 the gradient: [-20.0735613]\n",
      "iter_count:  1009 the loss: 227176.62813694277 the gradient: [-26.56337143]\n",
      "iter_count:  1010 the loss: 531205005310.16956 the gradient: [-17889.45840405]\n",
      "iter_count:  1011 the loss: 237471.17202153284 the gradient: [-54.97490671]\n",
      "iter_count:  1012 the loss: 391356.41165855114 the gradient: [-348.77654668]\n",
      "iter_count:  1013 the loss: 224959.7002180454 the gradient: [-20.23039107]\n",
      "iter_count:  1014 the loss: 743546.156609886 the gradient: [-674.88311694]\n",
      "iter_count:  1015 the loss: 223436.2803585901 the gradient: [-15.83287014]\n",
      "iter_count:  1016 the loss: 233738.72308199154 the gradient: [-44.8591259]\n",
      "iter_count:  1017 the loss: 409008.5096852059 the gradient: [-372.5858003]\n",
      "iter_count:  1018 the loss: 2251395.5204473822 the gradient: [-1209.20352333]\n",
      "iter_count:  1019 the loss: 344846.645996058 the gradient: [-278.80284813]\n",
      "iter_count:  1020 the loss: 218029.30394307227 the gradient: [0.08193862]\n",
      "iter_count:  1021 the loss: 225354.11610778308 the gradient: [-21.36282941]\n",
      "iter_count:  1022 the loss: 254323.6528470624 the gradient: [-98.20369831]\n",
      "iter_count:  1023 the loss: 261367.805349494 the gradient: [-115.17086258]\n",
      "iter_count:  1024 the loss: 234258.81332552002 the gradient: [-46.28109638]\n",
      "iter_count:  1025 the loss: 10656572.676819967 the gradient: [-2044.98302635]\n",
      "iter_count:  1026 the loss: 223964.90921491047 the gradient: [-17.36306235]\n",
      "iter_count:  1027 the loss: 218771.12620509084 the gradient: [-2.13030239]\n",
      "iter_count:  1028 the loss: 271137.4805197551 the gradient: [-137.72216033]\n",
      "iter_count:  1029 the loss: 234913.082831394 the gradient: [-48.06417993]\n",
      "iter_count:  1030 the loss: 218119.41233555987 the gradient: [-0.18727445]\n",
      "iter_count:  1031 the loss: 236686.84587411402 the gradient: [-52.8662707]\n",
      "iter_count:  1032 the loss: 1040602.790015356 the gradient: [-836.45830763]\n",
      "iter_count:  1033 the loss: 245904.48467583864 the gradient: [-77.09186042]\n",
      "iter_count:  1034 the loss: 236686.84587411402 the gradient: [-52.8662707]\n",
      "iter_count:  1035 the loss: 220519.75672020222 the gradient: [-7.30855961]\n",
      "iter_count:  1036 the loss: 261557.77532910267 the gradient: [-115.62002001]\n",
      "iter_count:  1037 the loss: 237022.08263069455 the gradient: [-53.76864573]\n",
      "iter_count:  1038 the loss: 217847.24232256855 the gradient: [0.6262946]\n",
      "iter_count:  1039 the loss: 226728.25984923597 the gradient: [-25.28884256]\n",
      "iter_count:  1040 the loss: 276750.0395949912 the gradient: [-150.19286817]\n",
      "iter_count:  1041 the loss: 360710.47471331724 the gradient: [-303.98159358]\n",
      "iter_count:  1042 the loss: 260693.42786303934 the gradient: [-113.57288162]\n",
      "iter_count:  1043 the loss: 257971.08126450403 the gradient: [-107.06587699]\n",
      "iter_count:  1044 the loss: 232661.68585543238 the gradient: [-41.90148027]\n",
      "iter_count:  1045 the loss: 743546.156609886 the gradient: [-674.88311694]\n",
      "iter_count:  1046 the loss: 225010.02385217912 the gradient: [-20.375018]\n",
      "iter_count:  1047 the loss: 223422.32167951675 the gradient: [-15.79240336]\n",
      "iter_count:  1048 the loss: 220040.32340637327 the gradient: [-5.89387347]\n",
      "iter_count:  1049 the loss: 280404.17971731734 the gradient: [-158.13168052]\n",
      "iter_count:  1050 the loss: 264397.04432752886 the gradient: [-122.28180838]\n",
      "iter_count:  1051 the loss: 222395.0345579289 the gradient: [-12.80555284]\n",
      "iter_count:  1052 the loss: 274816.49505642336 the gradient: [-145.9351536]\n",
      "iter_count:  1053 the loss: 218717.6477207295 the gradient: [-1.97112989]\n",
      "iter_count:  1054 the loss: 392893.78522545245 the gradient: [-350.90406804]\n",
      "iter_count:  1055 the loss: 1008174.086068907 the gradient: [-821.31977157]\n",
      "iter_count:  1056 the loss: 254404.26634534876 the gradient: [-98.40137865]\n",
      "iter_count:  1057 the loss: 217277.3991132931 the gradient: [2.33372222]\n",
      "iter_count:  1058 the loss: 220261.9979050455 the gradient: [-6.54845311]\n",
      "iter_count:  1059 the loss: 225690.29061034223 the gradient: [-22.32608086]\n",
      "iter_count:  1060 the loss: 218232.99635718614 the gradient: [-0.52642965]\n",
      "iter_count:  1061 the loss: 402836.6469981112 the gradient: [-364.41210831]\n",
      "iter_count:  1062 the loss: 225744.1494153763 the gradient: [-22.48023642]\n",
      "iter_count:  1063 the loss: 231471.86511406174 the gradient: [-38.61369268]\n",
      "iter_count:  1064 the loss: 245677.36470130284 the gradient: [-76.50923301]\n",
      "iter_count:  1065 the loss: 226107.46337421285 the gradient: [-23.5189113]\n",
      "iter_count:  1066 the loss: 260691.05602506938 the gradient: [-113.56725171]\n",
      "iter_count:  1067 the loss: 247232.17812147544 the gradient: [-80.48373398]\n",
      "iter_count:  1068 the loss: 338464.8896723305 the gradient: [-268.24110172]\n",
      "iter_count:  1069 the loss: 228946.11455376557 the gradient: [-31.56242606]\n",
      "iter_count:  1070 the loss: 237182.10679340368 the gradient: [-54.19881112]\n",
      "iter_count:  1071 the loss: 241447.68760600628 the gradient: [-65.52870978]\n",
      "iter_count:  1072 the loss: 271056.94393421017 the gradient: [-137.54070756]\n",
      "iter_count:  1073 the loss: 218752.11736064235 the gradient: [-2.07373027]\n",
      "iter_count:  1074 the loss: 256564.43814017528 the gradient: [-103.66790798]\n",
      "iter_count:  1075 the loss: 230264.5112018823 the gradient: [-35.25534014]\n",
      "iter_count:  1076 the loss: 218227.96745687997 the gradient: [-0.51141825]\n",
      "iter_count:  1077 the loss: 298397.4686998337 the gradient: [-195.29485531]\n",
      "iter_count:  1078 the loss: 233306.21297801734 the gradient: [-43.67351382]\n",
      "iter_count:  1079 the loss: 317489.14576155675 the gradient: [-231.58139463]\n",
      "iter_count:  1080 the loss: 307402.8612047918 the gradient: [-212.78601586]\n",
      "iter_count:  1081 the loss: 225754.8808024178 the gradient: [-22.51094644]\n",
      "iter_count:  1082 the loss: 225848.33449526594 the gradient: [-22.77830532]\n",
      "iter_count:  1083 the loss: 235503.56645035234 the gradient: [-49.6679575]\n",
      "iter_count:  1084 the loss: 306639.5343964249 the gradient: [-211.33012924]\n",
      "iter_count:  1085 the loss: 219184.03038555564 the gradient: [-3.35764916]\n",
      "iter_count:  1086 the loss: 219418.43396749033 the gradient: [-4.05313751]\n",
      "iter_count:  1087 the loss: 352172.37276367156 the gradient: [-290.61454272]\n",
      "iter_count:  1088 the loss: 229755.75618077247 the gradient: [-33.8334758]\n",
      "iter_count:  1089 the loss: 219049.17591647635 the gradient: [-2.95711196]\n",
      "iter_count:  1090 the loss: 404132.2201440666 the gradient: [-366.14099842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  1091 the loss: 264403.5530857163 the gradient: [-122.29697042]\n",
      "iter_count:  1092 the loss: 234282.4964185293 the gradient: [-46.34575151]\n",
      "iter_count:  1093 the loss: 220795.83317850527 the gradient: [-8.12146168]\n",
      "iter_count:  1094 the loss: 283184.3359020369 the gradient: [-164.0794922]\n",
      "iter_count:  1095 the loss: 232985.6682032304 the gradient: [-42.79300696]\n",
      "iter_count:  1096 the loss: 534462.2959937234 the gradient: [-511.70324597]\n",
      "iter_count:  1097 the loss: 216474.60889776907 the gradient: [4.74848118]\n",
      "iter_count:  1098 the loss: 224633.63308484427 the gradient: [-19.29230929]\n",
      "iter_count:  1099 the loss: 234951.38145726765 the gradient: [-48.16835766]\n",
      "iter_count:  1100 the loss: 292060.94000377995 the gradient: [-182.55814692]\n",
      "iter_count:  1101 the loss: 1562523.2696235296 the gradient: [-1031.21259334]\n",
      "iter_count:  1102 the loss: 221789.95198985448 the gradient: [-11.03819931]\n",
      "iter_count:  1103 the loss: 230626.21917692752 the gradient: [-36.26380961]\n",
      "iter_count:  1104 the loss: 218717.6477207295 the gradient: [-1.97112989]\n",
      "iter_count:  1105 the loss: 243966.1039739507 the gradient: [-72.0966061]\n",
      "iter_count:  1106 the loss: 1139471.3586226464 the gradient: [-879.83886318]\n",
      "iter_count:  1107 the loss: 218083.5371577271 the gradient: [-0.08010809]\n",
      "iter_count:  1108 the loss: 246502.68634026477 the gradient: [-78.62305404]\n",
      "iter_count:  1109 the loss: 262455.6264371936 the gradient: [-117.73701067]\n",
      "iter_count:  1110 the loss: 264629.91135983524 the gradient: [-122.82395898]\n",
      "iter_count:  1111 the loss: 226785.79674807817 the gradient: [-25.45257447]\n",
      "iter_count:  1112 the loss: 257286.43865947457 the gradient: [-105.41508838]\n",
      "iter_count:  1113 the loss: 362556.82108504267 the gradient: [-306.81797088]\n",
      "iter_count:  1114 the loss: 48444113.52398859 the gradient: [-3041.04753813]\n",
      "iter_count:  1115 the loss: 250700.7450878445 the gradient: [-89.23330394]\n",
      "iter_count:  1116 the loss: 263385.2784022642 the gradient: [-119.91885825]\n",
      "iter_count:  1117 the loss: 232208.92154351008 the gradient: [-40.65291082]\n",
      "iter_count:  1118 the loss: 300943.12536544964 the gradient: [-200.31018445]\n",
      "iter_count:  1119 the loss: 218144.9971302057 the gradient: [-0.26368812]\n",
      "iter_count:  1120 the loss: 270362.9283223475 the gradient: [-135.97406734]\n",
      "iter_count:  1121 the loss: 239079.87855070602 the gradient: [-59.27184522]\n",
      "iter_count:  1122 the loss: 221903.64725189246 the gradient: [-11.3707438]\n",
      "iter_count:  1123 the loss: 233323.41630949563 the gradient: [-43.72072597]\n",
      "iter_count:  1124 the loss: 254472.31070846797 the gradient: [-98.56817257]\n",
      "iter_count:  1125 the loss: 229410.49192058278 the gradient: [-32.8662523]\n",
      "iter_count:  1126 the loss: 231948.60857682698 the gradient: [-39.93364682]\n",
      "iter_count:  1127 the loss: 389905.9804812451 the gradient: [-346.75952508]\n",
      "iter_count:  1128 the loss: 231755.97066549998 the gradient: [-39.40070983]\n",
      "iter_count:  1129 the loss: 261169.52383974163 the gradient: [-114.70159077]\n",
      "iter_count:  1130 the loss: 224407.1169639349 the gradient: [-18.63962506]\n",
      "iter_count:  1131 the loss: 217108.9243368577 the gradient: [2.83957893]\n",
      "iter_count:  1132 the loss: 239079.87855070602 the gradient: [-59.27184522]\n",
      "iter_count:  1133 the loss: 236079.5513852238 the gradient: [-51.22737946]\n",
      "iter_count:  1134 the loss: 594351516.3098754 the gradient: [-5241.11687141]\n",
      "iter_count:  1135 the loss: 222717.82483955595 the gradient: [-13.74592311]\n",
      "iter_count:  1136 the loss: 228663.9900862979 the gradient: [-30.76866916]\n",
      "iter_count:  1137 the loss: 255839.76559934072 the gradient: [-101.90770415]\n",
      "iter_count:  1138 the loss: 233738.72308199154 the gradient: [-44.8591259]\n",
      "iter_count:  1139 the loss: 232102.78707235074 the gradient: [-40.35977787]\n",
      "iter_count:  1140 the loss: 223818.426947953 the gradient: [-16.93950163]\n",
      "iter_count:  1141 the loss: 221438.97888686234 the gradient: [-10.01030653]\n",
      "iter_count:  1142 the loss: 232654.50505936705 the gradient: [-41.88170234]\n",
      "iter_count:  1143 the loss: 257286.43865947457 the gradient: [-105.41508838]\n",
      "iter_count:  1144 the loss: 481875229.83277243 the gradient: [-5024.87008964]\n",
      "iter_count:  1145 the loss: 280404.17971731734 the gradient: [-158.13168052]\n",
      "iter_count:  1146 the loss: 218113.59714397372 the gradient: [-0.16990478]\n",
      "iter_count:  1147 the loss: 2850868.1216507177 the gradient: [-1326.95362909]\n",
      "iter_count:  1148 the loss: 307781.34666115185 the gradient: [-213.50610744]\n",
      "iter_count:  1149 the loss: 219776.10408702266 the gradient: [-5.112596]\n",
      "iter_count:  1150 the loss: 223910.8265798955 the gradient: [-17.20672015]\n",
      "iter_count:  1151 the loss: 232102.78707235074 the gradient: [-40.35977787]\n",
      "iter_count:  1152 the loss: 219297.6059070076 the gradient: [-3.69474873]\n",
      "iter_count:  1153 the loss: 218917.62454717146 the gradient: [-2.56609193]\n",
      "iter_count:  1154 the loss: 220913.20149936204 the gradient: [-8.46666843]\n",
      "iter_count:  1155 the loss: 286988.7755879201 the gradient: [-172.09303596]\n",
      "iter_count:  1156 the loss: 218203.5371710685 the gradient: [-0.43848714]\n",
      "iter_count:  1157 the loss: 404132.2201440666 the gradient: [-366.14099842]\n",
      "iter_count:  1158 the loss: 222877.86270682918 the gradient: [-14.21152357]\n",
      "iter_count:  1159 the loss: 218154.19463740085 the gradient: [-0.29115546]\n",
      "iter_count:  1160 the loss: 740671.0516542204 the gradient: [-673.002866]\n",
      "iter_count:  1161 the loss: 512302.4383365708 the gradient: [-490.19806284]\n",
      "iter_count:  1162 the loss: 236349.1176435657 the gradient: [-51.95552129]\n",
      "iter_count:  1163 the loss: 537727.2195238738 the gradient: [-514.78273335]\n",
      "iter_count:  1164 the loss: 256906.9838574699 the gradient: [-104.49765039]\n",
      "iter_count:  1165 the loss: 223174.95959409862 the gradient: [-15.07476553]\n",
      "iter_count:  1166 the loss: 228555.4922465532 the gradient: [-30.46307992]\n",
      "iter_count:  1167 the loss: 245626.16149929268 the gradient: [-76.37778489]\n",
      "iter_count:  1168 the loss: 228663.9900862979 the gradient: [-30.76866916]\n",
      "iter_count:  1169 the loss: 481383.5332742275 the gradient: [-458.26673188]\n",
      "iter_count:  1170 the loss: 326724.016881867 the gradient: [-248.10569272]\n",
      "iter_count:  1171 the loss: 232661.68585543238 the gradient: [-41.90148027]\n",
      "iter_count:  1172 the loss: 285449.6805294067 the gradient: [-168.86838031]\n",
      "iter_count:  1173 the loss: 2009755.109540203 the gradient: [-1153.40160678]\n",
      "iter_count:  1174 the loss: 220282.96461323698 the gradient: [-6.61032316]\n",
      "iter_count:  1175 the loss: 406565125.64001274 the gradient: [-4854.52325741]\n",
      "iter_count:  1176 the loss: 251174.47960368966 the gradient: [-90.41596082]\n",
      "iter_count:  1177 the loss: 216959.80778292308 the gradient: [3.28771335]\n",
      "iter_count:  1178 the loss: 222326.32294804568 the gradient: [-12.60515859]\n",
      "iter_count:  1179 the loss: 283277.2650791271 the gradient: [-164.27695089]\n",
      "iter_count:  1180 the loss: 525551.6562875173 the gradient: [-503.18479938]\n",
      "iter_count:  1181 the loss: 226573.18699201732 the gradient: [-24.84729283]\n",
      "iter_count:  1182 the loss: 236603.62990901602 the gradient: [-52.64201838]\n",
      "iter_count:  1183 the loss: 217998.54125624071 the gradient: [0.17387838]\n",
      "iter_count:  1184 the loss: 13291892.095444323 the gradient: [-2177.66751786]\n",
      "iter_count:  1185 the loss: 214345.72347065384 the gradient: [11.2054629]\n",
      "iter_count:  1186 the loss: 218853.09651741665 the gradient: [-2.37418436]\n",
      "iter_count:  1187 the loss: 218807.65119956608 the gradient: [-2.23898722]\n",
      "iter_count:  1188 the loss: 221438.97888686234 the gradient: [-10.01030653]\n",
      "iter_count:  1189 the loss: 305058.9366437115 the gradient: [-208.30000935]\n",
      "iter_count:  1190 the loss: 241399.58825228648 the gradient: [-65.40240237]\n",
      "iter_count:  1191 the loss: 243560.28653149796 the gradient: [-71.04424135]\n",
      "iter_count:  1192 the loss: 225983.6164832064 the gradient: [-23.16508285]\n",
      "iter_count:  1193 the loss: 233655.92209157968 the gradient: [-44.6323666]\n",
      "iter_count:  1194 the loss: 240169.97448789224 the gradient: [-62.16233119]\n",
      "iter_count:  1195 the loss: 233820.93078540335 the gradient: [-45.08415856]\n",
      "iter_count:  1196 the loss: 219744.61201898803 the gradient: [-5.01939868]\n",
      "iter_count:  1197 the loss: 9391807.46549702 the gradient: [-1970.88331872]\n",
      "iter_count:  1198 the loss: 229410.1992544728 the gradient: [-32.86543164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  1199 the loss: 230434.87349534174 the gradient: [-35.73057428]\n",
      "iter_count:  1200 the loss: 240427.07375753138 the gradient: [-62.84157416]\n",
      "iter_count:  1201 the loss: 237336.74605094918 the gradient: [-54.61414548]\n",
      "iter_count:  1202 the loss: 306090.96805672534 the gradient: [-210.28085855]\n",
      "iter_count:  1203 the loss: 225801.03585189377 the gradient: [-22.64300745]\n",
      "iter_count:  1204 the loss: 475896.3765811493 the gradient: [-452.34192051]\n",
      "iter_count:  1205 the loss: 222577.3440971137 the gradient: [-13.3368761]\n",
      "iter_count:  1206 the loss: 304092.99573518574 the gradient: [-206.43787676]\n",
      "iter_count:  1207 the loss: 356434.5986145424 the gradient: [-297.33966151]\n",
      "iter_count:  1208 the loss: 280885.67796943604 the gradient: [-159.16743039]\n",
      "iter_count:  1209 the loss: 6529299.201242425 the gradient: [-1764.43856667]\n",
      "iter_count:  1210 the loss: 243260.86146449533 the gradient: [-70.26630584]\n",
      "iter_count:  1211 the loss: 308317.96935793734 the gradient: [-214.52503917]\n",
      "iter_count:  1212 the loss: 245940.08696816958 the gradient: [-77.18312661]\n",
      "iter_count:  1213 the loss: 229457.65693784895 the gradient: [-32.99848943]\n",
      "iter_count:  1214 the loss: 305089.89397935924 the gradient: [-208.35955819]\n",
      "iter_count:  1215 the loss: 224248.69901919624 the gradient: [-18.18266785]\n",
      "iter_count:  1216 the loss: 269820.05344209156 the gradient: [-134.74485714]\n",
      "iter_count:  1217 the loss: 359015.75233889837 the gradient: [-301.36144388]\n",
      "iter_count:  1218 the loss: 255235.3706421063 the gradient: [-100.43459772]\n",
      "iter_count:  1219 the loss: 222487.2116481404 the gradient: [-13.07426181]\n",
      "iter_count:  1220 the loss: 254609.43778855586 the gradient: [-98.9041267]\n",
      "iter_count:  1221 the loss: 218403.29918265305 the gradient: [-1.03453665]\n",
      "iter_count:  1222 the loss: 243706.17387525432 the gradient: [-71.42281919]\n",
      "iter_count:  1223 the loss: 239013.5526968648 the gradient: [-59.09542543]\n",
      "iter_count:  1224 the loss: 6033443.637640664 the gradient: [-1720.85436541]\n",
      "iter_count:  1225 the loss: 314962.1109912316 the gradient: [-226.94788393]\n",
      "iter_count:  1226 the loss: 218219.87263378725 the gradient: [-0.48725409]\n",
      "iter_count:  1227 the loss: 260781.82414550017 the gradient: [-113.78265487]\n",
      "iter_count:  1228 the loss: 283957.7088606979 the gradient: [-165.7201387]\n",
      "iter_count:  1229 the loss: 216833.11111161747 the gradient: [3.6687671]\n",
      "iter_count:  1230 the loss: 347428.0237027355 the gradient: [-283.00222228]\n",
      "iter_count:  1231 the loss: 338341.6740442615 the gradient: [-268.03460342]\n",
      "iter_count:  1232 the loss: 279982.92829514097 the gradient: [-157.22357202]\n",
      "iter_count:  1233 the loss: 627065.896547244 the gradient: [-591.5271747]\n",
      "iter_count:  1234 the loss: 218406.44920914614 the gradient: [-1.04393032]\n",
      "iter_count:  1235 the loss: 222706.37216897696 the gradient: [-13.71258768]\n",
      "iter_count:  1236 the loss: 222958.15341057503 the gradient: [-14.4449572]\n",
      "iter_count:  1237 the loss: 290261.3905302031 the gradient: [-178.87330341]\n",
      "iter_count:  1238 the loss: 3706500.7457523653 the gradient: [-1461.08427578]\n",
      "iter_count:  1239 the loss: 232089.11830447626 the gradient: [-40.32201363]\n",
      "iter_count:  1240 the loss: 211179.43164769493 the gradient: [20.95434567]\n",
      "iter_count:  1241 the loss: 278763.03494598076 the gradient: [-154.58344481]\n",
      "iter_count:  1242 the loss: 245940.08696816958 the gradient: [-77.18312661]\n",
      "iter_count:  1243 the loss: 189771.9744992259 the gradient: [91.6949713]\n",
      "iter_count:  1244 the loss: 769207.8949952746 the gradient: [-691.3256753]\n",
      "iter_count:  1245 the loss: 216744.31674856512 the gradient: [3.93598837]\n",
      "iter_count:  1246 the loss: 515280.47386924404 the gradient: [-493.15152409]\n",
      "iter_count:  1247 the loss: 230506.11945623087 the gradient: [-35.92918585]\n",
      "iter_count:  1248 the loss: 232883.60696531125 the gradient: [-42.5123286]\n",
      "iter_count:  1249 the loss: 246214.20599680374 the gradient: [-77.88525]\n",
      "iter_count:  1250 the loss: 290049.10365210753 the gradient: [-178.43659615]\n",
      "iter_count:  1251 the loss: 278407.1466548259 the gradient: [-153.81030513]\n",
      "iter_count:  1252 the loss: 239635.35461870919 the gradient: [-60.74686873]\n",
      "iter_count:  1253 the loss: 224185.4066978224 the gradient: [-17.99998772]\n",
      "iter_count:  1254 the loss: 217274.8821085472 the gradient: [2.34127615]\n",
      "iter_count:  1255 the loss: 363987.728772081 the gradient: [-309.00324767]\n",
      "iter_count:  1256 the loss: 226785.79674807817 the gradient: [-25.45257447]\n",
      "iter_count:  1257 the loss: 218163.36206235646 the gradient: [-0.31853154]\n",
      "iter_count:  1258 the loss: 224697.44316959998 the gradient: [-19.47602277]\n",
      "iter_count:  1259 the loss: 221982.5539378028 the gradient: [-11.60141131]\n",
      "iter_count:  1260 the loss: 221664.01651471847 the gradient: [-10.66960587]\n",
      "iter_count:  1261 the loss: 228189.27613203684 the gradient: [-29.43025607]\n",
      "iter_count:  1262 the loss: 225174.129261102 the gradient: [-20.84636433]\n",
      "iter_count:  1263 the loss: 3706500.7457523653 the gradient: [-1461.08427578]\n",
      "iter_count:  1264 the loss: 274056.3618929355 the gradient: [-144.25033222]\n",
      "iter_count:  1265 the loss: 233515.4272078588 the gradient: [-44.24737069]\n",
      "iter_count:  1266 the loss: 229862.9513179508 the gradient: [-34.13339661]\n",
      "iter_count:  1267 the loss: 305392.93450487347 the gradient: [-208.94205395]\n",
      "iter_count:  1268 the loss: 544206.370982777 the gradient: [-520.8294695]\n",
      "iter_count:  1269 the loss: 276848.34122079774 the gradient: [-150.40826711]\n",
      "iter_count:  1270 the loss: 239013.5526968648 the gradient: [-59.09542543]\n",
      "iter_count:  1271 the loss: 229172.56733516598 the gradient: [-32.19865351]\n",
      "iter_count:  1272 the loss: 260238.89106352947 the gradient: [-112.49272648]\n",
      "iter_count:  1273 the loss: 221646.02529967076 the gradient: [-10.61692712]\n",
      "iter_count:  1274 the loss: 237860.68819661316 the gradient: [-56.01877026]\n",
      "iter_count:  1275 the loss: 263361.9931546976 the gradient: [-119.86433407]\n",
      "iter_count:  1276 the loss: 268020.8914828318 the gradient: [-130.64736671]\n",
      "iter_count:  1277 the loss: 224079.214056108 the gradient: [-17.69333946]\n",
      "iter_count:  1278 the loss: 277243.90078471403 the gradient: [-151.27398535]\n",
      "iter_count:  1279 the loss: 237451.83055674325 the gradient: [-54.92301594]\n",
      "iter_count:  1280 the loss: 324180.137191621 the gradient: [-243.61635832]\n",
      "iter_count:  1281 the loss: 233427.2181889702 the gradient: [-44.00550119]\n",
      "iter_count:  1282 the loss: 224665.33155102283 the gradient: [-19.38357947]\n",
      "iter_count:  1283 the loss: 220132.7175974002 the gradient: [-6.1668023]\n",
      "iter_count:  1284 the loss: 284291.4175303841 the gradient: [-166.42622603]\n",
      "iter_count:  1285 the loss: 214340.6794286399 the gradient: [11.22085434]\n",
      "iter_count:  1286 the loss: 219198.92183211193 the gradient: [-3.40186024]\n",
      "iter_count:  1287 the loss: 245017.15277184756 the gradient: [-74.81159382]\n",
      "iter_count:  1288 the loss: 215973.21716208276 the gradient: [6.2622186]\n",
      "iter_count:  1289 the loss: 237714.7120079174 the gradient: [-55.62782786]\n",
      "iter_count:  1290 the loss: 251001.8855893217 the gradient: [-89.98542793]\n",
      "iter_count:  1291 the loss: 217586.481402036 the gradient: [1.40693341]\n",
      "iter_count:  1292 the loss: 223270.19555820583 the gradient: [-15.35117832]\n",
      "iter_count:  1293 the loss: 2251395.5204473822 the gradient: [-1209.20352333]\n",
      "iter_count:  1294 the loss: 324106.2486688909 the gradient: [-243.48526431]\n",
      "iter_count:  1295 the loss: 305291.6167931149 the gradient: [-208.74739]\n",
      "iter_count:  1296 the loss: 220548.8017206896 the gradient: [-7.39414151]\n",
      "iter_count:  1297 the loss: 226461.47796346544 the gradient: [-24.52897966]\n",
      "iter_count:  1298 the loss: 225848.33449526594 the gradient: [-22.77830532]\n",
      "iter_count:  1299 the loss: 215511.7068438754 the gradient: [7.65935351]\n",
      "iter_count:  1300 the loss: 481004.9729514367 the gradient: [-457.86060921]\n",
      "iter_count:  1301 the loss: 246909.5594720723 the gradient: [-79.66173423]\n",
      "iter_count:  1302 the loss: 235988.30238829847 the gradient: [-50.98065858]\n",
      "iter_count:  1303 the loss: 256444.18343527341 the gradient: [-103.37626925]\n",
      "iter_count:  1304 the loss: 1303771.609548699 the gradient: [-944.26340654]\n",
      "iter_count:  1305 the loss: 9391807.46549702 the gradient: [-1970.88331872]\n",
      "iter_count:  1306 the loss: 273139.84624480427 the gradient: [-142.21056312]\n",
      "iter_count:  1307 the loss: 228050.20716585708 the gradient: [-29.03749556]\n",
      "iter_count:  1308 the loss: 205572.01794803626 the gradient: [38.65643044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  1309 the loss: 4174848.022497596 the gradient: [-1523.1571136]\n",
      "iter_count:  1310 the loss: 220427.023752362 the gradient: [-7.03522575]\n",
      "iter_count:  1311 the loss: 272009.9914716079 the gradient: [-139.68336474]\n",
      "iter_count:  1312 the loss: 266718.4116699042 the gradient: [-127.65807342]\n",
      "iter_count:  1313 the loss: 225744.1494153763 the gradient: [-22.48023642]\n",
      "iter_count:  1314 the loss: 228519.50604523864 the gradient: [-30.36168249]\n",
      "iter_count:  1315 the loss: 220414.13137682385 the gradient: [-6.99721371]\n",
      "iter_count:  1316 the loss: 218771.12620509084 the gradient: [-2.13030239]\n",
      "iter_count:  1317 the loss: 1179191.52402459 the gradient: [-896.21672911]\n",
      "iter_count:  1318 the loss: 250107.1821626928 the gradient: [-87.74734297]\n",
      "iter_count:  1319 the loss: 232758.95257818355 the gradient: [-42.16930324]\n",
      "iter_count:  1320 the loss: 226785.79674807817 the gradient: [-25.45257447]\n",
      "iter_count:  1321 the loss: 219756.00213614522 the gradient: [-5.05310839]\n",
      "iter_count:  1322 the loss: 326724.016881867 the gradient: [-248.10569272]\n",
      "iter_count:  1323 the loss: 750677.2113442768 the gradient: [-679.5129398]\n",
      "iter_count:  1324 the loss: 220427.023752362 the gradient: [-7.03522575]\n",
      "iter_count:  1325 the loss: 332896.1561809371 the gradient: [-258.80798108]\n",
      "iter_count:  1326 the loss: 229270.34431153032 the gradient: [-32.47311494]\n",
      "iter_count:  1327 the loss: 222395.0345579289 the gradient: [-12.80555284]\n",
      "iter_count:  1328 the loss: 222661.28345906033 the gradient: [-13.5813266]\n",
      "iter_count:  1329 the loss: 219418.43396749033 the gradient: [-4.05313751]\n",
      "iter_count:  1330 the loss: 48444113.52398859 the gradient: [-3041.04753813]\n",
      "iter_count:  1331 the loss: 9391807.46549702 the gradient: [-1970.88331872]\n",
      "iter_count:  1332 the loss: 235306.99019843776 the gradient: [-49.13462309]\n",
      "iter_count:  1333 the loss: 505966.83353800146 the gradient: [-483.8462288]\n",
      "iter_count:  1334 the loss: 218087.6246712057 the gradient: [-0.09231941]\n",
      "iter_count:  1335 the loss: 259409.9483641784 the gradient: [-110.5163743]\n",
      "iter_count:  1336 the loss: 225049.16039099396 the gradient: [-20.48746584]\n",
      "iter_count:  1337 the loss: 221903.64725189246 the gradient: [-11.3707438]\n",
      "iter_count:  1338 the loss: 257816.81709567428 the gradient: [-106.69442716]\n",
      "iter_count:  1339 the loss: 218819.14565858155 the gradient: [-2.27318583]\n",
      "iter_count:  1340 the loss: 284291.4175303841 the gradient: [-166.42622603]\n",
      "iter_count:  1341 the loss: 237419.23262497858 the gradient: [-54.83554735]\n",
      "iter_count:  1342 the loss: 245017.15277184756 the gradient: [-74.81159382]\n",
      "iter_count:  1343 the loss: 235406.03826807297 the gradient: [-49.40342371]\n",
      "iter_count:  1344 the loss: 262359.0863737803 the gradient: [-117.50984679]\n",
      "iter_count:  1345 the loss: 44167893.78462955 the gradient: [-2974.03601334]\n",
      "iter_count:  1346 the loss: 218053.0783300917 the gradient: [0.01089558]\n",
      "iter_count:  1347 the loss: 243200.3811048865 the gradient: [-70.10902042]\n",
      "iter_count:  1348 the loss: 219306.36144901012 the gradient: [-3.72072679]\n",
      "iter_count:  1349 the loss: 246847.06788315423 the gradient: [-79.50234927]\n",
      "iter_count:  1350 the loss: 239044.09088233882 the gradient: [-59.17666166]\n",
      "iter_count:  1351 the loss: 224185.4066978224 the gradient: [-17.99998772]\n",
      "iter_count:  1352 the loss: 221631.16156566964 the gradient: [-10.5734017]\n",
      "iter_count:  1353 the loss: 240053.76328346055 the gradient: [-61.85499755]\n",
      "iter_count:  1354 the loss: 353678.9066357523 the gradient: [-293.00372225]\n",
      "iter_count:  1355 the loss: 218680.03643506902 the gradient: [-1.8591555]\n",
      "iter_count:  1356 the loss: 220891.17616093627 the gradient: [-8.4019043]\n",
      "iter_count:  1357 the loss: 11884981.950395824 the gradient: [-2109.99324329]\n",
      "iter_count:  1358 the loss: 225529.7746179093 the gradient: [-21.86637463]\n",
      "iter_count:  1359 the loss: 231952.26716377868 the gradient: [-39.94376291]\n",
      "iter_count:  1360 the loss: 258592.09823128945 the gradient: [-108.55823906]\n",
      "iter_count:  1361 the loss: 228189.27613203684 the gradient: [-29.43025607]\n",
      "iter_count:  1362 the loss: 237565.43589630848 the gradient: [-55.227727]\n",
      "iter_count:  1363 the loss: 234258.81332552002 the gradient: [-46.28109638]\n",
      "iter_count:  1364 the loss: 352120.62453402666 the gradient: [-290.53223874]\n",
      "iter_count:  1365 the loss: 1179191.52402459 the gradient: [-896.21672911]\n",
      "iter_count:  1366 the loss: 246182.4582845308 the gradient: [-77.8039844]\n",
      "iter_count:  1367 the loss: 242691.41297884315 the gradient: [-68.78337374]\n",
      "iter_count:  1368 the loss: 240901.02533434433 the gradient: [-64.09126225]\n",
      "iter_count:  1369 the loss: 286961.3335358034 the gradient: [-172.03574343]\n",
      "iter_count:  1370 the loss: 299176.61268704716 the gradient: [-196.83595186]\n",
      "iter_count:  1371 the loss: 258116.3711263684 the gradient: [-107.41544864]\n",
      "iter_count:  1372 the loss: 221890.50256422965 the gradient: [-11.33230808]\n",
      "iter_count:  1373 the loss: 212325.49641658444 the gradient: [17.40539105]\n",
      "iter_count:  1374 the loss: 234608.3003963663 the gradient: [-47.23434958]\n",
      "iter_count:  1375 the loss: 268512.80000404123 the gradient: [-131.77129888]\n",
      "iter_count:  1376 the loss: 252026.71163953905 the gradient: [-92.53614148]\n",
      "iter_count:  1377 the loss: 225120.01641878686 the gradient: [-20.69098822]\n",
      "iter_count:  1378 the loss: 224481.39406192346 the gradient: [-18.85373836]\n",
      "iter_count:  1379 the loss: 220945.56140975785 the gradient: [-8.56180619]\n",
      "iter_count:  1380 the loss: 218268.66234644572 the gradient: [-0.63288134]\n",
      "iter_count:  1381 the loss: 367277.0565254138 the gradient: [-313.98463101]\n",
      "iter_count:  1382 the loss: 241557.25556859496 the gradient: [-65.81630975]\n",
      "iter_count:  1383 the loss: 218843.19869503586 the gradient: [-2.34474187]\n",
      "iter_count:  1384 the loss: 262479.91559947195 the gradient: [-117.79414686]\n",
      "iter_count:  1385 the loss: 2720153.8683295804 the gradient: [-1303.34045787]\n",
      "iter_count:  1386 the loss: 214924.21859029628 the gradient: [9.4431545]\n",
      "iter_count:  1387 the loss: 249750217.1037796 the gradient: [-4388.88523138]\n",
      "iter_count:  1388 the loss: 218144.9971302057 the gradient: [-0.26368812]\n",
      "iter_count:  1389 the loss: 233813.20832857408 the gradient: [-45.06302368]\n",
      "iter_count:  1390 the loss: 233731.69106415717 the gradient: [-44.83987197]\n",
      "iter_count:  1391 the loss: 229862.9513179508 the gradient: [-34.13339661]\n",
      "iter_count:  1392 the loss: 245017.15277184756 the gradient: [-74.81159382]\n",
      "iter_count:  1393 the loss: 593730.9112664361 the gradient: [-564.45085458]\n",
      "iter_count:  1394 the loss: 401670.06399327406 the gradient: [-362.84930252]\n",
      "iter_count:  1395 the loss: 348184.53910575336 the gradient: [-284.22516033]\n",
      "iter_count:  1396 the loss: 235806.31399932803 the gradient: [-50.4882277]\n",
      "iter_count:  1397 the loss: 758448.2392772831 the gradient: [-684.50452904]\n",
      "iter_count:  1398 the loss: 237182.10679340368 the gradient: [-54.19881112]\n",
      "iter_count:  1399 the loss: 289874.4851309036 the gradient: [-178.07705739]\n",
      "iter_count:  1400 the loss: 237302.2303194136 the gradient: [-54.52147247]\n",
      "iter_count:  1401 the loss: 230218.96094753148 the gradient: [-35.12819961]\n",
      "iter_count:  1402 the loss: 354457.84073045786 the gradient: [-294.23380106]\n",
      "iter_count:  1403 the loss: 235530.9481367977 the gradient: [-49.74220178]\n",
      "iter_count:  1404 the loss: 421432300.11570334 the gradient: [-4890.17094054]\n",
      "iter_count:  1405 the loss: 218606.91870310504 the gradient: [-1.64140515]\n",
      "iter_count:  1406 the loss: 244431.89143230268 the gradient: [-73.30167297]\n",
      "iter_count:  1407 the loss: 594351516.3098754 the gradient: [-5241.11687141]\n",
      "iter_count:  1408 the loss: 456105.2509615587 the gradient: [-430.26084231]\n",
      "iter_count:  1409 the loss: 290311.46182825236 the gradient: [-178.97624551]\n",
      "iter_count:  1410 the loss: 237302.2303194136 the gradient: [-54.52147247]\n",
      "iter_count:  1411 the loss: 257154.6907953727 the gradient: [-105.09675432]\n",
      "iter_count:  1412 the loss: 235806.31399932803 the gradient: [-50.4882277]\n",
      "iter_count:  1413 the loss: 7340651.630058616 the gradient: [-1829.88762836]\n",
      "iter_count:  1414 the loss: 218398.43464445652 the gradient: [-1.02002983]\n",
      "iter_count:  1415 the loss: 310978.69273507985 the gradient: [-219.54241959]\n",
      "iter_count:  1416 the loss: 229068.47345664224 the gradient: [-31.90629704]\n",
      "iter_count:  1417 the loss: 236380.33354224084 the gradient: [-52.03977133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  1418 the loss: 527674.174099224 the gradient: [-505.22925764]\n",
      "iter_count:  1419 the loss: 218011.35539358 the gradient: [0.13557911]\n",
      "iter_count:  1420 the loss: 229410.49192058278 the gradient: [-32.8662523]\n",
      "iter_count:  1421 the loss: 225461.63346962814 the gradient: [-21.67109881]\n",
      "iter_count:  1422 the loss: 290144.22856705525 the gradient: [-178.63233591]\n",
      "iter_count:  1423 the loss: 220282.96461323698 the gradient: [-6.61032316]\n",
      "iter_count:  1424 the loss: 327692.4419019161 the gradient: [-249.80255562]\n",
      "iter_count:  1425 the loss: 238958.75448111453 the gradient: [-58.94962012]\n",
      "iter_count:  1426 the loss: 218154.19463740085 the gradient: [-0.29115546]\n",
      "iter_count:  1427 the loss: 217060.20315910716 the gradient: [2.98595732]\n",
      "iter_count:  1428 the loss: 239364.4667854281 the gradient: [-60.02810113]\n",
      "iter_count:  1429 the loss: 230920.17745601444 the gradient: [-37.08190522]\n",
      "iter_count:  1430 the loss: 236334.11375390762 the gradient: [-51.9150215]\n",
      "iter_count:  1431 the loss: 220367.73091724273 the gradient: [-6.86038317]\n",
      "iter_count:  1432 the loss: 264389.662285327 the gradient: [-122.26461145]\n",
      "iter_count:  1433 the loss: 238189.85753859236 the gradient: [-56.89919]\n",
      "iter_count:  1434 the loss: 356996.0608021889 the gradient: [-298.21772338]\n",
      "iter_count:  1435 the loss: 245017.15277184756 the gradient: [-74.81159382]\n",
      "iter_count:  1436 the loss: 274441.1210623986 the gradient: [-145.10392396]\n",
      "iter_count:  1437 the loss: 303930.9214956397 the gradient: [-206.12465635]\n",
      "iter_count:  1438 the loss: 281251.59207876545 the gradient: [-159.95295848]\n",
      "iter_count:  1439 the loss: 217103.72938001892 the gradient: [2.85518479]\n",
      "iter_count:  1440 the loss: 237516.40700927086 the gradient: [-55.09624532]\n",
      "iter_count:  1441 the loss: 223347.8032476131 the gradient: [-15.57631782]\n",
      "iter_count:  1442 the loss: 235279.93034128434 the gradient: [-49.06116165]\n",
      "iter_count:  1443 the loss: 249230.27514879694 the gradient: [-85.54353374]\n",
      "iter_count:  1444 the loss: 237336.74605094918 the gradient: [-54.61414548]\n",
      "iter_count:  1445 the loss: 239046.7277368536 the gradient: [-59.18367547]\n",
      "iter_count:  1446 the loss: 229919.5827892532 the gradient: [-34.29177378]\n",
      "iter_count:  1447 the loss: 404132.2201440666 the gradient: [-366.14099842]\n",
      "iter_count:  1448 the loss: 218851.24245540163 the gradient: [-2.36866931]\n",
      "iter_count:  1449 the loss: 259347.60868315058 the gradient: [-110.3674062]\n",
      "iter_count:  1450 the loss: 218395.02524370444 the gradient: [-1.00986223]\n",
      "iter_count:  1451 the loss: 218114.60432893186 the gradient: [-0.17291323]\n",
      "iter_count:  1452 the loss: 247232.17812147544 the gradient: [-80.48373398]\n",
      "iter_count:  1453 the loss: 235303.2780395721 the gradient: [-49.12454605]\n",
      "iter_count:  1454 the loss: 219829.56088606518 the gradient: [-5.27075771]\n",
      "iter_count:  1455 the loss: 249296.97567032502 the gradient: [-85.71152086]\n",
      "iter_count:  1456 the loss: 222176.47053461964 the gradient: [-12.16785294]\n",
      "iter_count:  1457 the loss: 242691.41297884315 the gradient: [-68.78337374]\n",
      "iter_count:  1458 the loss: 221583.40573336591 the gradient: [-10.43353387]\n",
      "iter_count:  1459 the loss: 225335.63920737067 the gradient: [-21.30983457]\n",
      "iter_count:  1460 the loss: 226503.95325388448 the gradient: [-24.65003568]\n",
      "iter_count:  1461 the loss: 223270.19555820583 the gradient: [-15.35117832]\n",
      "iter_count:  1462 the loss: 976292.9482568647 the gradient: [-805.94631883]\n",
      "iter_count:  1463 the loss: 253400.4884131926 the gradient: [-95.93399875]\n",
      "iter_count:  1464 the loss: 243617.0507199998 the gradient: [-71.19157963]\n",
      "iter_count:  1465 the loss: 2565708.092918211 the gradient: [-1274.07762293]\n",
      "iter_count:  1466 the loss: 229530.8406052155 the gradient: [-33.20360703]\n",
      "iter_count:  1467 the loss: 223486.5597586866 the gradient: [-15.978606]\n",
      "iter_count:  1468 the loss: 242684.4999261965 the gradient: [-68.76534324]\n",
      "iter_count:  1469 the loss: 217984.58974577548 the gradient: [0.21558021]\n",
      "iter_count:  1470 the loss: 534462.2959937234 the gradient: [-511.70324597]\n",
      "iter_count:  1471 the loss: 228476.62185564465 the gradient: [-30.24082233]\n",
      "iter_count:  1472 the loss: 228244.54518159752 the gradient: [-29.58626352]\n",
      "iter_count:  1473 the loss: 59410780.46667481 the gradient: [-3192.15023228]\n",
      "iter_count:  1474 the loss: 3699153450.5801196 the gradient: [-7443.83257377]\n",
      "iter_count:  1475 the loss: 229391.5994746241 the gradient: [-32.81327364]\n",
      "iter_count:  1476 the loss: 244155.66397546916 the gradient: [-72.58739033]\n",
      "iter_count:  1477 the loss: 220957.037096181 the gradient: [-8.59554042]\n",
      "iter_count:  1478 the loss: 219776.10408702266 the gradient: [-5.112596]\n",
      "iter_count:  1479 the loss: 221276.10719419335 the gradient: [-9.53261643]\n",
      "iter_count:  1480 the loss: 218718.39864200552 the gradient: [-1.97336525]\n",
      "iter_count:  1481 the loss: 230745.77427274582 the gradient: [-36.59669558]\n",
      "iter_count:  1482 the loss: 300989.39936123166 the gradient: [-200.40082527]\n",
      "iter_count:  1483 the loss: 374678.7439920292 the gradient: [-324.98526271]\n",
      "iter_count:  1484 the loss: 224959.7002180454 the gradient: [-20.23039107]\n",
      "iter_count:  1485 the loss: 218290.43430230193 the gradient: [-0.69785321]\n",
      "iter_count:  1486 the loss: 219955.72673339426 the gradient: [-5.64385369]\n",
      "iter_count:  1487 the loss: 225435.00487687235 the gradient: [-21.59476762]\n",
      "iter_count:  1488 the loss: 281904.4412324289 the gradient: [-161.35107162]\n",
      "iter_count:  1489 the loss: 333848.67163113994 the gradient: [-260.4362125]\n",
      "iter_count:  1490 the loss: 223214.18367661635 the gradient: [-15.18862733]\n",
      "iter_count:  1491 the loss: 227617.72854748738 the gradient: [-27.81414359]\n",
      "iter_count:  1492 the loss: 243422.891941907 the gradient: [-70.68743172]\n",
      "iter_count:  1493 the loss: 216434.80733761718 the gradient: [4.86848807]\n",
      "iter_count:  1494 the loss: 768357.8794969376 the gradient: [-690.79054378]\n",
      "iter_count:  1495 the loss: 493132.83991257916 the gradient: [-470.68322053]\n",
      "iter_count:  1496 the loss: 220464.81621953673 the gradient: [-7.14663767]\n",
      "iter_count:  1497 the loss: 258700.48513931813 the gradient: [-108.81821594]\n",
      "iter_count:  1498 the loss: 268276.5446350806 the gradient: [-131.23183637]\n",
      "iter_count:  1499 the loss: 232058.65769934468 the gradient: [-40.23784647]\n",
      "iter_count:  1500 the loss: 229290.61616953532 the gradient: [-32.52999975]\n",
      "iter_count:  1501 the loss: 259474.19919221324 the gradient: [-110.66985961]\n",
      "iter_count:  1502 the loss: 226972.8137497631 the gradient: [-25.98440402]\n",
      "iter_count:  1503 the loss: 306005.0292467255 the gradient: [-210.11625127]\n",
      "iter_count:  1504 the loss: 247721.83238868628 the gradient: [-81.72864094]\n",
      "iter_count:  1505 the loss: 263353.59286204126 the gradient: [-119.84466257]\n",
      "iter_count:  1506 the loss: 238120.23440912197 the gradient: [-56.71310222]\n",
      "iter_count:  1507 the loss: 227468.36263272088 the gradient: [-27.39094906]\n",
      "iter_count:  1508 the loss: 295040.8784562212 the gradient: [-188.59359469]\n",
      "iter_count:  1509 the loss: 234257.48150087238 the gradient: [-46.27746023]\n",
      "iter_count:  1510 the loss: 220052.89735058285 the gradient: [-5.93102476]\n",
      "iter_count:  1511 the loss: 241993.68361345472 the gradient: [-66.96018863]\n",
      "iter_count:  1512 the loss: 269396.7349870585 the gradient: [-133.78406023]\n",
      "iter_count:  1513 the loss: 233262.4611856371 the gradient: [-43.55342301]\n",
      "iter_count:  1514 the loss: 229270.34431153032 the gradient: [-32.47311494]\n",
      "iter_count:  1515 the loss: 281762.24106025463 the gradient: [-161.04691166]\n",
      "iter_count:  1516 the loss: 282650.1281163679 the gradient: [-162.94270523]\n",
      "iter_count:  1517 the loss: 1026649.9587403354 the gradient: [-830.00423084]\n",
      "iter_count:  1518 the loss: 217065.49105421838 the gradient: [2.97006837]\n",
      "iter_count:  1519 the loss: 510292.70319001836 the gradient: [-488.19335442]\n",
      "iter_count:  1520 the loss: 298274.74816615577 the gradient: [-195.05163085]\n",
      "iter_count:  1521 the loss: 338464.8896723305 the gradient: [-268.24110172]\n",
      "iter_count:  1522 the loss: 250700.7450878445 the gradient: [-89.23330394]\n",
      "iter_count:  1523 the loss: 230155.5195044358 the gradient: [-34.95106758]\n",
      "iter_count:  1524 the loss: 224077.55478879073 the gradient: [-17.68854662]\n",
      "iter_count:  1525 the loss: 219239.59401923735 the gradient: [-3.52259262]\n",
      "iter_count:  1526 the loss: 274816.49505642336 the gradient: [-145.9351536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  1527 the loss: 404132.2201440666 the gradient: [-366.14099842]\n",
      "iter_count:  1528 the loss: 402532.2530695383 the gradient: [-364.00488329]\n",
      "iter_count:  1529 the loss: 241513.01327624096 the gradient: [-65.70020058]\n",
      "iter_count:  1530 the loss: 256050.91920183843 the gradient: [-102.42126925]\n",
      "iter_count:  1531 the loss: 268276.5446350806 the gradient: [-131.23183637]\n",
      "iter_count:  1532 the loss: 245272.31522660833 the gradient: [-75.46841529]\n",
      "iter_count:  1533 the loss: 216288.86271532575 the gradient: [5.3087614]\n",
      "iter_count:  1534 the loss: 225130.0073762092 the gradient: [-20.71967914]\n",
      "iter_count:  1535 the loss: 1013600.3888655865 the gradient: [-823.88701733]\n",
      "iter_count:  1536 the loss: 262727.1321384655 the gradient: [-118.3752827]\n",
      "iter_count:  1537 the loss: 218568.50536978856 the gradient: [-1.52697123]\n",
      "iter_count:  1538 the loss: 220276.70735255454 the gradient: [-6.59185955]\n",
      "iter_count:  1539 the loss: 257664.55971842483 the gradient: [-106.32752051]\n",
      "iter_count:  1540 the loss: 218684.07155804528 the gradient: [-1.8711698]\n",
      "iter_count:  1541 the loss: 24330743.263660975 the gradient: [-2561.96155077]\n",
      "iter_count:  1542 the loss: 534462.2959937234 the gradient: [-511.70324597]\n",
      "iter_count:  1543 the loss: 229002.16628155936 the gradient: [-31.7199798]\n",
      "iter_count:  1544 the loss: 263361.9931546976 the gradient: [-119.86433407]\n",
      "iter_count:  1545 the loss: 278273.4147502042 the gradient: [-153.51944086]\n",
      "iter_count:  1546 the loss: 278763.03494598076 the gradient: [-154.58344481]\n",
      "iter_count:  1547 the loss: 4769699.731385006 the gradient: [-1593.66157236]\n",
      "iter_count:  1548 the loss: 256050.91920183843 the gradient: [-102.42126925]\n",
      "iter_count:  1549 the loss: 217163.79702123292 the gradient: [2.67476715]\n",
      "iter_count:  1550 the loss: 4467042.632882606 the gradient: [-1558.82527752]\n",
      "iter_count:  1551 the loss: 221718.67772340498 the gradient: [-10.82962269]\n",
      "iter_count:  1552 the loss: 279982.92829514097 the gradient: [-157.22357202]\n",
      "iter_count:  1553 the loss: 266169.2176748819 the gradient: [-126.39179025]\n",
      "iter_count:  1554 the loss: 1179191.52402459 the gradient: [-896.21672911]\n",
      "iter_count:  1555 the loss: 523888.8419360141 the gradient: [-501.57631694]\n",
      "iter_count:  1556 the loss: 224704.17716953208 the gradient: [-19.49540659]\n",
      "iter_count:  1557 the loss: 223964.90921491047 the gradient: [-17.36306235]\n",
      "iter_count:  1558 the loss: 213637.53437285055 the gradient: [13.37075365]\n",
      "iter_count:  1559 the loss: 258716.12126395476 the gradient: [-108.85570884]\n",
      "iter_count:  1560 the loss: 233306.21297801734 the gradient: [-43.67351382]\n",
      "iter_count:  1561 the loss: 217240.23545926876 the gradient: [2.44526716]\n",
      "iter_count:  1562 the loss: 221917.29207126197 the gradient: [-11.41063894]\n",
      "iter_count:  1563 the loss: 237972.537338089 the gradient: [-56.31810647]\n",
      "iter_count:  1564 the loss: 748027.0641834433 the gradient: [-677.79790628]\n",
      "iter_count:  1565 the loss: 225010.02385217912 the gradient: [-20.375018]\n",
      "iter_count:  1566 the loss: 697534.4558557156 the gradient: [-643.80010713]\n",
      "iter_count:  1567 the loss: 268745.7982238069 the gradient: [-132.3027041]\n",
      "iter_count:  1568 the loss: 217468.7142571332 the gradient: [1.75986948]\n",
      "iter_count:  1569 the loss: 248829.09696631966 the gradient: [-84.53190792]\n",
      "iter_count:  1570 the loss: 218078.91581426916 the gradient: [-0.06630163]\n",
      "iter_count:  1571 the loss: 549748.8962312 the gradient: [-525.93564136]\n",
      "iter_count:  1572 the loss: 235550.4358393939 the gradient: [-49.7950351]\n",
      "iter_count:  1573 the loss: 218286.21194813336 the gradient: [-0.68525349]\n",
      "iter_count:  1574 the loss: 285245982.1225328 the gradient: [-4512.61744499]\n",
      "iter_count:  1575 the loss: 268745.7982238069 the gradient: [-132.3027041]\n",
      "iter_count:  1576 the loss: 214345.72347065384 the gradient: [11.2054629]\n",
      "iter_count:  1577 the loss: 230882.67958380657 the gradient: [-36.97762126]\n",
      "iter_count:  1578 the loss: 348076.8826252413 the gradient: [-284.05134218]\n",
      "iter_count:  1579 the loss: 202696.82352138212 the gradient: [47.95524369]\n",
      "iter_count:  1580 the loss: 229678.74468936346 the gradient: [-33.61789587]\n",
      "iter_count:  1581 the loss: 229163.41719732332 the gradient: [-32.17296133]\n",
      "iter_count:  1582 the loss: 239629.74445692232 the gradient: [-60.73199353]\n",
      "iter_count:  1583 the loss: 396472.6189927173 the gradient: [-355.81580133]\n",
      "iter_count:  1584 the loss: 216551.83837150093 the gradient: [4.51570133]\n",
      "iter_count:  1585 the loss: 490694370.06894374 the gradient: [-5043.30203805]\n",
      "iter_count:  1586 the loss: 217060.20315910716 the gradient: [2.98595732]\n",
      "iter_count:  1587 the loss: 221451.9896622245 the gradient: [-10.04844728]\n",
      "iter_count:  1588 the loss: 251397.17733104056 the gradient: [-90.97090129]\n",
      "iter_count:  1589 the loss: 279367.69589348644 the gradient: [-155.89399836]\n",
      "iter_count:  1590 the loss: 248468.6231655337 the gradient: [-83.62109505]\n",
      "iter_count:  1591 the loss: 268904.6852984589 the gradient: [-132.6647296]\n",
      "iter_count:  1592 the loss: 236462.213794493 the gradient: [-52.26069362]\n",
      "iter_count:  1593 the loss: 525551.6562875173 the gradient: [-503.18479938]\n",
      "iter_count:  1594 the loss: 221283.84894630327 the gradient: [-9.55533229]\n",
      "iter_count:  1595 the loss: 224606.32664351622 the gradient: [-19.21367221]\n",
      "iter_count:  1596 the loss: 284157.7292979095 the gradient: [-166.14349124]\n",
      "iter_count:  1597 the loss: 1601632.3402384494 the gradient: [-1043.1356802]\n",
      "iter_count:  1598 the loss: 218163.36206235646 the gradient: [-0.31853154]\n",
      "iter_count:  1599 the loss: 253105.78272644625 the gradient: [-95.20713318]\n",
      "iter_count:  1600 the loss: 221527.08802048303 the gradient: [-10.2685417]\n",
      "iter_count:  1601 the loss: 278763.03494598076 the gradient: [-154.58344481]\n",
      "iter_count:  1602 the loss: 276848.34122079774 the gradient: [-150.40826711]\n",
      "iter_count:  1603 the loss: 229270.34431153032 the gradient: [-32.47311494]\n",
      "iter_count:  1604 the loss: 250838.7745741985 the gradient: [-89.57819185]\n",
      "iter_count:  1605 the loss: 218909.52314933206 the gradient: [-2.54200205]\n",
      "iter_count:  1606 the loss: 286301.4700324801 the gradient: [-170.65588929]\n",
      "iter_count:  1607 the loss: 300943.12536544964 the gradient: [-200.31018445]\n",
      "iter_count:  1608 the loss: 223065.24755194146 the gradient: [-14.75615455]\n",
      "iter_count:  1609 the loss: 310737.8893165228 the gradient: [-219.09069457]\n",
      "iter_count:  1610 the loss: 222471.25989761917 the gradient: [-13.02777019]\n",
      "iter_count:  1611 the loss: 260691.05602506938 the gradient: [-113.56725171]\n",
      "iter_count:  1612 the loss: 202696.82352138212 the gradient: [47.95524369]\n",
      "iter_count:  1613 the loss: 300029.55115680187 the gradient: [-198.51686016]\n",
      "iter_count:  1614 the loss: 277974.09984970046 the gradient: [-152.86775817]\n",
      "iter_count:  1615 the loss: 229530.8406052155 the gradient: [-33.20360703]\n",
      "iter_count:  1616 the loss: 338252.2864085535 the gradient: [-267.88473568]\n",
      "iter_count:  1617 the loss: 248495.62440926902 the gradient: [-83.68937946]\n",
      "iter_count:  1618 the loss: 254404.26634534876 the gradient: [-98.40137865]\n",
      "iter_count:  1619 the loss: 252185.03424150392 the gradient: [-92.9289756]\n",
      "iter_count:  1620 the loss: 219068.07468624 the gradient: [-3.01326238]\n",
      "iter_count:  1621 the loss: 500731.7572552794 the gradient: [-478.52573992]\n",
      "iter_count:  1622 the loss: 219565.30168031374 the gradient: [-4.48843425]\n",
      "iter_count:  1623 the loss: 624352.0596090426 the gradient: [-589.38448645]\n",
      "iter_count:  1624 the loss: 239635.35461870919 the gradient: [-60.74686873]\n",
      "iter_count:  1625 the loss: 229715.2136400607 the gradient: [-33.71999564]\n",
      "iter_count:  1626 the loss: 236347.52415117918 the gradient: [-51.95122016]\n",
      "iter_count:  1627 the loss: 3046608.719769229 the gradient: [-1360.54762095]\n",
      "iter_count:  1628 the loss: 217584.4370889991 the gradient: [1.413058]\n",
      "iter_count:  1629 the loss: 244431.89143230268 the gradient: [-73.30167297]\n",
      "iter_count:  1630 the loss: 412789.27220003045 the gradient: [-377.51619398]\n",
      "iter_count:  1631 the loss: 268024.37557852233 the gradient: [-130.65533699]\n",
      "iter_count:  1632 the loss: 235406.03826807297 the gradient: [-49.40342371]\n",
      "iter_count:  1633 the loss: 218677.99315912236 the gradient: [-1.85307168]\n",
      "iter_count:  1634 the loss: 220763.7053561078 the gradient: [-8.02692677]\n",
      "iter_count:  1635 the loss: 217762.97689956517 the gradient: [0.8784339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  1636 the loss: 277104.39211192273 the gradient: [-150.96884668]\n",
      "iter_count:  1637 the loss: 241533.66433670354 the gradient: [-65.75440053]\n",
      "iter_count:  1638 the loss: 218286.21194813336 the gradient: [-0.68525349]\n",
      "iter_count:  1639 the loss: 384825.43584867695 the gradient: [-339.61784753]\n",
      "iter_count:  1640 the loss: 324137.6299780924 the gradient: [-243.5409463]\n",
      "iter_count:  1641 the loss: 268904.6852984589 the gradient: [-132.6647296]\n",
      "iter_count:  1642 the loss: 268984.5038842488 the gradient: [-132.8464892]\n",
      "iter_count:  1643 the loss: 218119.41233555987 the gradient: [-0.18727445]\n",
      "iter_count:  1644 the loss: 218568.50536978856 the gradient: [-1.52697123]\n",
      "iter_count:  1645 the loss: 228663.9900862979 the gradient: [-30.76866916]\n",
      "iter_count:  1646 the loss: 221915.360186315 the gradient: [-11.40499062]\n",
      "iter_count:  1647 the loss: 523888.8419360141 the gradient: [-501.57631694]\n",
      "iter_count:  1648 the loss: 236380.33354224084 the gradient: [-52.03977133]\n",
      "iter_count:  1649 the loss: 224481.39406192346 the gradient: [-18.85373836]\n",
      "iter_count:  1650 the loss: 223207.6362459164 the gradient: [-15.16962283]\n",
      "iter_count:  1651 the loss: 286961.3335358034 the gradient: [-172.03574343]\n",
      "iter_count:  1652 the loss: 231616.23748403293 the gradient: [-39.01378088]\n",
      "iter_count:  1653 the loss: 317595.9975976829 the gradient: [-231.77623218]\n",
      "iter_count:  1654 the loss: 48444113.52398859 the gradient: [-3041.04753813]\n",
      "iter_count:  1655 the loss: 286906.1335103904 the gradient: [-171.92047656]\n",
      "iter_count:  1656 the loss: 223797.40588034198 the gradient: [-16.87868969]\n",
      "iter_count:  1657 the loss: 290261.3905302031 the gradient: [-178.87330341]\n",
      "iter_count:  1658 the loss: 218044.4205812991 the gradient: [0.03676571]\n",
      "iter_count:  1659 the loss: 221881.77664269178 the gradient: [-11.30679148]\n",
      "iter_count:  1660 the loss: 228697.98417801334 the gradient: [-30.86437729]\n",
      "iter_count:  1661 the loss: 222550.6159579883 the gradient: [-13.25901354]\n",
      "iter_count:  1662 the loss: 773089.3659657866 the gradient: [-693.76123745]\n",
      "iter_count:  1663 the loss: 252904.14026836696 the gradient: [-94.70915608]\n",
      "iter_count:  1664 the loss: 624352.0596090426 the gradient: [-589.38448645]\n",
      "iter_count:  1665 the loss: 268984.5038842488 the gradient: [-132.8464892]\n",
      "iter_count:  1666 the loss: 25622117.020122886 the gradient: [-2596.35537759]\n",
      "iter_count:  1667 the loss: 358330.66092364816 the gradient: [-300.29766692]\n",
      "iter_count:  1668 the loss: 218305.3479593459 the gradient: [-0.74235394]\n",
      "iter_count:  1669 the loss: 322321.8368927951 the gradient: [-240.30722643]\n",
      "iter_count:  1670 the loss: 218163.36206235646 the gradient: [-0.31853154]\n",
      "iter_count:  1671 the loss: 748027.0641834433 the gradient: [-677.79790628]\n",
      "iter_count:  1672 the loss: 226386.4155645402 the gradient: [-24.31497926]\n",
      "iter_count:  1673 the loss: 243972.28222580996 the gradient: [-72.11260988]\n",
      "iter_count:  1674 the loss: 256643.72481143518 the gradient: [-103.86009308]\n",
      "iter_count:  1675 the loss: 218091.62241128876 the gradient: [-0.10426226]\n",
      "iter_count:  1676 the loss: 218290.43430230193 the gradient: [-0.69785321]\n",
      "iter_count:  1677 the loss: 259977.72207665438 the gradient: [-111.87095414]\n",
      "iter_count:  1678 the loss: 223220.91575856737 the gradient: [-15.20816707]\n",
      "iter_count:  1679 the loss: 317454.1794199213 the gradient: [-231.51761675]\n",
      "iter_count:  1680 the loss: 231952.26716377868 the gradient: [-39.94376291]\n",
      "iter_count:  1681 the loss: 254804.99276989346 the gradient: [-99.3828127]\n",
      "iter_count:  1682 the loss: 221646.02529967076 the gradient: [-10.61692712]\n",
      "iter_count:  1683 the loss: 239505.4426430757 the gradient: [-60.40229472]\n",
      "iter_count:  1684 the loss: 55329406.98886578 the gradient: [-3138.95583176]\n",
      "iter_count:  1685 the loss: 227812.42316423258 the gradient: [-28.36523978]\n",
      "iter_count:  1686 the loss: 299307.49762214994 the gradient: [-197.09430668]\n",
      "iter_count:  1687 the loss: 281400.20692234836 the gradient: [-160.27160742]\n",
      "iter_count:  1688 the loss: 250021.23139864567 the gradient: [-87.53178416]\n",
      "iter_count:  1689 the loss: 221646.02529967076 the gradient: [-10.61692712]\n",
      "iter_count:  1690 the loss: 222227.8159453194 the gradient: [-12.31773269]\n",
      "iter_count:  1691 the loss: 222213.63972143567 the gradient: [-12.27635592]\n",
      "iter_count:  1692 the loss: 269283.41980258125 the gradient: [-133.52652915]\n",
      "iter_count:  1693 the loss: 231616.23748403293 the gradient: [-39.01378088]\n",
      "iter_count:  1694 the loss: 240809.21202706272 the gradient: [-63.84942367]\n",
      "iter_count:  1695 the loss: 219837.02899056478 the gradient: [-5.29284967]\n",
      "iter_count:  1696 the loss: 224729.0015565364 the gradient: [-19.56685727]\n",
      "iter_count:  1697 the loss: 241656.94580509004 the gradient: [-66.07783481]\n",
      "iter_count:  1698 the loss: 268450.79024352564 the gradient: [-131.6297679]\n",
      "iter_count:  1699 the loss: 216959.80778292308 the gradient: [3.28771335]\n",
      "iter_count:  1700 the loss: 202696.82352138212 the gradient: [47.95524369]\n",
      "iter_count:  1701 the loss: 276994.2062296876 the gradient: [-150.72769857]\n",
      "iter_count:  1702 the loss: 41120393.39471099 the gradient: [-2922.78804684]\n",
      "iter_count:  1703 the loss: 241909.45621723702 the gradient: [-66.73963749]\n",
      "iter_count:  1704 the loss: 290000.41407259257 the gradient: [-178.3363737]\n",
      "iter_count:  1705 the loss: 299176.61268704716 the gradient: [-196.83595186]\n",
      "iter_count:  1706 the loss: 254002.31037686803 the gradient: [-97.41487926]\n",
      "iter_count:  1707 the loss: 222877.86270682918 the gradient: [-14.21152357]\n",
      "iter_count:  1708 the loss: 223346.46639900963 the gradient: [-15.57244046]\n",
      "iter_count:  1709 the loss: 217046.01717461649 the gradient: [3.02858541]\n",
      "iter_count:  1710 the loss: 220759.53187747885 the gradient: [-8.01464522]\n",
      "iter_count:  1711 the loss: 253604.88354360376 the gradient: [-96.43746635]\n",
      "iter_count:  1712 the loss: 315969.4096048658 the gradient: [-228.80076055]\n",
      "iter_count:  1713 the loss: 218527.2118432552 the gradient: [-1.40392951]\n",
      "iter_count:  1714 the loss: 244451.96148296384 the gradient: [-73.35353003]\n",
      "iter_count:  1715 the loss: 224226.6222880839 the gradient: [-18.1189553]\n",
      "iter_count:  1716 the loss: 217109.28195063522 the gradient: [2.83850466]\n",
      "iter_count:  1717 the loss: 361000.20676945447 the gradient: [-304.42793227]\n",
      "iter_count:  1718 the loss: 223227.71105755438 the gradient: [-15.22788955]\n",
      "iter_count:  1719 the loss: 235109.46872369305 the gradient: [-48.59814673]\n",
      "iter_count:  1720 the loss: 216434.80733761718 the gradient: [4.86848807]\n",
      "iter_count:  1721 the loss: 223536.9284114207 the gradient: [-16.12455942]\n",
      "iter_count:  1722 the loss: 242030.41967133744 the gradient: [-67.05635146]\n",
      "iter_count:  1723 the loss: 229103.54478146383 the gradient: [-32.00481665]\n",
      "iter_count:  1724 the loss: 292987.5810768288 the gradient: [-184.44373619]\n",
      "iter_count:  1725 the loss: 217893.85108962125 the gradient: [0.48688321]\n",
      "iter_count:  1726 the loss: 245004.0635482467 the gradient: [-74.77787635]\n",
      "iter_count:  1727 the loss: 233262.4611856371 the gradient: [-43.55342301]\n",
      "iter_count:  1728 the loss: 220301.3764554934 the gradient: [-6.66464811]\n",
      "iter_count:  1729 the loss: 328113.89369019965 the gradient: [-250.53894272]\n",
      "iter_count:  1730 the loss: 507040.0388190225 the gradient: [-484.92882773]\n",
      "iter_count:  1731 the loss: 223486.5597586866 the gradient: [-15.978606]\n",
      "iter_count:  1732 the loss: 697534.4558557156 the gradient: [-643.80010713]\n",
      "iter_count:  1733 the loss: 217425.99069123453 the gradient: [1.88796568]\n",
      "iter_count:  1734 the loss: 279309.2863937013 the gradient: [-155.76756629]\n",
      "iter_count:  1735 the loss: 244918.83855530006 the gradient: [-74.55828158]\n",
      "iter_count:  1736 the loss: 218286.10962780425 the gradient: [-0.68494816]\n",
      "iter_count:  1737 the loss: 243490.10722696685 the gradient: [-70.8620206]\n",
      "iter_count:  1738 the loss: 217060.20315910716 the gradient: [2.98595732]\n",
      "iter_count:  1739 the loss: 217182.73986633835 the gradient: [2.61788361]\n",
      "iter_count:  1740 the loss: 218920.7958321774 the gradient: [-2.57552159]\n",
      "iter_count:  1741 the loss: 486328.72544280684 the gradient: [-463.53678883]\n",
      "iter_count:  1742 the loss: 320607.88249419065 the gradient: [-237.23257416]\n",
      "iter_count:  1743 the loss: 1339107.1186294605 the gradient: [-957.07360394]\n",
      "iter_count:  1744 the loss: 457984.357125101 the gradient: [-432.40726162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  1745 the loss: 259977.72207665438 the gradient: [-111.87095414]\n",
      "iter_count:  1746 the loss: 1679230094.7006364 the gradient: [-6417.15702308]\n",
      "iter_count:  1747 the loss: 283957.7088606979 the gradient: [-165.7201387]\n",
      "iter_count:  1748 the loss: 233406.62914207115 the gradient: [-43.94902908]\n",
      "iter_count:  1749 the loss: 279367.69589348644 the gradient: [-155.89399836]\n",
      "iter_count:  1750 the loss: 286292.04380726314 the gradient: [-170.63614705]\n",
      "iter_count:  1751 the loss: 254232.86867146788 the gradient: [-97.98097832]\n",
      "iter_count:  1752 the loss: 231952.26716377868 the gradient: [-39.94376291]\n",
      "iter_count:  1753 the loss: 286316.511285504 the gradient: [-170.68738981]\n",
      "iter_count:  1754 the loss: 225893.51965629534 the gradient: [-22.90752425]\n",
      "iter_count:  1755 the loss: 218747.77824672748 the gradient: [-2.06081581]\n",
      "iter_count:  1756 the loss: 230270.4313432263 the gradient: [-35.27186218]\n",
      "iter_count:  1757 the loss: 256444.18343527341 the gradient: [-103.37626925]\n",
      "iter_count:  1758 the loss: 432235.17160391354 the gradient: [-402.00800564]\n",
      "iter_count:  1759 the loss: 220977.40177309074 the gradient: [-8.65539959]\n",
      "iter_count:  1760 the loss: 226081.74832521856 the gradient: [-23.44546393]\n",
      "iter_count:  1761 the loss: 223555.0480752032 the gradient: [-16.17705478]\n",
      "iter_count:  1762 the loss: 251001.8855893217 the gradient: [-89.98542793]\n",
      "iter_count:  1763 the loss: 221985.95511317736 the gradient: [-11.61135166]\n",
      "iter_count:  1764 the loss: 246620.10097943444 the gradient: [-78.92302467]\n",
      "iter_count:  1765 the loss: 282593.53079764283 the gradient: [-162.82209828]\n",
      "iter_count:  1766 the loss: 230197.6336421065 the gradient: [-35.06865952]\n",
      "iter_count:  1767 the loss: 218825.65832329873 the gradient: [-2.29256149]\n",
      "iter_count:  1768 the loss: 237884.8660237041 the gradient: [-56.08349157]\n",
      "iter_count:  1769 the loss: 225832.17963431342 the gradient: [-22.73209832]\n",
      "iter_count:  1770 the loss: 235609.44518173914 the gradient: [-49.95498165]\n",
      "iter_count:  1771 the loss: 221789.95198985448 the gradient: [-11.03819931]\n",
      "iter_count:  1772 the loss: 279178.7105237244 the gradient: [-155.48479624]\n",
      "iter_count:  1773 the loss: 218810.52498895724 the gradient: [-2.2475376]\n",
      "iter_count:  1774 the loss: 740694.8201441052 the gradient: [-673.01844239]\n",
      "iter_count:  1775 the loss: 254002.31037686803 the gradient: [-97.41487926]\n",
      "iter_count:  1776 the loss: 221828.78733622996 the gradient: [-11.15181173]\n",
      "iter_count:  1777 the loss: 319114.1706010716 the gradient: [-234.53512822]\n",
      "iter_count:  1778 the loss: 221981.67261178925 the gradient: [-11.5988355]\n",
      "iter_count:  1779 the loss: 243372.01115145427 the gradient: [-70.55522919]\n",
      "iter_count:  1780 the loss: 297678.88585320197 the gradient: [-193.868761]\n",
      "iter_count:  1781 the loss: 241557.25556859496 the gradient: [-65.81630975]\n",
      "iter_count:  1782 the loss: 254472.31070846797 the gradient: [-98.56817257]\n",
      "iter_count:  1783 the loss: 218485.78941268905 the gradient: [-1.2804749]\n",
      "iter_count:  1784 the loss: 221917.29207126197 the gradient: [-11.41063894]\n",
      "iter_count:  1785 the loss: 216754.3949736654 the gradient: [3.90565181]\n",
      "iter_count:  1786 the loss: 282650.1281163679 the gradient: [-162.94270523]\n",
      "iter_count:  1787 the loss: 254404.26634534876 the gradient: [-98.40137865]\n",
      "iter_count:  1788 the loss: 225435.00487687235 the gradient: [-21.59476762]\n",
      "iter_count:  1789 the loss: 228082.90921505494 the gradient: [-29.12988041]\n",
      "iter_count:  1790 the loss: 264629.91135983524 the gradient: [-122.82395898]\n",
      "iter_count:  1791 the loss: 249914.70304052913 the gradient: [-87.26448283]\n",
      "iter_count:  1792 the loss: 218521.66160795302 the gradient: [-1.38738937]\n",
      "iter_count:  1793 the loss: 217740.3171456097 the gradient: [0.94625697]\n",
      "iter_count:  1794 the loss: 220525.71350051305 the gradient: [-7.32611257]\n",
      "iter_count:  1795 the loss: 277974.09984970046 the gradient: [-152.86775817]\n",
      "iter_count:  1796 the loss: 220256.3236080167 the gradient: [-6.53170774]\n",
      "iter_count:  1797 the loss: 244306.5859726149 the gradient: [-72.97778257]\n",
      "iter_count:  1798 the loss: 243972.28222580996 the gradient: [-72.11260988]\n",
      "iter_count:  1799 the loss: 240970.4240106406 the gradient: [-64.27398069]\n",
      "iter_count:  1800 the loss: 270682.627727885 the gradient: [-136.69640912]\n",
      "iter_count:  1801 the loss: 356434.5986145424 the gradient: [-297.33966151]\n",
      "iter_count:  1802 the loss: 218725.3757629848 the gradient: [-1.99413448]\n",
      "iter_count:  1803 the loss: 396185.4280079526 the gradient: [-355.4237389]\n",
      "iter_count:  1804 the loss: 217065.49105421838 the gradient: [2.97006837]\n",
      "iter_count:  1805 the loss: 229182.92151417493 the gradient: [-32.22772488]\n",
      "iter_count:  1806 the loss: 250107.1821626928 the gradient: [-87.74734297]\n",
      "iter_count:  1807 the loss: 251001.8855893217 the gradient: [-89.98542793]\n",
      "iter_count:  1808 the loss: 240715.99726403307 the gradient: [-63.6037711]\n",
      "iter_count:  1809 the loss: 261557.77532910267 the gradient: [-115.62002001]\n",
      "iter_count:  1810 the loss: 256548.56843498288 the gradient: [-103.62943153]\n",
      "iter_count:  1811 the loss: 216754.3949736654 the gradient: [3.90565181]\n",
      "iter_count:  1812 the loss: 226461.47796346544 the gradient: [-24.52897966]\n",
      "iter_count:  1813 the loss: 361000.20676945447 the gradient: [-304.42793227]\n",
      "iter_count:  1814 the loss: 226503.95325388448 the gradient: [-24.65003568]\n",
      "iter_count:  1815 the loss: 224704.17716953208 the gradient: [-19.49540659]\n",
      "iter_count:  1816 the loss: 268276.5446350806 the gradient: [-131.23183637]\n",
      "iter_count:  1817 the loss: 286292.04380726314 the gradient: [-170.63614705]\n",
      "iter_count:  1818 the loss: 222677.7966916608 the gradient: [-13.62940334]\n",
      "iter_count:  1819 the loss: 419054.39396699576 the gradient: [-385.5623546]\n",
      "iter_count:  1820 the loss: 226068.66669217488 the gradient: [-23.40809611]\n",
      "iter_count:  1821 the loss: 229746.32043543333 the gradient: [-33.80706705]\n",
      "iter_count:  1822 the loss: 10728056.716263957 the gradient: [-2048.93935359]\n",
      "iter_count:  1823 the loss: 217846.05175482002 the gradient: [0.62985619]\n",
      "iter_count:  1824 the loss: 229627.84146780486 the gradient: [-33.47535094]\n",
      "iter_count:  1825 the loss: 269811.29137181886 the gradient: [-134.72499045]\n",
      "iter_count:  1826 the loss: 233225.1651889148 the gradient: [-43.45102943]\n",
      "iter_count:  1827 the loss: 241175.13272269416 the gradient: [-64.81255636]\n",
      "iter_count:  1828 the loss: 222778.37139980088 the gradient: [-13.9221212]\n",
      "iter_count:  1829 the loss: 218486.27547972344 the gradient: [-1.28192373]\n",
      "iter_count:  1830 the loss: 217993.4221483667 the gradient: [0.18917929]\n",
      "iter_count:  1831 the loss: 261039.97929869627 the gradient: [-114.39474264]\n",
      "iter_count:  1832 the loss: 214635.31729984737 the gradient: [10.32253133]\n",
      "iter_count:  1833 the loss: 261514.3061136272 the gradient: [-115.5172814]\n",
      "iter_count:  1834 the loss: 232633.1272551765 the gradient: [-41.82281723]\n",
      "iter_count:  1835 the loss: 228519.50604523864 the gradient: [-30.36168249]\n",
      "iter_count:  1836 the loss: 233321.89163294501 the gradient: [-43.71654188]\n",
      "iter_count:  1837 the loss: 221985.46635557068 the gradient: [-11.60992321]\n",
      "iter_count:  1838 the loss: 225405.85061510192 the gradient: [-21.51118356]\n",
      "iter_count:  1839 the loss: 251299.68049593386 the gradient: [-90.72802863]\n",
      "iter_count:  1840 the loss: 225787.59581330637 the gradient: [-22.60455569]\n",
      "iter_count:  1841 the loss: 234172.0153874047 the gradient: [-46.04406522]\n",
      "iter_count:  1842 the loss: 229627.84146780486 the gradient: [-33.47535094]\n",
      "iter_count:  1843 the loss: 526355.8518928182 the gradient: [-503.96056373]\n",
      "iter_count:  1844 the loss: 238497.62447116125 the gradient: [-57.72094353]\n",
      "iter_count:  1845 the loss: 220510.42973284167 the gradient: [-7.28107442]\n",
      "iter_count:  1846 the loss: 243514.46632412178 the gradient: [-70.92527682]\n",
      "iter_count:  1847 the loss: 244918.83855530006 the gradient: [-74.55828158]\n",
      "iter_count:  1848 the loss: 220967.6492433192 the gradient: [-8.62673422]\n",
      "iter_count:  1849 the loss: 219114.13406213708 the gradient: [-3.15008502]\n",
      "iter_count:  1850 the loss: 243514.46632412178 the gradient: [-70.92527682]\n",
      "iter_count:  1851 the loss: 599956.0240977021 the gradient: [-569.63638617]\n",
      "iter_count:  1852 the loss: 242108.7340688125 the gradient: [-67.26128924]\n",
      "iter_count:  1853 the loss: 458049.68708620203 the gradient: [-432.48169074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  1854 the loss: 238497.62447116125 the gradient: [-57.72094353]\n",
      "iter_count:  1855 the loss: 229068.47345664224 the gradient: [-31.90629704]\n",
      "iter_count:  1856 the loss: 254472.31070846797 the gradient: [-98.56817257]\n",
      "iter_count:  1857 the loss: 276764.93322648277 the gradient: [-150.22550974]\n",
      "iter_count:  1858 the loss: 217778.14390431903 the gradient: [0.83304228]\n",
      "iter_count:  1859 the loss: 266742.42638948397 the gradient: [-127.71336512]\n",
      "iter_count:  1860 the loss: 469298.41556391853 the gradient: [-445.10696675]\n",
      "iter_count:  1861 the loss: 594351516.3098754 the gradient: [-5241.11687141]\n",
      "iter_count:  1862 the loss: 239040.5619570974 the gradient: [-59.16727487]\n",
      "iter_count:  1863 the loss: 594351516.3098754 the gradient: [-5241.11687141]\n",
      "iter_count:  1864 the loss: 222395.0345579289 the gradient: [-12.80555284]\n",
      "iter_count:  1865 the loss: 286282.7208386156 the gradient: [-170.61662021]\n",
      "iter_count:  1866 the loss: 235868.17778414127 the gradient: [-50.65567589]\n",
      "iter_count:  1867 the loss: 218100.7741676783 the gradient: [-0.13160121]\n",
      "iter_count:  1868 the loss: 368762.96854180057 the gradient: [-316.21595589]\n",
      "iter_count:  1869 the loss: 228435.8365060386 the gradient: [-30.12585065]\n",
      "iter_count:  1870 the loss: 224003.17411717534 the gradient: [-17.47365004]\n",
      "iter_count:  1871 the loss: 276994.2062296876 the gradient: [-150.72769857]\n",
      "iter_count:  1872 the loss: 222860.76206808453 the gradient: [-14.16179236]\n",
      "iter_count:  1873 the loss: 218048.1224165435 the gradient: [0.02570414]\n",
      "iter_count:  1874 the loss: 222577.3440971137 the gradient: [-13.3368761]\n",
      "iter_count:  1875 the loss: 275792.72782140627 the gradient: [-148.08983489]\n",
      "iter_count:  1876 the loss: 221985.46635557068 the gradient: [-11.60992321]\n",
      "iter_count:  1877 the loss: 218403.29918265305 the gradient: [-1.03453665]\n",
      "iter_count:  1878 the loss: 247136.8010681005 the gradient: [-80.24086929]\n",
      "iter_count:  1879 the loss: 237837.44166238807 the gradient: [-55.9565339]\n",
      "iter_count:  1880 the loss: 301373.84380534507 the gradient: [-201.15314891]\n",
      "iter_count:  1881 the loss: 750677.2113442768 the gradient: [-679.5129398]\n",
      "iter_count:  1882 the loss: 236229.12373010349 the gradient: [-51.63153062]\n",
      "iter_count:  1883 the loss: 249230.27514879694 the gradient: [-85.54353374]\n",
      "iter_count:  1884 the loss: 248581.58142572537 the gradient: [-83.9066944]\n",
      "iter_count:  1885 the loss: 217290.5691759297 the gradient: [2.29419853]\n",
      "iter_count:  1886 the loss: 443931.3834441934 the gradient: [-416.08646305]\n",
      "iter_count:  1887 the loss: 7340651.630058616 the gradient: [-1829.88762836]\n",
      "iter_count:  1888 the loss: 229258.23707438738 the gradient: [-32.43913781]\n",
      "iter_count:  1889 the loss: 226068.66669217488 the gradient: [-23.40809611]\n",
      "iter_count:  1890 the loss: 240342.7636044173 the gradient: [-62.618935]\n",
      "iter_count:  1891 the loss: 610333.0105869714 the gradient: [-578.14566896]\n",
      "iter_count:  1892 the loss: 223438.72791580064 the gradient: [-15.83996539]\n",
      "iter_count:  1893 the loss: 298274.74816615577 the gradient: [-195.05163085]\n",
      "iter_count:  1894 the loss: 302062.38005315745 the gradient: [-202.49734854]\n",
      "iter_count:  1895 the loss: 224397.41649810367 the gradient: [-18.61165566]\n",
      "iter_count:  1896 the loss: 356800.7734897158 the gradient: [-297.9125213]\n",
      "iter_count:  1897 the loss: 237516.40700927086 the gradient: [-55.09624532]\n",
      "iter_count:  1898 the loss: 338252.2864085535 the gradient: [-267.88473568]\n",
      "iter_count:  1899 the loss: 245272.31522660833 the gradient: [-75.46841529]\n",
      "iter_count:  1900 the loss: 217359.086449659 the gradient: [2.0886239]\n",
      "iter_count:  1901 the loss: 216988.89578874895 the gradient: [3.20026659]\n",
      "iter_count:  1902 the loss: 253998.19768737792 the gradient: [-97.40477505]\n",
      "iter_count:  1903 the loss: 229270.34431153032 the gradient: [-32.47311494]\n",
      "iter_count:  1904 the loss: 216959.80778292308 the gradient: [3.28771335]\n",
      "iter_count:  1905 the loss: 228947.66811817835 the gradient: [-31.56679357]\n",
      "iter_count:  1906 the loss: 325339.8535210215 the gradient: [-245.66875752]\n",
      "iter_count:  1907 the loss: 624352.0596090426 the gradient: [-589.38448645]\n",
      "iter_count:  1908 the loss: 220788.96850211962 the gradient: [-8.10126406]\n",
      "iter_count:  1909 the loss: 217642.57902749485 the gradient: [1.23889712]\n",
      "iter_count:  1910 the loss: 1013600.3888655865 the gradient: [-823.88701733]\n",
      "iter_count:  1911 the loss: 13291892.095444323 the gradient: [-2177.66751786]\n",
      "iter_count:  1912 the loss: 324106.2486688909 the gradient: [-243.48526431]\n",
      "iter_count:  1913 the loss: 223754.81502750004 the gradient: [-16.75545648]\n",
      "iter_count:  1914 the loss: 219955.72673339426 the gradient: [-5.64385369]\n",
      "iter_count:  1915 the loss: 227196.52094404874 the gradient: [-26.61984499]\n",
      "iter_count:  1916 the loss: 523888.8419360141 the gradient: [-501.57631694]\n",
      "iter_count:  1917 the loss: 311846.263974292 the gradient: [-221.16603987]\n",
      "iter_count:  1918 the loss: 223054.30034712362 the gradient: [-14.7243524]\n",
      "iter_count:  1919 the loss: 258511.477859157 the gradient: [-108.36476926]\n",
      "iter_count:  1920 the loss: 221083.40458736816 the gradient: [-8.9668697]\n",
      "iter_count:  1921 the loss: 261931.58675064368 the gradient: [-116.50258014]\n",
      "iter_count:  1922 the loss: 217093.85292729764 the gradient: [2.88485532]\n",
      "iter_count:  1923 the loss: 233262.4611856371 the gradient: [-43.55342301]\n",
      "iter_count:  1924 the loss: 593730.9112664361 the gradient: [-564.45085458]\n",
      "iter_count:  1925 the loss: 59410780.46667481 the gradient: [-3192.15023228]\n",
      "iter_count:  1926 the loss: 218920.7958321774 the gradient: [-2.57552159]\n",
      "iter_count:  1927 the loss: 302387.35559427197 the gradient: [-203.13036084]\n",
      "iter_count:  1928 the loss: 242184.0128735487 the gradient: [-67.458202]\n",
      "iter_count:  1929 the loss: 217018.94230690054 the gradient: [3.10995337]\n",
      "iter_count:  1930 the loss: 5043799.331122604 the gradient: [-1623.56964939]\n",
      "iter_count:  1931 the loss: 218724.95467046194 the gradient: [-1.99288101]\n",
      "iter_count:  1932 the loss: 218020.22108588953 the gradient: [0.10908269]\n",
      "iter_count:  1933 the loss: 216074.1370725836 the gradient: [5.95718883]\n",
      "iter_count:  1934 the loss: 221319.56474518284 the gradient: [-9.66011683]\n",
      "iter_count:  1935 the loss: 237471.17202153284 the gradient: [-54.97490671]\n",
      "iter_count:  1936 the loss: 277205.8562312689 the gradient: [-151.19079324]\n",
      "iter_count:  1937 the loss: 217018.94230690054 the gradient: [3.10995337]\n",
      "iter_count:  1938 the loss: 589065074139.5171 the gradient: [-18208.78125505]\n",
      "iter_count:  1939 the loss: 261931.58675064368 the gradient: [-116.50258014]\n",
      "iter_count:  1940 the loss: 217846.05175482002 the gradient: [0.62985619]\n",
      "iter_count:  1941 the loss: 239889.07499848542 the gradient: [-61.41913015]\n",
      "iter_count:  1942 the loss: 219297.6059070076 the gradient: [-3.69474873]\n",
      "iter_count:  1943 the loss: 219913.02880886607 the gradient: [-5.51761762]\n",
      "iter_count:  1944 the loss: 217400.25501919264 the gradient: [1.96514275]\n",
      "iter_count:  1945 the loss: 406565125.64001274 the gradient: [-4854.52325741]\n",
      "iter_count:  1946 the loss: 217364.54710196817 the gradient: [2.07224354]\n",
      "iter_count:  1947 the loss: 1435194.770306518 the gradient: [-990.31710375]\n",
      "iter_count:  1948 the loss: 230351.6629795116 the gradient: [-35.49851028]\n",
      "iter_count:  1949 the loss: 240053.76328346055 the gradient: [-61.85499755]\n",
      "iter_count:  1950 the loss: 219335.36782499042 the gradient: [-3.80678078]\n",
      "iter_count:  1951 the loss: 249686.17949907848 the gradient: [-86.690565]\n",
      "iter_count:  1952 the loss: 525551.6562875173 the gradient: [-503.18479938]\n",
      "iter_count:  1953 the loss: 216434.80733761718 the gradient: [4.86848807]\n",
      "iter_count:  1954 the loss: 251657.25813176626 the gradient: [-91.61817756]\n",
      "iter_count:  1955 the loss: 228050.20716585708 the gradient: [-29.03749556]\n",
      "iter_count:  1956 the loss: 222717.82483955595 the gradient: [-13.74592311]\n",
      "iter_count:  1957 the loss: 218167.6399087192 the gradient: [-0.33130571]\n",
      "iter_count:  1958 the loss: 219632.14125911065 the gradient: [-4.68641882]\n",
      "iter_count:  1959 the loss: 222740.03669314636 the gradient: [-13.81056934]\n",
      "iter_count:  1960 the loss: 245352.71380357814 the gradient: [-75.67518668]\n",
      "iter_count:  1961 the loss: 300575.958437159 the gradient: [-199.59032149]\n",
      "iter_count:  1962 the loss: 481004.9729514367 the gradient: [-457.86060921]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_count:  1963 the loss: 217046.01717461649 the gradient: [3.02858541]\n",
      "iter_count:  1964 the loss: 224438.3816510503 the gradient: [-18.72976038]\n",
      "iter_count:  1965 the loss: 217596.61855714076 the gradient: [1.37656437]\n",
      "iter_count:  1966 the loss: 225839.8322816341 the gradient: [-22.75398735]\n",
      "iter_count:  1967 the loss: 300029.55115680187 the gradient: [-198.51686016]\n",
      "iter_count:  1968 the loss: 213637.53437285055 the gradient: [13.37075365]\n",
      "iter_count:  1969 the loss: 233641.21487880722 the gradient: [-44.59207856]\n",
      "iter_count:  1970 the loss: 456992.2995254459 the gradient: [-431.2754276]\n",
      "iter_count:  1971 the loss: 281904.4412324289 the gradient: [-161.35107162]\n",
      "iter_count:  1972 the loss: 229425.616728159 the gradient: [-32.90866167]\n",
      "iter_count:  1973 the loss: 308625.6394230784 the gradient: [-215.10817077]\n",
      "iter_count:  1974 the loss: 228476.62185564465 the gradient: [-30.24082233]\n",
      "iter_count:  1975 the loss: 234356.26500959444 the gradient: [-46.54708691]\n",
      "iter_count:  1976 the loss: 300540.7359646751 the gradient: [-199.52120288]\n",
      "iter_count:  1977 the loss: 223205.91426807956 the gradient: [-15.16462452]\n",
      "iter_count:  1978 the loss: 223800.2479537469 the gradient: [-16.88691195]\n",
      "iter_count:  1979 the loss: 277974.09984970046 the gradient: [-152.86775817]\n",
      "iter_count:  1980 the loss: 262013.18443898318 the gradient: [-116.6950078]\n",
      "iter_count:  1981 the loss: 219067.83087984603 the gradient: [-3.01253804]\n",
      "iter_count:  1982 the loss: 283957.7088606979 the gradient: [-165.7201387]\n",
      "iter_count:  1983 the loss: 229182.92151417493 the gradient: [-32.22772488]\n",
      "iter_count:  1984 the loss: 218649.21887245055 the gradient: [-1.76738932]\n",
      "iter_count:  1985 the loss: 218091.62241128876 the gradient: [-0.10426226]\n",
      "iter_count:  1986 the loss: 5043799.331122604 the gradient: [-1623.56964939]\n",
      "iter_count:  1987 the loss: 252984.06132255375 the gradient: [-94.90659211]\n",
      "iter_count:  1988 the loss: 239364.4667854281 the gradient: [-60.02810113]\n",
      "iter_count:  1989 the loss: 243200.3811048865 the gradient: [-70.10902042]\n",
      "iter_count:  1990 the loss: 233664.9101158522 the gradient: [-44.65698624]\n",
      "iter_count:  1991 the loss: 219067.83087984603 the gradient: [-3.01253804]\n",
      "iter_count:  1992 the loss: 252904.14026836696 the gradient: [-94.70915608]\n",
      "iter_count:  1993 the loss: 338841.51069469366 the gradient: [-268.87167033]\n",
      "iter_count:  1994 the loss: 235907.09246753168 the gradient: [-50.76097822]\n",
      "iter_count:  1995 the loss: 697534.4558557156 the gradient: [-643.80010713]\n",
      "iter_count:  1996 the loss: 260693.42786303934 the gradient: [-113.57288162]\n",
      "iter_count:  1997 the loss: 245626.16149929268 the gradient: [-76.37778489]\n",
      "iter_count:  1998 the loss: 297678.88585320197 the gradient: [-193.868761]\n",
      "iter_count:  1999 the loss: 221879.49711326527 the gradient: [-11.3001254]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.01130013]), array([2.01130013])]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    observed_r, predicted_r, imputed_e, observed_e, propensities = get_data()\n",
    "    theta1,theta2,theta,loss_list = SGD(predicted_r, imputed_e, observed_e, propensities, step_size = 0.001)\n",
    "    print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e8fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
