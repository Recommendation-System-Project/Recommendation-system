{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c76105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count # 迭代器\n",
    "from collections import defaultdict # 使用dict时，如果引用的Key不存在，就会抛出KeyError。如果希望key不存在时，返回一个默认值，就可以用defaultdict\n",
    "from scipy.sparse import csr # csr_matrix，全名为Compressed Sparse Row，是按行对矩阵进行压缩的。CSR需要三类数据：数值，列号，以及行偏移量。CSR是一种编码的方式，其中，数值与列号的含义，与coo里是一致的。行偏移表示某一行的第一个元素在values里面的起始偏移位置。 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm # 可以显示循环的进度条的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_dic(dic, ix=None, p=None):\n",
    "    \"\"\" \n",
    "    Creates a scipy csr matrix from a list of lists (each inner list is a set of values corresponding to a feature) \n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    dic -- dictionary of feature lists. Keys are the name of features\n",
    "    ix -- index generator (default None)\n",
    "    p -- dimension of featrure space (number of columns in the sparse matrix) (default None)\n",
    "    \"\"\"\n",
    "    if (ix == None):\n",
    "        d = count(0)\n",
    "        ix = defaultdict(lambda: next(d)) \n",
    "        \n",
    "    n = len(list(dic.values())[0]) # num samples\n",
    "    g = len(list(dic.keys())) # num groups\n",
    "    nz = n * g # number of non-zeros\n",
    "\n",
    "    col_ix = np.empty(nz, dtype=int)     \n",
    "    \n",
    "    i = 0\n",
    "    for k, lis in dic.items():     \n",
    "        # append index el with k in order to prevet mapping different columns with same id to same index\n",
    "        col_ix[i::g] = [ix[str(el) + str(k)] for el in lis]\n",
    "        i += 1\n",
    "        \n",
    "    row_ix = np.repeat(np.arange(0, n), g)      \n",
    "    data = np.ones(nz)\n",
    "    \n",
    "    if (p == None):\n",
    "        p = len(ix)\n",
    "        \n",
    "    ixx = np.where(col_ix < p)\n",
    "\n",
    "    return csr.csr_matrix((data[ixx],(row_ix[ixx], col_ix[ixx])), shape=(n, p)), ix\n",
    "\n",
    "cols = ['user','item','rating','timestamp']\n",
    "\n",
    "train = pd.read_csv('data/ua.base',delimiter='\\t',names = cols)\n",
    "test = pd.read_csv('data/ua.test',delimiter='\\t',names = cols)\n",
    "\n",
    "x_train,ix = vectorize_dic({'users':train['user'].values,  'items':train['item'].values},n=len(train.index),g=2)\n",
    "\n",
    "x_test,ix = vectorize_dic({'users':test['user'].values,   'items':test['item'].values},ix,x_train.shape[1],n=len(test.index),g=2)\n",
    "\n",
    "print(x_train)\n",
    "y_train = train['rating'].values\n",
    "y_test = test['rating'].values\n",
    "\n",
    "x_train = x_train.todense() # toarray returns an ndarray; todense returns a matrix. If you want a matrix, use todense otherwise, use toarray\n",
    "x_test = x_test.todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batcher(X_, y_=None, batch_size=-1):\n",
    "    n_samples = X_.shape[0]\n",
    "\n",
    "    if batch_size == -1:\n",
    "        batch_size = n_samples\n",
    "    if batch_size < 1:\n",
    "       raise ValueError('Parameter batch_size={} is unsupported'.format(batch_size))\n",
    "\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        upper_bound = min(i + batch_size, n_samples)\n",
    "        ret_x = X_[i:upper_bound]\n",
    "        ret_y = None\n",
    "        if y_ is not None:\n",
    "            ret_y = y_[i:i + batch_size]\n",
    "            yield (ret_x, ret_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e4ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n,p = x_train.shape\n",
    "\n",
    "k = 10\n",
    "\n",
    "x = tf.placeholder('float',[None,p])\n",
    "\n",
    "y = tf.placeholder('float',[None,1])\n",
    "\n",
    "w0 = tf.Variable(tf.zeros([1]))\n",
    "w = tf.Variable(tf.zeros([p]))\n",
    "\n",
    "v = tf.Variable(tf.random_normal([k,p],mean=0,stddev=0.01))\n",
    "\n",
    "#y_hat = tf.Variable(tf.zeros([n,1]))\n",
    "\n",
    "linear_terms = tf.add(w0,tf.reduce_sum(tf.multiply(w,x),1,keep_dims=True)) # n * 1\n",
    "pair_interactions = 0.5 * tf.reduce_sum(\n",
    "    tf.subtract(\n",
    "        tf.pow(\n",
    "            tf.matmul(x,tf.transpose(v)),2),\n",
    "        tf.matmul(tf.pow(x,2),tf.transpose(tf.pow(v,2)))\n",
    "    ),axis = 1 , keep_dims=True)\n",
    "\n",
    "y_hat = tf.add(linear_terms,pair_interactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f258025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_w = tf.constant(0.001,name='lambda_w')\n",
    "lambda_v = tf.constant(0.001,name='lambda_v')\n",
    "\n",
    "l2_norm = tf.reduce_sum(\n",
    "    tf.add(\n",
    "        tf.multiply(lambda_w,tf.pow(w,2)),\n",
    "        tf.multiply(lambda_v,tf.pow(v,2))\n",
    "    )\n",
    ")\n",
    "\n",
    "error = tf.reduce_mean(tf.square(y-y_hat))\n",
    "loss = tf.add(error,l2_norm)\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b82c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 1000\n",
    "\n",
    "# Launch the graph\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in tqdm(range(epochs), unit='epoch'):\n",
    "        perm = np.random.permutation(x_train.shape[0]) # 函数shuffle与permutation都是对原来的数组进行重新洗牌（即随机打乱原来的元素顺序）；区别在于shuffle直接在原来的数组上进行操作，改变原来数组的顺序，无返回值。而permutation不直接在原来的数组上进行操作，而是返回一个新的打乱顺序的数组，并不改变原来的数组。\n",
    "        # iterate over batches\n",
    "        for bX, bY in batcher(x_train[perm], y_train[perm], batch_size):\n",
    "            _,t = sess.run([train_op,loss], feed_dict={x: bX.reshape(-1, p), y: bY.reshape(-1, 1)})\n",
    "            print(t)\n",
    "\n",
    "\n",
    "    errors = []\n",
    "    for bX, bY in batcher(x_test, y_test):\n",
    "        errors.append(sess.run(error, feed_dict={x: bX.reshape(-1, p), y: bY.reshape(-1, 1)}))\n",
    "        print(errors)\n",
    "    RMSE = np.sqrt(np.array(errors).mean())\n",
    "    print (RMSE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
